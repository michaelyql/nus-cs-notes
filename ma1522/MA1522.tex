\documentclass[11pt]{article}

%:packages
\usepackage{geometry}                	                  
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts} 	  % \mathbb commands
\usepackage{enumitem} 	  % control description indentation
\usepackage{tcolorbox}
\usepackage{mathtools}
%\usepackage{pifont} 	  % \ding{51} checkmark, \ding{55} x mark
\tcbuselibrary{theorems}
\tcbuselibrary{skins} 	  % enable enhanced tcb option
\tcbuselibrary{breakable} % allow boxes to break to next page

%:cover page
\begin{document}
\title{MA1522 Notes (AY24/25 Sem1)}
\author{Michael Yang}
\date{September 3, 2024}							
\maketitle

%:colors
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{BabyBlue}{rgb}{0, 0.5, 1}
\definecolor{Amber}{rgb}{1.0, 0.6, 0.4}
\definecolor{CadmiumRed}{rgb}{0.89, 0.0, 0.13}
\definecolor{Silver}{rgb}{0.65, 0.65, 0.65}
\definecolor{Amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{Byzantium}{rgb}{0.44, 0.16, 0.39}

%:commands
\newcommand{\R}[1]{\mbox{$\mathbb{R}^{#1}$}}  % R^n
\newcommand{\Bf}[1]{\mathbf{#1}}
\newcommand{\Bb}[1]{\mathbb{#1}}
\newcommand{\q}{\quad}
\newcommand{\qq}{\qquad}
\newcommand{\constants}{c_{1}, c_{2}, \dots, c_{k} \in \mathbb{R}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\purple}[1]{{\color{violet}#1}}
\newcommand{\orange}[1]{{\color{orange}#1}}
\newcommand{\brown}[1]{{\color{brown}#1}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\br}{\\\;\\}
\newcommand{\true}{(\textbf{T})}
\newcommand{\false}{(\textbf{F})}

% Usage: \createNewTcbBox{name}{displayName}{prefix}{colour}
\newcommand{\createNewTcbBox}[4]{
    \NewTcbTheorem{#1}{#2}{%
    	enhanced,
    	separator sign none,
    	frame empty,
    	colback=#4!5!white,
    	colbacktitle=#4!25!white,
    	colframe=#4!80!white,
    	coltitle=#4!50!black,
    	borderline={0.5mm}{0mm}{#4!10!white},
    	borderline={0.5mm}{0mm}{#4!60!white},
    	attach boxed title to top left={yshift=-2mm, xshift=2mm},
    	boxed title style={boxrule=0.4pt},
    	fonttitle=\bfseries,
    	theorem name,
		breakable,
    	}{#3}
}
%:tcb boxes
\createNewTcbBox{definitionBox}{Definition}{def}{BabyBlue}
\createNewTcbBox{theoremBox}{Theorem}{theo}{ForestGreen}
\createNewTcbBox{corollaryBox}{Corollary}{corr}{Amber}
\createNewTcbBox{algorithmBox}{Algorithm}{algo}{CadmiumRed}
\createNewTcbBox{propertiesBox}{Properties}{props}{Silver}
\createNewTcbBox{lemmaBox}{Lemma}{lemma}{Amethyst}
\createNewTcbBox{noteBox}{Note}{note}{Byzantium}
\createNewTcbBox{questionBox}{Question}{question}{Byzantium}
	
\newcommand{\defn}[1]{% Definition
\begin{definitionBox}{}{}
#1
\end{definitionBox}%
\vspace{0.1cm}
}

\newcommand{\theo}[1]{% Theorem
\begin{theoremBox}{}{} 
#1
\end{theoremBox}%
\vspace{0.1cm}
}

\newcommand{\corr}[1]{% Corollary
\begin{corollaryBox}{}{}
#1
\end{corollaryBox}%
\vspace{0.1cm}
}

\newcommand{\algo}[2]{% Algorithm
\begin{algorithmBox}{#1}{}
#2
\end{algorithmBox}%
\vspace{0.1cm}
}

\newcommand{\props}[2]{% Properties
\begin{propertiesBox}{#1}{}
#2
\end{propertiesBox}%
\vspace{0.1cm}
}

\newcommand{\lemma}[1]{% Lemma
\begin{lemmaBox}{}{}
#1
\end{lemmaBox}%
\vspace{0.1cm}
}

\newcommand{\note}[1]{% Note
\begin{noteBox}{}{}
#1
\end{noteBox}%
\vspace{0.1cm}
}

\newcommand{\qn}[1]{% Question
\begin{questionBox}{}{}
#1
\end{questionBox}%
\vspace{0.1cm}
}

\newcommand{\sectiontitle}[1]{% Section title
\begin{flushleft}\large{\textbf{#1}}\end{flushleft}} 
\newcommand{\chapter}[1]{% New chapter
\newpage
\section*{#1}
\hrule
\vspace{0.3cm}
}

% Margins
\newgeometry{left=1cm, right=1cm, top=1cm, bottom=2cm} 

% Increase the max number columns for a matrix to 20 (default 10)
\setcounter{MaxMatrixCols}{20}

% Suppress underfull hbox warnings
\hbadness = 10000

%:[HIDE] Ch1-3
%\begin{comment}
% ========================== Chapter 1 ==========================
\chapter{Chapter 1: Linear Systems}

\sectiontitle{1.1 Introduction to Linear Systems}
\defn{A \textbf{linear equation} with $n$ variables in \textbf{standard form} has the form \[a_{1}x_{1}+a_{2}x_{2}+\dots+a_{n}x_{n}=b.\] Here $a_{1},a_{2},\dots,a_{n}$ are known constants, called the \textbf{coefficients}, $b$ is called the \textbf{constant} and $x_{1},x_{2},x_{n}$ are variables. \\\;\\ The linear equation is homogeneous if $b=0$, i.e. \[a_{1}x_{1}+a_{2}x_{2}+\dots+a_{n}x_{n}=0.\] A \textbf{system of linear equations}, or a \textbf{linear system} consists of a finite number of linear equations. In general, a linear system with $m$ variables and $n$ equations in standard form is written as 
\[ \left\{ \begin{matrix} 
\q a_{11}x_{1}\q+\q a_{12}x_{2}\q+\q\dots\q+\q a_{1n}x_{n}\q=\q b_{1} \\
\q a_{21}x_{1}\q+\q a_{22}x_{2}\q+\q\dots\q+\q a_{2n}x_{n}\q=\q b_{2} \\
\q\q\vdots \\
\q a_{m1}x_{1}\q+\q a_{m2}x_{2}\q+\q\dots\q+\q a_{mn}x_{n}\q=\q b_{m} 
\end{matrix} \right. 
\]
The linear system is \textbf{homogeneous} if $b_{1}=b_{2}=\dots=b_{m}=0$, that is, all the linear equations are homogeneous.\\

Given a linear system, we say that \[ x_{1}=c_{1}, x_{2}=c_{2},\dots,x_{n}=c_{n}\] is a \textbf{solution} to the \textbf{linear system} if the equations are simultaneously satisfied after making the substitution.\\

The \textbf{general solution} to a linear system captures all possible solutions to the linear system.\\

A linear system is said to be \textbf{inconsistent} if it does not have any solutions. It is \textbf{consistent} otherwise, that is, a linear system is consistent if it has at least one solution.
}

\sectiontitle{1.2 Solving a Linear System and Row-Echelon Form}
\defn{(Augmented Matrix)\\

A linear system 
\[ \left\{ \begin{matrix} 
\q a_{11}x_{1}\q+\q a_{12}x_{2}\q+\q\dots\q+\q a_{1n}x_{n}\q=\q b_{1} \\
\q a_{21}x_{1}\q+\q a_{22}x_{2}\q+\q\dots\q+\q a_{2n}x_{n}\q=\q b_{2} \\
\q\q\vdots \\
\q a_{m1}x_{1}\q+\q a_{m2}x_{2}\q+\q\dots\q+\q a_{mn}x_{n}\q=\q b_{m} 
\end{matrix} \right. 
\]
can be expressed uniquely as an \textbf{augmented matrix}
\[ \left( \begin{matrix} 
\q a_{11} && a_{12} && \dots && a_{1n} \\
\q a_{21} && a_{22} && \dots && a_{2n} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q a_{m1} && a_{m2} && \dots && a_{mn} \\
\end{matrix}
\q\middle|\q
\begin{matrix}
b_{1}\\b_{2}\\ \vdots\\ b_{m}
\end{matrix}\q
\right)
\]
}

\defn{In the augmented matrix, a \textbf{zero row} is a row with all entries 0. A \textbf{leading entry} of a row is the first nonzero entry of the row counting from the left.\\

An augmented matrix is in \textbf{row-echelon form (REF)} if
\begin{enumerate}
\item If \textbf{zero rows} exists, they are at the bottom of the matrix.
\item The \textbf{leading entries} are \textbf{further to the right} as we move down the rows.
\end{enumerate} \;\\
An augmented matrix in \textbf{REF} has the form
\[
\left(\q
\begin{matrix}
*      &   \   & \ & \ & \     & \ & \ & \ \\
0      & \dots & 0 & * & \     & \ & \ & \ \\
0      & \dots & 0 & 0 & \dots & 0 & * & \ \\
\vdots &  \    & \ & \ &   \   & \ & \ & \vdots \\
0      & \dots & \ & \ &   \   & \ & \dots & 0 \\
\end{matrix}
\q\middle|\q
\begin{matrix}
* \\ * \\ * \\ \vdots \\ 0
\end{matrix}\q
\right)
\]

\;\\In the \textbf{row-echelon form}, a \textbf{pivot column} is a column containing a \textbf{leading entry}. Otherwise, it is called a \textbf{non-pivot column}.\\

The augmented matrix is in \textbf{reduced row-echelon form (RREF)} if further
\begin{enumerate}
\item The leading entries are 1.
\item In each pivot column, all entries except the leading entry is 0.
\end{enumerate}
\;\\ A matrix in \textbf{RREF} has the form
\[
\left(\q
\begin{matrix}{}
0      & \dots & 1 &     \ & *  & 0 & \ & * & 0 & * \\
0      & \dots & 0 & \dots & 0  & 1 & \ & * & 0 & * \\
0      & \dots & 0 & \dots & 0  & 0 & \dots & 0 & 1 & * \\
0      & \dots & 0 & \dots & \  & 0 & \ & \ & 0 & 0 \\
\vdots & \     & \ & \     & \  & \vdots & \ & \ & \vdots & \vdots \\
0      & \dots & 0 & \dots & \  & 0 & \ & \dots & 0 & 0 \\
\end{matrix}
\q\middle|\q
\begin{matrix}
* \\ * \\ * \\ 0\\ \vdots \\ 0
\end{matrix}\q
\right)
\]
\;\\
\textbf{Solutions from REF and RREF}
\begin{itemize}
\item If the augmented matrix is in row-echelon form, we perform \textbf{back substitution} to obtain the solutions.
\item If the augmented matrix is in reduced row-echelon form, we will read off the solutions directly. 
\end{itemize}\;\\
\textbf{Number of solutions from row-echelon form.}
\begin{itemize}
\item \textbf{No solution}: a row of zero before the bar (coefficient matrix) and a non zero number after the bar.
\item Unique solution: all columns of coefficient matrix are pivot columns (not possible if there is more variables than equations)
\item \textbf{Infinitely many solutions}: when there is a non-pivot column in the augmented matrix before the bar
\end{itemize}
}

\sectiontitle{1.3 Elementary Row Operations}
\defn{There are 3 types of \textbf{elementary row operations}.
\begin{enumerate}
\item Exchanging 2 rows, $R_{i}\leftrightarrow R_{j}$;
\item Adding a multiple of a row to another, $R_{i}+cR_{j},c\in\mathbb{R}$ and $i\not = j$;
\item Multiplying a row by a nonzero constant, $aR_{j}, a\not =0$. 
\end{enumerate}
Two \textbf{(augmented) matrices} are \textbf{row equivalent} if one can be obtained from the other by \textbf{elementary row operations}.}

\theo{Two linear systems have the \textbf{same solutions} if their \textbf{augmented matrices} are \textbf{row equivalent}.}

\defn{(Reverse of elementary row operations)\\\;

Every elementary row operation has a reverse elementary row operation. The reverse of the row operations are given as such.
\begin{enumerate}
\item The reverse of exchanging 2 rows, $R_{i}\leftrightarrow R_{j}$, is itself.
\item The reverse of adding a multiple of a row to another, $R_{i}+cR_{j}$ is subtracting the multiple of that row, $R_{i}-cR_{j}$.
\item The reverse of multiplying a row by a nonzero constant, $aR_{j}$ is the multiplication of the reciprocal of the constant, $\frac{1}{a}R_{j}$.
\end{enumerate}
}

\sectiontitle{1.4 Row Reduction, Gaussian and Gauss-Jordan Elimination}
\defn{\textbf{Gaussian elimination}\\

\textbf{Step 1:} Locate the leftmost column that does not consist entirely of zeros.\\

\textbf{Step 2}: Interchange the top row with another row, if necessary, to bring a nonzero entry to the top of the column found in Step 1.\\

\textbf{Step 3}: For each row below the top row, add a suitable multiple of the top row to it so that the entry below the leading entry of the top row becomes zero.\\

\textbf{Step 4}: Now cover the top row in the augmented matrix and begin again with Step 1 applied to the submatrix that remains. Continue this way until the entire matrix is in row-echelon form.

The (augmented) matrix is now is row-echelon form. The result of step 1 to 4 reduces to (augmented) matrix to a row-echelon form. The process up to step 4 is called \textbf{Gaussian Elimination}.\\

\textbf{Gauss-Jordan elimination}\\
If we continue to perform the next 2 steps, the entire process is called \textbf{Gauss-Jordan Elimination}.\\

\textbf{Step 5}: Multiply a suitable constant to each row so that all the leading entries become 1.\\ 

\textbf{Step 6}: Beginning with the last nonzero row and working upward, add suitable multiples of each row to the rows above to introduce zeros above the leading entries.\\

The (augmented) matrix is now in reduced row-echelon form.
}

\sectiontitle{1.5 More on Linear Systems}
\underline{\textbf{Summary to solving linear systems}}
\begin{enumerate}
\item Write the linear system in its standard form.
\item Form the augmented matrix of the linear system.
\item Reduce the augmented matrix to either a row-echelon form or reduced row echelon form. May use Gaussian/Gauss-Jordan elimination.
\item Check if the system is consistent
\begin{itemize}
\item If the last column is a pivot column, the system is inconsistent.
\item Otherwise, the system is consistent. If there are any non-pivot columns in the left hand side of the augmented matrix, assign the corresponding variables as parameters, $s, t$, or $s_{1},s_{2},\dots,s_{k}$.
\end{itemize}
\item If the system is in reduced row-echelon form, read off the solutions directly.
\item If the system is in row-echelon form only, do back substitution, starting from the lowest nonzero row.
\item Write down the (general) solution to the system.
\end{enumerate}

% ========================== Chapter 2 ==========================
\chapter{Chapter 2: Matrix Algebra}

\sectiontitle{2.1 Definition and Special types of Matrices}
\defn{A (real-valued) \textbf{matrix} is a rectangular array of (real) numbers
\[
\Bf{A}=
\left(\q\begin{matrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn} \\
\end{matrix}\q\right)=(a_{ij})_{m\times n}={(a_{ij})^{m}_{i=1}}^{n}_{j=1}
\]
where $a_{ij}\in$ \R{} are real numbers. The size of the matrix is said to be $m\times n$ (read as $m$ by $n$), where $m$ is the number of rows and $n$ is the number of columns. \\

The numbers in the array are called \textbf{entries}. The $(i,j)$-entry, $a_{ij}$, $i=1,\dots,m$, $j=1,\dots,n$, is the number in the $i$-th row $j$-th column.
}

\defn{(Special Types of Matrices)\\

\textbf{Vectors}: A $n \times 1$ matrix is called a (column) \textbf{vector}, and a $1\times n$ matrix is called a (row) \textbf{vector}.\\

\textbf{Zero matrices}: All entries equal 0, denoted as $\Bf{0}_{m\times n}$. \textbf{Not necessarily a square matrix}.\\

\textbf{Square matrices}: Number of rows = number of columns. \[
\Bf{A}=(a_{ij})_{n}=
\left(\q\begin{matrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{m2} & \dots & a_{nn} \\
\end{matrix}\q\right)
\]

A size $n \times n$ matrix is a square matrix of \textbf{order} $n$.\\

The entries $a_{ii}, i = 1, 2, \dots, n,$ (explicitly, $a_{11}$, $a_{22}$, $\dots$, $a_{nn}$) are called the \textbf{diagonal entries} of the \textbf{(square) matrix}.\\

\textbf{Diagonal matrix}: \textbf{D} = $(a_{ij})_{n}$, $a_{ij}=0$ for $i\not= j$. Denote as \textbf{D} = \[ diag (d_{1}, d_{2}, \dots, d_{n})= 
\left(\q\begin{matrix}
d_{1} & 0 & \dots & 0 \\
0 & d_{2} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_{n} \\
\end{matrix}\q\right)\]

\textbf{Scalar matrix}: \textbf{C} = $(a_{ij})$, $a_{ij}=\begin{cases}c\q \text{if } i = j\\ 0\q  \text{if } i \not=j \end{cases}$, \textbf{C} = \[ diag (c, c, \dots, c)= 
\left(\q\begin{matrix}
c & 0 & \dots & 0 \\
0 & c & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & c \\
\end{matrix}\q\right)\]

\textbf{Identity matrix}: \textbf{I} = $(a_{ij})$, $a_{ij}=\begin{cases}1\q \text{if } i = j\\ 0\q \text{if } i \not=j \end{cases}$, $\Bf{I}_{n}$ = \[ diag (1, 1, \dots, 1)= 
\left(\q\begin{matrix}
1 & 0 & \dots & 0 \\
0 & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & 1 \\
\end{matrix}\q\right)\]

A scalar matrix can also be denoted as $\textbf{C}$ = $c\textbf{I}$, where \textbf{I} is the identity matrix. \\

\textbf{Upper triangular}: \textbf{A} = $(a_{ij})$, $a_{ij}=0$ for $i > j$: \[ 
\left(\q\begin{matrix}
* & * & \dots & * \\
0 & * & \dots & * \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & * \\
\end{matrix}\q\right)\]\\

\textbf{Strictly upper triangular}: \textbf{A} = $(a_{ij})$, $a_{ij}=0$ for $i \geq j$: \[ 
\left(\q\begin{matrix}
0 & * & \dots & * \\
0 & 0 & \dots & * \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & 0 \\
\end{matrix}\q\right)\]\\

\textbf{Lower triangular}: \textbf{A} = $(a_{ij})$, $a_{ij}=0$ for $i < j$: \[ 
\left(\q\begin{matrix}
* & 0 & \dots & 0 \\
* & * & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
* & * & \dots & * \\
\end{matrix}\q\right)\]\\

\textbf{Strictly lower triangular}: \textbf{A} = $(a_{ij})$, $a_{ij}=0$ for $i \leq j$: \[ 
\left(\q\begin{matrix}
0 & 0 & \dots & 0 \\
* & 0 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
* & * & \dots & 0 \\
\end{matrix}\q\right)\]\\

\textbf{Symmetric matrices}: \textbf{A} = $(a_{ij})$, $a_{ij}=a_{ji}$: \[ 
\left(\q\begin{matrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{m2} & \dots & a_{nn} \\
\end{matrix}\q\right)=
\left(\q\begin{matrix}
a_{11} & a_{21} & \dots & a_{n1} \\
a_{12} & a_{22} & \dots & a_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \dots & a_{nn} \\
\end{matrix}\q\right)
\]}

\sectiontitle{2.2 Matrix Algebra}

\defn{
Scalar multiplication: $c\left( \begin{matrix} 
\q a_{11} && a_{12} && \dots && a_{1n} \\
\q a_{21} && a_{22} && \dots && a_{2n} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q a_{m1} && a_{m2} && \dots && a_{mn} \\
\end{matrix}
\q\right)=\left( \begin{matrix} 
\q ca_{11} && ca_{12} && \dots && ca_{1n} \\
\q ca_{21} && ca_{22} && \dots && ca_{2n} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q ca_{m1} && ca_{m2} && \dots && ca_{mn} \\
\end{matrix}
\q\right)$\\

Matrix addition: \begin{align*}
\left( \begin{matrix} 
\q a_{11} && a_{12} && \dots && a_{1n} \\
\q a_{21} && a_{22} && \dots && a_{2n} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q a_{m1} && a_{m2} && \dots && a_{mn} \\
\end{matrix}
\q\right) + 
\left( \begin{matrix} 
\q b_{11} && b_{12} && \dots && b_{1n} \\
\q b_{21} && b_{22} && \dots && b_{2n} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q b_{m1} && b_{m2} && \dots && b_{mn} \\
\end{matrix}
\q\right) \\ =
\left( \begin{matrix} 
\q a_{11}+b_{11} && a_{12}+b_{12} && \dots && a_{1n}+b_{1n} \\
\q a_{21}+b_{21} && a_{22}+b_{22} && \dots && a_{2n}+b_{2n}\\
\q \vdots && \vdots && \ddots && \vdots \\
\q a_{m1}+b_{m1} && a_{m2}+b_{m2} && \dots && a_{mn}+b_{mn} \\
\end{matrix}
\q\right)
\end{align*}
}

\theo{(Properties of Matrix Addition and Scalar Multiplication)\\

For matrices $\Bf{A}=(a_{ij})_{m\times n},\Bf{B}=(b_{ij})_{m\times n},\Bf{C}=(c_{ij})_{m\times n}$ and real numbers $a,b\in$ \R{},
\begin{enumerate}
\item (Commutative) \textbf{A} + \textbf{B} = \textbf{B} + \textbf{A}
\item (Associative) \textbf{A} + (\textbf{B} + \textbf{C}) = (\textbf{A} + \textbf{B}) + \textbf{C}
\item (Additive identity) $\Bf{0}_{m\times n}$ + \textbf{A} = \textbf{A}
\item (Additive inverse) (\textbf{A}) + (-\textbf{A}) =  $\Bf{0}_{m\times n}$
\item (Distributive law) $a(\Bf{A}+\Bf{B})$ = $a\Bf{A}+a\Bf{B}$
\item (Scalar addition) $(a+b)\Bf{A}=a\Bf{A}+b\Bf{A}$
\item (Associative) $(ab)\Bf{A} = a(b\Bf{A})$
\item If $a\Bf{A}=\Bf{0}_{m\times n}$, then either $a=0$ or \textbf{A} = \textbf{0}.
\end{enumerate}}

\defn{\textbf{Matrix multiplication} \\

\textbf{AB} = $\Bf{A}=(a_{ij})_{m\times p} \Bf{B}=(b_{ij})_{p\times n} = (\sum^{p}_{k=1}a_{ik}b_{kj})_{m\times n}$

\[
\left( \begin{matrix} 
\q a_{11} && a_{12} && \dots && a_{1p} \\
\q a_{21} && a_{22} && \dots && a_{2p} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q a_{m1} && a_{m2} && \dots && a_{mp} \\
\end{matrix}
\q\right) 
\left( \begin{matrix} 
\q b_{11} && b_{12} && \dots && b_{1p} \\
\q b_{21} && b_{22} && \dots && b_{2p} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q b_{m1} && b_{m2} && \dots && b_{mp} \\
\end{matrix}
\q\right) =
\]

\[ \left( \begin{matrix} 
a_{11}b_{11}+a_{12}b_{21}+\dots+a_{1p}b_{p1} & a_{11}b_{12}+a_{12}b_{22}+\dots+a_{1p}b_{p2} & \dots & a_{1p}b_{1n}+a_{12}b_{2n}+\dots+a_{1p}b_{pn} \\
a_{21}b_{11}+a_{22}b_{21}+\dots+a_{2p}b_{p1} & a_{21}b_{22}+a_{22}b_{22}+\dots+a_{2p}b_{p2} & \dots & a_{21}b_{1n}+a_{22}b_{2n}+\dots+a_{2p}b_{pn} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1}b_{11}+a_{m2}b_{21}+\dots+a_{mp}b_{p1} & a_{m1}b_{12}+a_{m2}b_{22}+\dots+a_{mp}b_{p2} & a_{m2} & a_{m1}b_{1n}+a_{m2}b_{2n}+\dots+a_{mp}b_{pn} \\
\end{matrix}
\right) 
\]\\

Caution: Matrix multiplication is not commutative $\Bf{AB}\not = \Bf{BA}$ in general.
}

\defn{
If we multiply \textbf{A} to the left of \textbf{B}, we are \textbf{pre-multiplying} \textbf{A} to \textbf{B}.\\

If we multiply \textbf{A} to the right of \textbf{B}, we are \textbf{post-multiplying} \textbf{A} to \textbf{B}.
}

\theo{(Properties of Matrix Multiplication)\\

\begin{enumerate}
\item (Associative) (\textbf{AB})\textbf{C}=\textbf{A}(\textbf{BC})
\item (Left distributive law) \textbf{A(B+C)}=\textbf{AB}+\textbf{AC}
\item (Right distributive law) \textbf{(A + B)C} = \textbf{AC} + \textbf{BC}
\item (Commute with scalar multiplication) c(\textbf{AB}) = (c\textbf{A})\textbf{B} = \textbf{A}(c\textbf{B})
\item (Multiplicative identity) For any $m\times n$ matrix \textbf{A}, $\Bf{I}_{n}\Bf{A}=\Bf{A}=\Bf{AI}_{n}$
\item (Nonzero Zero divisor) There exists $\Bf{A}\not=\Bf{0}_{m\times p}$ and $\Bf{B}\not=\Bf{0}_{p\times n}$ such that $\Bf{AB}=\Bf{0}_{m\times n}$
\item (Zero matrix) For any $m\times n$ matrix \textbf{A}, $\Bf{A0}_{n\times p}=\Bf{0}_{m\times p}$ and $\Bf{0}_{p\times m}\Bf{A}=\Bf{0}_{p\times n}$
\end{enumerate}}

\defn{Define the power of square matrices inductively as such.
\begin{enumerate}
\item $\Bf{A}^{0}=\textbf{I}$
\item $\Bf{A}^{n}=\Bf{AA}^{n-1}$, for $n\geq 1$.
\end{enumerate}}

\sectiontitle{2.3 Linear System and Matrix Equation}

\defn{(Matrix Equation)\\

A linear system in standard form
\[ \left\{ \begin{matrix} 
\q a_{11}x_{1}\q+\q a_{12}x_{2}\q+\q\dots\q+\q a_{1n}x_{n}\q=\q b_{1} \\
\q a_{21}x_{1}\q+\q a_{22}x_{2}\q+\q\dots\q+\q a_{2n}x_{n}\q=\q b_{2} \\
\q\q\vdots \\
\q a_{m1}x_{1}\q+\q a_{m2}x_{2}\q+\q\dots\q+\q a_{mn}x_{n}\q=\q b_{m} 
\end{matrix} \right. 
\]
can be expressed as a \textbf{matrix equation}
\[ \left( \begin{matrix} 
\q a_{11} && a_{12} && \dots && a_{1n} \\
\q a_{21} && a_{22} && \dots && a_{2n} \\
\q \vdots && \vdots && \ddots && \vdots \\
\q a_{m1} && a_{m2} && \dots && a_{mn} \\
\end{matrix}
\q\right)
\left(
\begin{matrix}
x_{1}\\x_{2}\\ \vdots\\ x_{n}
\end{matrix}
\right) = 
\left(
\begin{matrix}
b_{1}\\b_{2}\\ \vdots\\ b_{n}
\end{matrix}
\right), \Bf{Ax = b}
\]
Here $\Bf{A}=(a_{ij})_{m\times n}$ is called the \textbf{coefficient matrix}, $\Bf{x} = (x_{i})_{n\times 1}$ the \textbf{variable vector}, and $\Bf{b} = (b_{i})_{m\times 1}$ the \textbf{constant vector}. \\

It can also be expressed as a \textbf{vector equation}:
\[x_{1}\left(\begin{matrix}a_{11}\\a_{21}\\ \vdots\\ a_{m1}\end{matrix}\right) + x_{2}\left(\begin{matrix}a_{12}\\a_{22}\\ \vdots\\ a_{m2}\end{matrix}\right)  + \dots + 
x_{n}\left(\begin{matrix}a_{1n}\\a_{2n}\\ \vdots\\ a_{mn}\end{matrix}\right) = 
\left(\begin{matrix}b_{1}\\b_{2}\\ \vdots\\ b_{m}\end{matrix}\right), x_{1}\Bf{a}_{1}+x_{2}\Bf{a}_{2}+\dots+x_{n}\Bf{a}_{n} = \textbf{b}
\]

Here $\Bf{a}_{i}$ is called the \textbf{coefficient vector} for variable $x_{i}$, for $i=1,\dots,n$.
}

\props{of Homogeneous Linear System}{A homogeneous linear system \textbf{Ax = 0} is \textbf{always consistent}, since the \textbf{zero vector} is a solution, \textbf{A0 = 0}.}

\defn{The zero vector is called the \textbf{trivial solution}. If $x\not= 0$ is a nonzero solution to the homogeneous system, it is called a \textbf{nontrivial solution}.}

\theo{
A \textbf{homogeneous linear system} \textbf{Ax = 0} has \textbf{infinitely many solutions} if and only if it has a \textbf{nontrivial solution}.}

\lemma{Let \textbf{v} be a particular solution \textbf{Ax = b}, and \textbf{u} be a particular solution to the homogeneous system \textbf{Ax = 0} with the same coefficient matrix \textbf{A}. Then \textbf{v + u} is also a solution to \textbf{Ax = b}.}

\lemma{Suppose $\Bf{v}_{1}$ and $\Bf{v}_{2}$ are solutions to the linear system \textbf{Ax = b}. Then $\Bf{v}_{1} - \Bf{v}_{2}$ is a solution to the homogeneous linear system \textbf{Ax = 0} with the same coefficient matrix.
}

\defn{A $p \times q $ \textbf{submatrix} of an $m \times n$ matrix \textbf{A}, $p \leq m$, $q \leq  n$, is formed by taking a $p \times q$ block of the entries of the matrix \textbf{A}.}

\theo{(Block Multiplication)\\

Let \textbf{A} be an $m \times p$ matrix and \textbf{B} a $p \times n$ matrix. Let $\Bf{A}_{1}$ be a $(m_{2} - m_{1} + 1) \times p$ submatrix of \textbf{A} obtained by taking rows $m_{1}$ to $m_{2}$, and $\Bf{b}_{1}$ a $p \times (n_{2} - n_{1} + 1)$ submatrix of \textbf{B} obtained by taking columns $n_{1}$ to $n_{2}$. Then the product $\Bf{A}_{1}\Bf{B}_{1}$ is a $(m_{2} - m_{1} + 1) \times (n_{2} - n_{1} + 1)$ submatrix of \textbf{AB} obtained by taking rows $m_{1}$ to $m_{2}$ and columns $n_{1}$ to $n_{2}$.\\

In particular, let $\Bf{b}_{i}$ be the $j$-th column of \textbf{B}. Then \[ \Bf{AB}=\Bf{A}(\Bf{b}_{1}\q\Bf{b}_{2}\q\dots\q\Bf{b}_{n})=(\Bf{Ab_{1}}\q\Bf{Ab_{2}}\q\dots\q \Bf{Ab_{n}}).\]

Also, if $\Bf{a}_{i}$ is the $i$-th row of B, then \[ \Bf{AB}=\left(\begin{matrix}\Bf{a}_{1}\\ \Bf{a}_{2} \\ \vdots \\ \Bf{a}_{m} \end{matrix} \right) \Bf{B} = \left(\begin{matrix}\Bf{a}_{1}\Bf{B} \\ \Bf{a}_{2}\Bf{B} \\ \vdots \\ \Bf{a}_{m}\Bf{B} \end{matrix} \right)
\]
}

\sectiontitle{2.4 Inverse of Matrices}

\defn{
A $n \times n$ square matrix \textbf{A} is \textbf{invertible} if there exists a \textbf{square} matrix \textbf{B} of the same size such that $\Bf{AB} = \Bf{I}_{n} = \Bf{BA}.$\\

A matrix is said to be \textbf{non-invertible} otherwise.\\

A non-invertible square matrix is called a \textbf{singular matrix}.}

\theo{(Uniqueness of inverse)\\ 

If \textbf{B} and \textbf{C} are both inverses of a square matrix \textbf{A}, then \textbf{B} = \textbf{C}.
}

\defn{Since the inverse is \textbf{unique}, we can denote the \textbf{inverse} of an \textbf{invertible} matrix \textbf{A} by $\Bf{A}^{-1}$ and call it the \textbf{inverse} of
\textbf{A}. That is, \textbf{A} is invertible and $\Bf{A}^{-1}$ is its (unique) inverse if \[ \Bf{AA^{-1}}=\Bf{I}_{n}=\Bf{A^{-1}A}.\]
}


\theo{(Inverse of 2 by 2 square matrices)\\

A $2 \times 2$ square matrix \textbf{A} = $\left( \begin{matrix} a & b \\ c & d \end{matrix} \right)$ is invertible if and only if $ad - bc \not= 0$. In this case, the inverse is given by \[ \Bf{A}^{-1}=\frac{1}{ad-bc} \left( \begin{matrix} d & -b \\ -c & a \end{matrix} \right). \]
}

\theo{(Cancellation Law for Matrices)\\

Let \textbf{A} be an \textbf{invertible} matrix of order $n$.
\begin{enumerate}
\item (Left cancellation) If \textbf{B} and \textbf{C} are $n \times m$ matrices with \textbf{AB} = \textbf{AC}, then \textbf{B} = \textbf{C}.
\item (Right cancellation) If \textbf{B} and \textbf{C} are $m \times n$ matrices with \textbf{BA} = \textbf{CA}, then \textbf{B} = \textbf{C}.
\end{enumerate}

Caution: If \textbf{AB} = \textbf{CA}, we cannot conclude that \textbf{B} = \textbf{C}.
}

\theo{
Suppose \textbf{A} is an $n \times n$ invertible square matrix. Then for any $n \times 1$ vector \textbf{b}, \textbf{Ax} = \textbf{b} has a unique solution.
}

\corr{Suppose \textbf{A} is \textbf{invertible}. Then the \textbf{trivial solution} is the \textbf{only solution} to the homogeneous system \textbf{Ax} = \textbf{0}.}

\algo{to compute inverse}{
Suppose \textbf{A} is an invertible $n \times n$ matrix. By uniqueness of the inverse, there must be a unique solution to \[ \Bf{AX} = \Bf{I}. \] By block multiplication, we are solving the augmented matrix \[ (\Bf{A}\;|\;\textbf{I})\xrightarrow{RREF}(\Bf{I}\;|\;\Bf{A}^{-1}). \]
}

\theo{(Properties of inverses)\\

Let \textbf{A} be an \textbf{invertible matrix} of order $n$.
\begin{enumerate}
\item $(\Bf{A}^{-1})^{-1} = \Bf{A}.$
\item For any \textbf{nonzero} real number $a \in$ \R{}, $(a\Bf{A})$ is \textbf{invertible} with \textbf{inverse} $(a\Bf{A})^{-1} = \frac{1}{a}\Bf{A}^{-1}$.
\item $\Bf{A}^{T}$ is \textbf{invertible} with \textbf{inverse} $(\Bf{A}^{T})^{-1} = (\Bf{A}^{-1})^{T}$. (that is, the inverse of the transpose is the transpose of the inverse).
\item If \textbf{B} is an \textbf{invertible} matrix of order n, then (\textbf{AB}) is \textbf{invertible} with \textbf{inverse} $(\Bf{AB})^{-1} = \Bf{B}^{-1}\Bf{A}^{-1}$.
\end{enumerate}

By (4): Product of invertible matrices is invertible.\\

If $\mathbf{A}_{1},  \mathbf{A}_{2},\dots, \mathbf{A}_{k}$ are \textbf{invertible} matrices of the same size, then the product $\mathbf{A}_{1}\mathbf{A}_{2}\dots \mathbf{A}_{k}$ is \textbf{invertible} with $(\mathbf{A}_{1}\mathbf{A}_{2}\dots \mathbf{A}_{k})^{-1} = \mathbf{A}_{k}^{-1}\dots\mathbf{A}_{2}^{-1} \mathbf{A}_{1}^{-1}$.}

\defn{
The \textbf{negative power} of an \textbf{invertible} matrix is defined to be \[ \Bf{A}^{-n} = (\Bf{A}^{-1})^{n}\] for any $n > 0$.
}

\sectiontitle{2.5 Elementary Matrices}

\defn{A square matrix of order $n$ \textbf{E} is called an \textbf{elementary matrix} if it can be obtained from the identity matrix $I_{n}$ by performing a \textbf{single elementary row operation} where \[ \Bf{I}_{n} \xrightarrow{r} \Bf{E} \] is an elementary row operation.\\

The \textbf{elementary row operation} performed to obtain \textbf{E} is said to be the \textbf{row operation} corresponding to the elementary matrix.}

\theo{(Elementary matrix and elementary row operation)\\

Let \textbf{A} be an $n\times m$ matrix and \textbf{E} be the \textbf{elementary matrix corresponding} to the \textbf{elementary row operation} $r$.\\

Then the product \textbf{EA} is the \textbf{resultant} of performing the row operation $r$ on \textbf{A}, \[\Bf{A}\xrightarrow{r}\Bf{EA}.\]

Suppose now \textbf{B} is row equivalent to \textbf{A}, \[ \Bf{A}\xrightarrow{r_{1}}\xrightarrow{r_{2}}\dots\xrightarrow{r_{k}}\Bf{B}.\]

Let $\Bf{E}_{i}$ be the elementary matrix corresponding to the row operation $r_{i}$, for $i=1,2,\dots,k$. Then \[\Bf{B}=\Bf{E}_{k}\dots\Bf{E}_{2}\Bf{E}_{1}\Bf{A}.\]}

\theo{Two $n\times m$ matrices \textbf{A} and \textbf{B} are \textbf{row equivalent} if and only if there exists \textbf{elementary matrices} $\Bf{E}_{1},\Bf{E}_{2}\dots,\Bf{E}_{k}$ such that $\Bf{B}=\Bf{E}_{k}\dots\Bf{E}_{2}\Bf{E}_{1}\Bf{A}$.}

\theo{(Inverse of elementary matrices)\\

Every elementary matrix \textbf{E} is \textbf{invertible}. The inverse $\Bf{E}^{-1}$ is the elementary row operation corresponding to the \textbf{reverse} of the original corresponding row operation.
\begin{enumerate}
\item $\q\Bf{I}_{n}\xrightarrow{R_{i}+cR_{j}}\Bf{E}\xrightarrow{R_{i}-cR_{j}}\Bf{I}_{n}\implies \Bf{E}:R_{i}+cR_{j}, \Bf{E}^{-1}:R_{i}-cR_{j}$.
\item $\q\Bf{I}_{n}\xrightarrow{R_{i}\leftrightarrow R_{j}}\Bf{E}\xrightarrow{R_{i}\leftrightarrow R_{j}}\Bf{I}_{n}\implies \Bf{E}:R_{i}\leftrightarrow R_{j}, \Bf{E}^{-1}:R_{i}\leftrightarrow R_{j}$.
\item $\q\Bf{I}_{n}\xrightarrow{\;cR_{i}\q}\Bf{E}\xrightarrow{\;\frac{1}{c}R_{i}\q}\Bf{I}_{n}\implies \Bf{E}:cR_{i}, \Bf{E}^{-1}:\frac{1}{c}R_{i}$.
\end{enumerate}

}

\sectiontitle{2.6 Equivalent Statements for Invertibility}

\theo{If \textbf{A} = $\Bf{E}_{k}\dots\Bf{E}_{2}\Bf{E}_{1}$ is a product of elementary matrices, then \textbf{A} is invertible.}

\corr{If the reduce row-echelon form of \textbf{A} is the identity matrix, then \textbf{A} is invertible.}

\theo{A square matrix \textbf{A} is invertible if and only if the homogeneous system $\Bf{Ax = 0}$ has only the trivial solution}

\theo{A square matrix \textbf{A} is invertible if and only if it’s reduced row-echelon form is the identity matrix.}

\theo{A square matrix \textbf{A} is invertible if and only if it is a product of elementary matrices.}

\defn{
Let \textbf{A} be a $n \times m$ matrix.
\begin{enumerate}
\item A $m \times n$ matrix \textbf{B} is said to be a left inverse of \textbf{A} if $\Bf{BA = I}_{m}$, where $\Bf{I}_{m}$ is the $m \times m$ identity matrix.
\item A $m \times n$ matrix \textbf{B} is said to be a right inverse of \textbf{A} if $\Bf{AB = I}_{n}$, where $\Bf{I}_{n}$ is the $n \times n$ identity matrix.
\end{enumerate}
\textbf{B} is a left inverse of \textbf{A} if and only if \textbf{A} is a right inverse of \textbf{B}.
}

\theo{A square matrix \textbf{A} is invertible if and only if it has a left inverse.}

\theo{A square matrix \textbf{A} is invertible if and only if it has a right inverse.}

\theo{A square matrix \textbf{A} is invertible if and only if \textbf{Ax = b} has a unique solution for all \textbf{b}.}

\algo{for Finding Inverse}{
Let \textbf{A} be a $ n \times n$ matrix.\\

Step 1: Form the $n \times 2n$ (augmented matrix) $(\;\Bf{A}\;|\;\Bf{I}_{n})$. \\

Step 2: Reduce the matrix $(\;\Bf{A}\;|\;\Bf{I}_{n}) \rightarrow (\;\Bf{R}\;|\;\Bf{B})$ to its REF or RREF. \\

Step 3: If RREF $\Bf{R}\not= \Bf{I}$ or REF has a zero row, then \textbf{A} is not invertible. If RREF \textbf{R = I} or REF has no zero row, \textbf{A} is invertible with inverse $\Bf{A}^{-1}=\Bf{B}$.}

\sectiontitle{2.7 LU Factorization}

\defn{A square matrix \textbf{L} is a \textbf{unit lower triangular} matrix if \textbf{L} is a lower triangular matrix with 1 in the diagonal entries.\\

An \textbf{LU factorization} of a $m\times n$ matrix \textbf{A} is the decomposition
\[\Bf{A=LU},\]where \textbf{L} is a unit lower triangular matrix, and \textbf{U} is a row-echelon form of \textbf{A}.\\

If such LU factorization exits for \textbf{A}, we say that \textbf{A} is LU factorizable.}

\lemma{(Product of unit lower triangular matrix is unit lower triangular)\\

Let \textbf{A} and \textbf{B} be unit lower triangular matrices of the same size. Then \textbf{AB} is a unit lower triangular matrix too.}

\lemma{If \textbf{E} is an elementary matrix corresponding to the operation $R_{i}+cR_{j}$ for $i > j$ for some real number $c$, then \textbf{E} is a lower triangular matrix.}

\algo{to LU factorization}{Suppose $\Bf{A}\xrightarrow{r_{1},r_{2},\dots,r_{k}} \Bf{U}$, where each row operation $r_{l}$ is of the form $R_{i}+cR_{j}$ for some $i>j$ and real number $c$, and \textbf{U} is an row-echelon form of \textbf{A}. Let $\Bf{E}_{i}$ be the elementary matrix corresponding for $r_{i}$, for $r=1,2,\dots,k$. Then \[ \Bf{E}_{k}\dots\Bf{E}_{2}\Bf{E}_{1}\Bf{A}=\Bf{U} \implies \Bf{A}=\Bf{E}_{1}^{-1}\Bf{E}_{2}^{-1}\dots\Bf{E}_{k}^{-1}\Bf{U}=\Bf{LU} \] where $\Bf{L}=\Bf{E}_{1}^{-1}\Bf{E}_{2}^{-1}\dots\Bf{E}_{k}^{-1}$. Then \[\Bf{A}=\Bf{LU}=
\left(\q\begin{matrix}
1 & 0 & \dots & 0 \\
* & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
* & * & \dots & 1 \\
\end{matrix}\q\right)
\left(\q
\begin{matrix}{}
* & \ & \ & \ & \ & \dots & * \\
0 & \dots & 0 & * & \ & \dots & * \\
\vdots & \ & \ & \ & \ & \ & \vdots \\
0 & \dots & \ & \ & \ & \dots & * \\
\end{matrix}
\q\right)
\]\\
\textbf{Solving Linear System using LU Factorization}\\

Let \textbf{A = LU} be a LU-factorization. Consider the linear system \textbf{Ax = b}.
\begin{itemize}
\item Since \textbf{L} is a unit lower, can solve \textbf{Ly = b} by substitution starting from top row.
\item Since \textbf{U} is in row-echelon form, can solve \textbf{Ux = y} by back subsitution.
\end{itemize}
}

\defn{A $n \times n$ matrix \textbf{P} is a \textbf{permutation matrix} if every rows and columns has a 1 in only one entry, and 0 everywhere else. Equivalently, \textbf{P} is a permutation matrix if and only if \textbf{P} is the product of elementary matrices corresponding to row
swaps.
}

\sectiontitle{2.8 Determinant by Cofactor Expansion}

\defn{The \textbf{determinant} of \textbf{A} is defined to be
\begin{align}
\text{det}(\Bf{A})=a_{i1}A_{i1}+a_{i2}A_{i2}+\dots+ a_{in}A_{in}=\sum_{k=1}^{n}a_{ik}A_{ik}\\
=a_{1j}A_{1j}+a_{2j}A_{2j}+\dots+ a_{nj}A_{nj}=\sum_{k=1}^{n}a_{kj}A_{kj} 
\end{align}
where \[ A_{ij}=(-1)^{i+j}\text{det}(\Bf{M}_{ij})\] is the $(i, j)$-\textbf{cofactor} of \textbf{A}, and $\Bf{M}_{ij}$, the $(i, j)$ \textbf{matrix minor} of \textbf{A}, obtained from \textbf{A} by deleting the $i$-th row and $j$-th column.\\

This is called the \textbf{cofactor expansion} along $\begin{cases}\text{row}\q\q i\q(1) \\ \text{column } j\q(2)\end{cases}$.\\

The determinant of \textbf{A} is also denoted as $\text{det}(\Bf{A})=|\Bf{A}|$.}

\theo{(Determinant is invariant under transpose)\\

The determinant of a square matrix \textbf{A} is equal to the determinant of its transpose, 
\[ \text{det}(\Bf{A})=\text{det}(\Bf{A}^{T}).\]}

\corr{The \textbf{determinant} of a \textbf{triangular matrix} is the \textbf{multiplication of the diagonal entries}. That is, if \textbf{A} = $(a_{ij})_{n}$ is a \textbf{triangular matrix}, then \[\text{det}(\Bf{A})=a_{11}a_{22}=\dots a_{nn}=\prod^{n}_{k=1}a_{ii}.\]}

\sectiontitle{2.9 Determinant by Reduction}

\theo{Suppose \textbf{B} is obtained from \textbf{A} by a \textbf{single} \textbf{elementary row operation}, $\Bf{A}\xrightarrow{r} \Bf{B}$.\\
Then the \textbf{determinant} of \textbf{B} is obtained from the \textbf{determinant} of \textbf{A} as such.
\begin{itemize}
\item If $r=R_{i} + aR_{j}$, then det(\textbf{B}) = det(\textbf{A});
\item If $r=cR_{i}$, then det(\textbf{B}) = $c$ det(\textbf{A});
\item If $r=R_{i} \leftrightarrow R_{j}$, then det(\textbf{B}) = $-$det(\textbf{A}).
\end{itemize}
}

\corr{The determinant of an elementary matrix \textbf{E} is given as such.
\begin{itemize}
\item If \textbf{E} corresponds to $R_{i}+aR_{j}$, then det(\textbf{E}) = 1.
\item If \textbf{E} corresponds to $cR_{i}$, then det(\textbf{E}) = $c$.
\item If \textbf{E} corresponds to $R_{i}\leftrightarrow R_{j}$, then det(\textbf{E}) = $-$1.
\end{itemize}
}

\theo{Let \textbf{A} and \textbf{R} be square matrices such that \[\Bf{R}=\Bf{E}_{k}\dots\Bf{E}_{2}\textbf{E}_{1}\Bf{A}\] for some elementary matrices $\mathbf{E}_{1},  \mathbf{E}_{2},\dots, \mathbf{E}_{k}$. Then \[ \text{det}(\Bf{R}) = \text{det}(\Bf{E}_{k})\dots\text{det}(\Bf{E}_{2})\text{det}(\Bf{E}_{1})\text{det}(\Bf{A}).  \]
}

\corr{Let \textbf{A} be a $n\times n$ square matrix.\\
Suppose $\Bf{A}\xrightarrow{r_{1}}\xrightarrow{r_{2}}\dots\xrightarrow{r_{k}}\Bf{R}=\left( 
\begin{matrix} 
d_{1} & * & \dots & * \\ 
0 & d_{2} & \dots & * \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_{n} \\
\end{matrix} \right)$
, where \textbf{R} is the reduced row-echelon form of \textbf{A}. Let $\Bf{E}_{1}$ be the elementary matrix corresponding to the elementary row operation $r_{i}$, for $i=1,\dots,k$. Then \[ 
\text{det}(\Bf{A})=\frac{d_{1}d_{2}\dots d_{n}}{\text{det}(\Bf{E}_{k})\dots\text{det}(\Bf{E}_{2})\text{det}(\Bf{E}_{1})}.
\]
}

\sectiontitle{2.10 Properties of Determinant}

\theo{A \textbf{square matrix A} is \textbf{invertible} if and only if $\text{det}(\Bf{A})\not=0$.}

\theo{(Determinant of product is the product of determinant)\\\;\\
Let \textbf{A} and \textbf{B} be square matrices of the same size. Then \[\text{det}(\Bf{AB})=\text{det}(\Bf{A})\text{det}(\Bf{B}).\] By induction, for square matrices $\mathbf{A}_{1},  \mathbf{A}_{2},\dots, \mathbf{A}_{k}$ of the same size, \[\text{det}(\mathbf{A}_{1}  \mathbf{A}_{2}\dots \mathbf{A}_{k})=\text{det}(\Bf{A}_{1})\text{det}(\Bf{A}_{2})\dots\text{det}(\Bf{A}_{k}).\]
}

\theo{(Determinant of inverse is the inverse of determinant)\\\;\\
If \textbf{A} is \textbf{invertible}, then \[\text{det}(\Bf{A}^{-1})=\text{det}(\Bf{A})^{-1}\]
}

\theo{(Determinant of scalar multiplication)\\\;\\
For any square matrix \textbf{A} of order $n$ and scalar $c$, \[\text{det}(c\Bf{A})=c^{n}\;\text{det}(\Bf{A}).\]
}

\defn{Let \textbf{A} be a $n\times n$ square matrix. The \textbf{adjoint} of \textbf{A}, denoted as  \textbf{adj(A)}, is the $n\times n$ square matrix whose $(i, j)$ entry is the $(j, i)$-cofactor of \textbf{A},\[ \Bf{adj(A)}=\left( \begin{matrix}A_{11} & A_{12} & \dots & A_{1n}\\ A_{21} & A_{22} & \dots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \dots & A_{nn} \end{matrix} \right)^{T} = \left( \begin{matrix}A_{11} & A_{21} & \dots & A_{n1}\\ A_{12} & A_{22} & \dots & A_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ A_{1n} & A_{2n} & \dots & A_{nn} \end{matrix} \right) \]}

\theo{(Adjoint formula)\\\;\\
Let \textbf{A} be a \textbf{square} matrix and \textbf{adj(A)} be its \textbf{adjoint}. Then\[ \Bf{A(adj(A))}=\text{det}(\Bf{A})\Bf{I},\] where \textbf{I} is the identity matrix.}

\corr{(Adjoint formula for inverse)\\\;\\
Let \textbf{A} be an \textbf{invertible} matrix. Then the \textbf{inverse} of \textbf{A} is given by \[\textbf{A}^{-1}=\frac{1}{\text{det}(\Bf{A})}\Bf{adj(A)}\]}

\note{
For any square matrix \textbf{A}, $\Bf{A}^{2}\not=-\Bf{I}$ (False) \\
Take \textbf{A} = [0 -1; 1 0]\br
For any square matrix \textbf{A}, \textbf{A = 0} iff \textbf{adj(A) = 0} (False) \\
Take \textbf{A} = [1 1 1; 1 1 1; 1 1 1]\br

}

% ========================== Chapter 3 ==========================
\chapter{Chapter 3: Euclidean Vector Spaces}

\sectiontitle{3.1 Euclidian Vector Spaces}

\defn{A (real) \emph{n}-\textbf{vector} is a collection of \emph{n} ordered real numbers, \[ \mathbf{v}\;=\; \begin{pmatrix} v_{1} \\ v_{2} \\ \vdots \\ v_{n} \end{pmatrix} ,where\text{ } v_{i} \in \mathbb{R} \text{ \emph{for} } i = 1, \dots, n. \] The real number $v_{i}$ is called the \emph{i}-th coordinate of the vector \textbf{v}. The \textbf{Euclidiean} \emph{n}-\textbf{space}, denoted \R{n}, is the collection of all \emph{n}-vectors \[ \mathbb{R}^{n} = \left\{ v =  \begin{matrix}   \begin{pmatrix}  v_{1} \\ v_{2} \\ \vdots \\ v_{n} \end{pmatrix} \end{matrix} \middle| v_{i} \in \mathbb{R} \text{ \emph{for} } i = 1, \dots, n.\right\} \]}

\props{of Vector Addition and Scalar Multiplication}{
Since vectors are matrices (column vectors are $n \times 1$  matrices and row vectors are $1 \times n$ matrices), the properties of matrix addition and scalar multiplication holds for vectors. For any vectors $\mathbf{u}, \mathbf{v}, \mathbf{w}$ and scalars $a, b\in \mathbb{R}$ , \begin{enumerate}
\item The sum $\Bf{u} + \Bf{v} $ is a vector in \R{n}
\item (Commutative) $\Bf{u} + \Bf{v} = \Bf{v} + \Bf{u}$
\item (Associative) $\Bf{u} + (\Bf{v} + \Bf{w}) = (\Bf{u} + \Bf{v}) +\Bf{w} $.
\item (Zero vector) $\Bf{0} + \Bf{v} = \Bf{v}$.
\item The negative $-\Bf{v}$ is a vector in \R{n} such that $\Bf{v}-\Bf{v}=\Bf{0}$.
\item (Scalar multiple) $a\Bf{v}$  is a vector in \R{n}.
\item (Distribution) $a(\Bf{u} + \Bf{v})= a\Bf{u} + a\Bf{v}$.
\item (Distribution) $(a+b)\Bf{u} = a\Bf{u} + b\Bf{u}$.
\item (Associativity of scalar multiplication) $(ab)\textbf{u} = a(b\Bf{u})$.
\item If $a\Bf{u} = \Bf{0}$, then either $a=0$ or $\Bf{u} = \Bf{0}$.
 \end{enumerate}
}

\defn{A \textbf{linear combination} of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k} \in $ \R{n} is $c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}$ for some $c_{1},  c_{2},\dots, c_{k} \in$  \R{}.}

\defn{A set $V$ equipped with \textbf{addition} and \textbf{scalar multiplication} is said to be a \textbf{vector space} over \R{}  if it satisfies the following axioms. 
\begin{enumerate}
\item For any vectors $\Bf{u, v}$ in $V$, the sum $\Bf{u} + \Bf{v}$ is in $V$. 
\item (Commutative) For any vectors $\Bf{u, v}$ in $V$, $\Bf{u} + \Bf{v} = \Bf{v} + \Bf{u}$
\item (Associative) For any vectors $\Bf{u, v, w}$ in $V$, $\Bf{u} + (\Bf{v} + \Bf{w}) = (\Bf{u} + \Bf{v}) +\Bf{w} $
\item (Zero vector) There is a vector $\Bf{0}$ in $V$ such that $\Bf{0} + \Bf{v} = \Bf{v}$ for all vectors $\Bf{v}$ in $V$. 
\item (Negative) For any vector $\Bf{u}$ in $V$, there exists a vector $-\Bf{u}$ in $V$ such that 
$\Bf{u} + (-\Bf{u}) = \Bf{0}$.
\item For any scalar $a$ in \R{} and vector $\Bf{v}$ in $V$, $a\Bf{v}$  is a vector in $V$.
\item (Distribution) For any scalar $a$ in \R{} and vector $\Bf{u,v}$ in $V$, $a(\Bf{u} + \Bf{v})= a\Bf{u} + a\Bf{v}$.
\item (Distribution) For any scalars  $a, b$ in \R{} and vector $\Bf{u}$ in $V$, $(a + b)\Bf{u} = a\Bf{u} + b\Bf{u}$.
\item (Associativity of scalar multiplication) For any scalars $a, b$ in \R{} and vector $\Bf{u}$ in $V$, $a(b\Bf{u}) = (ab)\Bf{u}$.
\item For any vector $\Bf{u}$ in $V$, $1\Bf{u} = \Bf{u}$.
\end{enumerate}
}

\sectiontitle{3.2 Dot Product, Norm, Distance} 

\defn{The \textbf{inner product} (or \textbf{dot product}) of vectors $\textbf{u} = (u_{i})$
 and  $\textbf{v} = (v_{i})$ 
in \R{n}
 is defined to be \[ \Bf{u}\cdot \Bf{v} = {u}_{1} v_{1} +  {u}_{2} v_{2}, + \dots + {u}_{n}v_{n}.  \]  
\\ Define the \textbf{norm} of a vector $\Bf{u} \in$ \R{n}, $\Bf{u}=(u_{i})$, to be the square root of the inner product of $\Bf{u}$ with itself, and is denoted as $||\Bf{u}||$, \[ ||u|| = \sqrt{\Bf{u} \cdot \Bf{u}}=\sqrt{{u}^{2}_{1}+{u}^{2}_{2}+\dots+{u}^{2}_{n}}.\]
\\ This is also known as the \textbf{length} or \textbf{magnitude} of the vector.
}

\theo{(Properties of inner product and norm)\\\;\\Let $\Bf{u, v} \in$ \R{n} be vectors and $a,b,c\in \mathbb{R}$ be real numbers.
\begin{enumerate}
\item Inner product is \textbf{symmetric}, \[ \Bf{u}\cdot\Bf{v} = \Bf{v}\cdot\Bf{u}. \]
\item Inner product \textbf{commutes} with scalar multiple, \[c\Bf{u}\cdot\Bf{v}=(c\Bf{u})\cdot\Bf{v}=\Bf{u}\cdot(c\Bf{v}).\]
\item Inner product is \textbf{distributive}, \[ \Bf{u}\cdot(a\Bf{v} + b\Bf{w})  = a\Bf{u}\cdot\Bf{v} + b\Bf{u}\cdot\Bf{w} \]
\item Inner product is \textbf{positive definite}, $\Bf{u}\cdot\Bf{u}\geq 0$  with equality if and only if $\Bf{u} = \Bf{0}$.
\item $||c\Bf{u}||=|c| \;||\Bf{u}||$.
\end{enumerate}
}

\defn{A vector $\Bf{u} \in$ \R{n} is a \textbf{unit vector} if its norm is 1, \[||\Bf{u}||=1 \]  \textbf{Normalizing a vector} \\
Let $\Bf{u}$ be a nonzero vector $\Bf{u} \neq \Bf{0}$. By multiplying by the reciprocal of the norm, we get a unit vector, \[ \Bf{u} \longrightarrow \frac{\Bf{u}}{||\Bf{u}||} \]
This is called \textbf{normalizing} \textbf{u}.
}

\defn{The \textbf{distance} between two vectors \textbf{u} and \textbf{v}, denoted as $d(\Bf{u}, \Bf{v})$, is defined to be \[ d(\Bf{u}, \Bf{v}) = ||\Bf{u}-\Bf{v}||. \] 
Define the \textbf{angle} $\theta$ between two \textbf{nonzero} vectors, $\Bf{u, v}\neq\Bf{0}$ to be such that \[ cos(\theta)=\frac{\Bf{u}\cdot\textbf{v}}{||\Bf{u}||\;||\Bf{v}||} \]
}


\sectiontitle{3.3 Linear Combinations and Linear Spans}

\defn{
A linear combination of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k} \in$ \R{n} is \[  c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}\text{, for some } c_{1}, c_{2},\dots c_{k} \in \mathbb{R}. \]
The scalars $c_{1}, c_{2},\dots c_{k}$ are called \textbf{coefficients}.\\
\\ Let $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}$ be vectors in \R{n}. The \textbf{span} (or \textbf{linear span}) of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}$ is the subset of \R{n} containing all the linear combinations of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}$, \[ \text{span}\{\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}\}=\{ c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}\;|\; \constants  \}. \] 
}

\algo{to Check for Linear Combination}{
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a set of vectors in \R{n}.
\begin{enumerate}
\item Form the $n\times k$ matrix $\Bf{A}=(\mathbf{u}_{1}\q \mathbf{u}_{2}\q\dots\q \mathbf{u}_{k})$ whose columns are the vectors in $S$.
\item Then a vector \textbf{v} in \R{n} is in $span\{ \mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k} \}$  if and only if the system $\Bf{Ax=v}$ is consistent.
\item If the system is consistent, then the solutions to the system are the possible coefficients of the linear combination. That is, if $\Bf{u}=\left( \begin{matrix} c_{1}\\c_{2}\\ \vdots \\c_{k} \end{matrix} \right)$ is a solution to $\Bf{Ax=v}$, then \[ \Bf{v}=c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}. \]
\end{enumerate}
}

\algo{to Check if $span(S)=$ \R{n}}{ 
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a set of vectors in \R{n}. \begin{enumerate}
\item Form the $n\times k$ matrix $\Bf{A}=(\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k})$ whose columns are the vectors in $S$.
\item Then $span(S)=$ \R{n} if and only if the system $\Bf{Ax=v}$ is consistent for all \textbf{v}.
\item This is equivalent to the reduced row-echelon form of \textbf{A} having no zero rows.
\end{enumerate}
}

\props{of linear span}{
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a finite set of vector. The span of $S$, $span(S)$ has the following properties.
\begin{enumerate}
\item The span of $S$ \textbf{contains the origin}, \[ \Bf{0} \in span(S). \]
\item The span of $S$ is \textbf{closed under vector addition}, for any \textbf{u, v} $\in span(S)$, \[\Bf{u} + \Bf{v} \in span(S) \]
\item The span $S$ is \textbf{closed under scalar multiplication}, for any $\Bf{u}\in span(S)$ and real number $\alpha\in$ \R{}, \[ \alpha\Bf{u} \in span(S). \]
Properties (ii) and (iii) can be combined together into one property (ii'): The span is \textbf{closed under linear combinations}, that is, if \textbf{u, v} are vectors in $span(S)$ and $\alpha, \beta$ are any scalars, then the linear combination $\alpha\Bf{u} + \beta\Bf{v}$ is a vector in $span(S)$.
\end{enumerate}
}

\theo{(Linear span is closed under linear combinations) \\ \\
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a set of vectors in \R{n}. For any vectors $\mathbf{v}_{1}, \mathbf{v}_{2},\dots, \mathbf{v}_{m}$ in $span(S)$, the span of $\mathbf{v}_{1}, \mathbf{v}_{2},\dots, \mathbf{v}_{m}$ is a subset of $span(S)$, \[ span\{\mathbf{v}_{1},  \mathbf{v}_{2},\dots, \mathbf{v}_{m}\} \subseteq span(S). \]
}

\algo{to check for Set Relations between Spans}{
Suppose we are given 2 sets of vectors $T=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{m}\}$ and $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$. \begin{enumerate}
\item By the corollary, if $\Bf{v}_{i} \in span(S)$ for $i = 1,\dots,m$, we can conclude that $span(T) \subseteq span(S)$.
\item Recall that to check if $\Bf{v}_{i} \in span(S)$, we check that the system $(\;\textbf{u}_{1}\;\textbf{u}_{2}\;\dots\;\Bf{u}_{k} \;|\; \Bf{v}_{i}\;)$ is consistent for all $i = 1,\dots,m$.
\item There are in total $m$ such linear systems to check. However, since they have the same coefficient matrix, we may combine and check them together, that is, check that \[ (\;\textbf{u}_{1}\;\textbf{u}_{2}\;\dots\;\Bf{u}_{k} \;|\; \Bf{v}_{1}\ \;|\; \Bf{v}_{2} \;|\; \dots \;|\; \Bf{v}_{m}\; ) \] is consistent.
\end{enumerate}
}

\theo{(Algorithm to check for set relations between spans) \\ \\
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ and $T=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{m}\}$ be sets of vectors in \R{n}. Then $span(T) \subseteq span(S)$ if and only if $(\;\textbf{u}_{1}\;\textbf{u}_{2}\;\dots\;\Bf{u}_{k} \;|\; \Bf{v}_{1}\ \;|\; \Bf{v}_{2} \;|\; \dots \;|\; \Bf{v}_{m}\; )$ is consistent. 
}
 
\sectiontitle{3.4 Subspaces} 

\defn{The set of solutions to a linear system \textbf{Ax = b} can be expressed \textbf{implicitly} as \[ V = \{\Bf{u}\in \mathbb{R}^{n} \;|\; \Bf{Au=b}\} \] or \textbf{explicitly} as \[ V=\{ \Bf{u} + s_{1}\Bf{v}_{1} +  s_{2}\Bf{v}_{2} + \dots + s_{k}\Bf{v}_{k} \;|\; s_{1},s_{2},\dots,s_{k} \in \mathbb{R} \}, \] where $\Bf{u} + s_{1}\Bf{v}_{1} +  s_{2}\Bf{v}_{2} + \dots + s_{k}\Bf{v}_{k},\; s_{1},s_{2},\dots,s_{k} \in \mathbb{R}$ is the general solution.
}

\defn{A subset $V$ of \R{n} is a \textbf{subspace} if it satisfies the following properties.
\begin{enumerate}
\item $V$ \textbf{contains the zero vector}, $\textbf{0} \in V$.
\item $V$  is \textbf{closed under scalar multiplication}. For any vector $v$ in $V$ and scalar $\alpha$, the vector $\alpha\Bf{v}$ is in $V$.
\item $V$ is \textbf{closed under addition}. For any vectors \textbf{u, v} in $V$, the sum \textbf{u + v} is in $V$.
\end{enumerate}
Property (i) can be replaced with property (i'): $V$ is \textbf{nonempty}.\\ \\
Properties (ii) and (iii) is equivalent to property (ii'):
$V$ is \textbf{closed under linear combination}. For any \textbf{u, v} in $V$, and scalars $\alpha, \beta$, the linear combination $\alpha\Bf{u} + \beta\Bf{v}$ is in $V$.
}

\theo{(Solution set of a homogeneous system is a subspace) \\ \\
The solution set $V=\{\textbf{u}\;|\;\Bf{Au=b}\}$ to a linear system \textbf{Ax = b} is a \textbf{subspace} if and only if \textbf{b = 0}, that is, the system is \textbf{homogeneous}.
}

\defn{The \textbf{solution set} to a \textbf{homogeneous system} is call a \textbf{solution space.}}

\theo{(Subspaces are equivalent to linear spans) \\ \\
A subset $V\subseteq$ \R{n} is a subspace if and only if it is a linear span, $V=span(S)$, for some finite set $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$.\\ \\
\textbf{Check if a set is a subspace} \\
To show that a set $V$ is a subspace, we can either
\begin{itemize}
\item find a spanning set, that is, find a set $S$  such that $V=span(S)$, or
\item show that $V$ satisfies the 3 conditions of being a subspace.
\end{itemize}
To show that a subset $V$ is not a subspace, we can either
\begin{itemize}
\item show that it does not contain the zero vector, $\Bf{0} \not\in V$,
\item find a vector $\Bf{v}\in V$ and a scalar $\alpha\in$ \R{} such that $\alpha\Bf{v}\not\in V$, or
\item find vectors $\Bf{u, v} \in V$ such that the sum is not in $V$, $\Bf{u + v} \not\in V$.
\end{itemize}
}

\theo{(Affine spaces)\\ \\
The solution set $W=\{\;\mathbf{w}\;|\;\mathbf{Aw=b}\;\}$ of a non-homogeneous linear system \textbf{Ax = b}, $\Bf{b}\not =0$, is given by \[ \Bf{u} + V := \{\; \Bf{u+v} \;|\;\Bf{v}\in V\; \} \] where $V=\{\;\Bf{v}\;|\;\Bf{Av=0}\;\}$ is the solution space to the associated homogeneous system and \textbf{u} is a particular solution, \textbf{Au = b}. \\ \\
That is, vectors in \textbf{u} + $V$ are of the form \textbf{u + v} for some \textbf{v} in $V$.
}

\sectiontitle{3.5 Linear Independence} 

\defn{A set $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is \textbf{linearly independent} if the \textbf{only coefficients} $\constants$ satisfying the equation \[ c_{1}\mathbf{u}_{1} +  c_{2}\mathbf{u}_{2} +\dots + c_{k}\mathbf{u}_{k} = \Bf{0} \] are $c_{1} = c_{2} = \dots = c_{k} = 0$. Otherwise, we say that the set is \textbf{linearly dependent}.}

\algo{to Check for Linear Independence}{
Let $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a set of vectors in \R{n}. \begin{itemize}
\item $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is \textbf{linearly independent} if and only if the \textbf{homogeneous system} $(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)\Bf{x} = \Bf{0}$ has only the \textbf{trivial solution}.
\item The homogeneous system has only the \textbf{trivial solution} if and only if the \textbf{reduced row-echelon form} of $(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)$ has \textbf{no non-pivot column}.
\end{itemize}
}

\theo{(Solution set of a homogeneous system is a subspace) \\ \\
A subset $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ of \R{n} is \textbf{linearly independent} if and only if the \textbf{reduced row-echelon form} of $\Bf{A}=(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)$ has \textbf{no non-pivot columns}.
}

\sectiontitle{3.6 Basis and Coordinates} 

\defn{Let $V$ be a subspace of \R{n}. A set $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is a \textbf{basis} for $V$ if \begin{itemize}
\item $span(S)=V$ and
\item $S$ is linearly independent.
\end{itemize}
}

\theo{Suppose $S\;\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is a basis for $V$. Then every vector \textbf{v} in the subspace $V$ \textbf{can be written} as a linear combination of vectors in $S$ \textbf{uniquely}.}

\theo{(Basis for Solution Set of Homogeneous System) \\ \\ 
Let $V=\{\Bf{u}|\Bf{Au=0}\}$ be the solution space to some homogeneous system. Suppose \[s_{1}\mathbf{u}_{1} +  s_{1}\mathbf{u}_{2} + \dots + s_{k}\mathbf{u}_{k}, s_{1},s_{2},\dots,s_{k}\in\mathbb{R}\] is a general solution to the homogeneous system $\Bf{Ax = 0}$. \\
Then $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is a basis for the subspace $V=\{\Bf{u}|\Bf{Au=0}\}$.
}

\theo{Basis for the zero space $\{\Bf{0}\}$ of \R{n} is the \textbf{empty set} $\{\}$ or $\emptyset$.}

\theo{A $n\times n$ square matrix \textbf{A} is invertible if and only if the columns are linearly independent.}

\theo{A $n\times n$ square matrix \textbf{A} is invertible if and only if the columns spans \R{n}.}


\corr{Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{n}\}$  be a subset of \R{n}  containing $n$  vectors. Then $S$ is linearly independent if and only if $S$ spans \R{n}.
}

\corr{Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$  be a subset of \R{n} and  $\Bf{A}=(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)$ be the matrix whose columns are vectors in $S$. Then $S$ is a \textbf{basis} for \R{n} if and only if \textbf{$k=n$} and \textbf{A} is an \textbf{invertible matrix}.
}

\theo{A $n\times n$ \textbf{square} matrix \textbf{A} is invertible if and only if the \textbf{columns} of  \textbf{A} form a \textbf{basis} for \R{n}.
}

\theo{A $n\times n$ \textbf{square} matrix \textbf{A} is invertible if and only if the \textbf{rows} of \textbf{A} form a \textbf{basis} for \R{n}.
}

\theo{(Equivalent Statements for Invertibility) \\ \\ 
Let \textbf{A} be a square matrix of order $n$. The following statements are equivalent.
\begin{enumerate}
\item \textbf{A} is \textbf{invertible}.
\item $\Bf{A}^{T}$ is \textbf{invertible}.
\item \textbf{A} has a \textbf{left-inverse}, that is, there is a matrix \textbf{B} such that \textbf{BA = I}.
\item \textbf{A} has a \textbf{right-inverse}, that is, there is a matrix \textbf{B} such that \textbf{AB = I}.
\item The \textbf{reduced row-echelon form} of \textbf{A} is the \textbf{identity matrix}.
\item \textbf{A} can be expressed as a \textbf{product} of \textbf{elementary matrices}.
\item The \textbf{homogeneous system} \textbf{Ax = 0} has \textbf{only the trivial solution}.
\item For \textbf{any} \textbf{b}, the system \textbf{Ax = b} is \textbf{consistent}.
\item The \textbf{determinant} of \textbf{A} is \textbf{nonzero}, $\text{det}(\Bf{A})\not = 0$.
\item The \textbf{columns/rows} of \textbf{A} are \textbf{linearly independent} for \R{n}.
\item The \textbf{columns/rows} of \textbf{A} \textbf{spans} \R{n}.
\end{enumerate}
}

\defn{(Coordinates Relative to a Basis) \\ \\ 
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a basis for a subspace $V$ of \R{n}.\\
Then given any vector $\Bf{v} \in V$, we can write \textbf{v} unique as \[ c_{1}\mathbf{u}_{1}+  c_{2}\mathbf{u}_{2}+\dots+ c_{k}\mathbf{u}_{k}. \]
The coordinates of \textbf{v} relative to the basis $S$ is defined to be the vector \[ [\Bf{v}]_{s} = \left(\; \begin{matrix} c_{1}\\c_{2}\\ \vdots \\ c_{k} \end{matrix} \;\right). \]
}

\algo{for Computing Relative Coordinate}{
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u_{k}}\}$ be a basis for a subspace $V$ of \R{n}.\\ \\
For $\Bf{v}\in V$, find real numbers $\constants$  such that \[ c_{1}\mathbf{u}_{1} +  c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k} = \Bf{v}. \] 
That is, we are solving for \[ (\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;|\;\Bf{v}). \]
}

\theo{Let $V$ be a subspace of \R{n} and $B$ a basis for $V$.
\begin{enumerate} 
\item For any vectors \textbf{u, v }$\in V$, \textbf{u = v} if and only if $[\Bf{u}]_{B}=[\Bf{v}]_{B}$.
\item For any $\mathbf{v}_{1},  \mathbf{v}_{2},\dots, \mathbf{v}_{m}\in V$, \[ [c_{1}\mathbf{v}_{1} +  c_{2}\mathbf{v}_{2} + \dots + c_{m}\mathbf{v}_{m}]_{B} = c_{1}[\mathbf{v}_{1}]_{B} +  c_{2}[\mathbf{v}_{2}]_{B} + \dots + c_{m}[\mathbf{v}_{m}]_{B}. \]
\end{enumerate}
}

\sectiontitle{3.7 Dimensions}

\theo{Let $V$ be a subspace of \R{n} and $B$ a basis for $V$. Suppose $B$ contains $k$ vectors, $|B|=k$. Let $\mathbf{v}_{1},  \mathbf{v}_{2},\dots, \mathbf{v}_{m}$ be vectors in $V$. Then \begin{enumerate} 
\item $\mathbf{v}_{1},  \mathbf{v}_{2},\dots, \mathbf{v}_{m}$ is linearly independent (respectively, dependent) if and only if [$\mathbf{v}_{1}]_{B},  [\mathbf{v}_{2}]_{B},\dots, [\mathbf{v}_{m}]_{B}$ is linearly independent (respectively, dependent) in \R{k}; and
\item \{$\mathbf{v}_{1}, \mathbf{v}_{2},\dots, \mathbf{v}_{m}$\} spans $V$ if and only if [$\mathbf{v}_{1}]_{B},  [\mathbf{v}_{2}]_{B},\dots, [\mathbf{v}_{m}]_{B}$ spans \R{k}.
\end{enumerate}}

\corr{Let $V$ be a subspace of \R{n} and $V$ a basis for $B$. Suppose $B$ contains $k$ vectors, $|B|=k$.\begin{enumerate}
\item If $S=\{\mathbf{v}_{1},\mathbf{v}_{2}\dots,\mathbf{v}_{m}\}$ is a subset of $V$ with $m > k$, then $S$ is \textbf{linearly dependent}.
\item If $S=\{\mathbf{v}_{1},\mathbf{v}_{2}\dots,\mathbf{v}_{m}\}$ is a subset of $V$ with $m < k$, then $S$ \textbf{cannot span} $V$.
\end{enumerate}}

\corr{Suppose $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ and $T=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{m}\}$ are bases for a subspace $V\subseteq$\;\R{n}. Then $k=m$.}

\defn{Let $V$ be a subspace of \R{n}. The \textbf{dimension} of $V$, denoted by $\text{dim}(V)$, is defined to be the \textbf{number of vectors} in any \textbf{basis} of $V$.}

\theo{(Dimension of solution space) \\ \\ 
Let \textbf{A} be a $m \times n$ matrix. The \textbf{number of non-pivot columns} in the reduced row-echelon form of $A$ is the \textbf{dimension} of the solution space \[V=\{ \mathbf{u}\in \text{\R{n}}\; |\; \mathbf{Au=0} \}. \]}

\theo{(Spanning Set Theorem) \\ \\
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a subset of vectors in \R{n}, and let $V=span(S)$. Suppose $V$ is not the zero space, $V\neq \{\mathbf{0}\}$. Then there must be a subset of $S$ that is a basis for $V$.}

\theo{(Linear Independence Theorem) \\ \\ 
Let $V$ be a subspace of \R{n} and $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ a linearly independent subset of $V$, $S\subseteq V$. Then there must be a set $T$ containing $S$, $S\subseteq T$ such that $T$ is a basis for $V$.
}

\theo{Let $U$ and $V$ be \textbf{subspaces} of \R{n}.
\begin{enumerate}
\item If \textbf{$U\subseteq V$}, then \textbf{$dim(U) \leq dim(V)$}.
\item If \textbf{$U\subseteq V$}, and \textbf{$U\not = V$}, then \textbf{$dim(U) < dim(V)$}
That is, $U\subseteq V$, then \textbf{$dim(U) \leq dim(V)$} with \textbf{equality} if and only if \textbf{$U=V$}.
\end{enumerate}
}

\theo{(B1) \\ 
Let $V$ be a $k$-dimensional subspace of \R{n}, \textbf{$dim(V)=k$}. Suppose $S\subseteq V$ is a \textbf{linearly independent} subset containing \textbf{$k$} vectors, \textbf{$|S|=k$}. Then $S$ is a \textbf{basis} for $V$. \\\;\\In summary,
\begin{enumerate}
\item $|S|=dim(V)$
\item $S\subseteq V$
\item $S$ is linearly independent
\end{enumerate}
}

\theo{(B2) \\ 
Let $V$ be a $k$ dimensional subspace of \R{n}, \textbf{$dim(V)=k$}. Suppose $S$ is a set containing \textbf{$k$} vectors, \textbf{$|S|=k$}, such that \textbf{$V\subseteq span(S)$}. Then $S$ is a \textbf{basis} for $V$.\\\;\\ In summary,
\begin{enumerate}
\item $|S|=dim(V)$
\item $V\subseteq span(S)$
\end{enumerate}
}


\sectiontitle{3.8 Transition Matrices}

\defn{Let $V$ be a subspace of \R{n}. Suppose $S=\{\Bf{u}_{1},\dots,\Bf{u}_{k}\}$ and $T=\{\Bf{v}_{1},\dots,\Bf{v}_{k}\}$ are \textbf{basis} for the subspace $V$. Define the \textbf{transition matrix} from \textbf{$T$ to $S$} to be \[\mathbf{P}=(\;[\mathbf{v}_{1}]_{S}\q[\mathbf{v}_{2}]_{S}\q\dots\q[\mathbf{v}_{k}]_{S}\;),\]the matrix whose columns are the coordinates of the vectors in $T$ relative to the basis $S$.}

\theo{(Transition Matrix) \\ \\
Let $V$ be a subspace of \R{n}. Suppose $S =  \{ \Bf{u}_{1}  ,\dots, \Bf{u}_{k} \}$ and $T=\{\Bf{v}_{1},\dots,\Bf{v}_{k}\}$ are \textbf{bases} for the subspace $V$. Let \textbf{P} be the transition matrix from \textbf{$T$ to $S$}. Then for any vector $w$ in $V$, \[ [\mathbf{w}]_{S} = \mathbf{P}[\mathbf{w}]_{T}. \] }

\algo{to find Transition Matrix}{ Let $S=\{\Bf{u}_{1},\dots,\Bf{u}_{k}\}$ and $T=\{\Bf{v}_{1},\dots,\Bf{v}_{k}\}$ be basis for a subspace $V$ in \R{n}. To find \textbf{P}, the transition matrix from $T$ to $S$, \[(``S''|``T'') = (\Bf{u}_{1}\q \Bf{u}_{2}\q \dots \q\Bf{u}_{k}\q|\q \Bf{v}_{1}\q \Bf{v}_{2}\q\dots \Bf{v}_{k} \q) \xrightarrow{\text{rref}} \left(\q \begin{matrix}\mathbf{I}_{k}\\ \mathbf{0}_{(n-k)\times k}  \end{matrix}\q \middle|\q \begin{matrix}\mathbf{P}\\ \mathbf{0}_{(n-k)\times k}  \end{matrix}\q \right)  \]}

\theo{(Inverse of Transition Matrix) \\ \\ 
Suppose $S = \{ \Bf{u}_{1},\dots, \Bf{u}_{k}\}$ and $T = \{ \Bf{v}_{1},\dots, \Bf{v}_{k} \}$ are \textbf{bases} for a subspace $V$ of \R{n}. Let \textbf{P} be the \textbf{transition matrix from $T$ to $S$}. Then $\Bf{P}^{-1}$ is the \textbf{transition matrix from $S$ to $T$}.}

%:[HIDE] Ch 1-3
%\end{comment} 

%:[HIDE] Ch 4
%\begin{comment}

% ========================== Chapter 4 ==========================
\chapter{Chapter 4: Subspaces Associated to a Matrix}

\sectiontitle{4.1 Column Space, Row Space, and Nullspace}

\defn{Let \textbf{A} be an $m\times n$ matrix, 
\[ \Bf{A}=\left( \begin{matrix}a_{11} & a_{12} & \dots & a_{1n}\\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \dots & a_{mn} \end{matrix} \right) \] \\
The \textbf{row space} of \textbf{A} is the subspace of \R{n} spanned by the rows of \textbf{A}, 
\[\text{Row(\textbf{A})} = \text{span}\{(a_{11}\q a_{12}\q \dots\q a_{1n}),(a_{21}\q a_{22}\q \dots\q a_{2n}),\dots,(a_{m1}\q a_{m2}\q \dots\q a_{mn})\}\] 
The \textbf{column space} of \textbf{A} is the subspace of \R{m} spanned by the columns of \textbf{A}, 
\[
\text{Col(\textbf{A})} = \text{span} \left\{ \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\  a_{m1} \end{pmatrix}, \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\  a_{m2} \end{pmatrix}, \dots, \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\  a_{mn} \end{pmatrix} \right\}
\]
\textbf{Remark}: May write the vectors in row space as column vectors.
}

\theo{(Row operations preserve row space)\br
Suppose \textbf{A} and \textbf{B} are \textbf{row equivalent matrices}. Then $Row(\Bf{A})=Row(\Bf{B})$.
}

\theo{(Basis for row space)\br
For any matrix \textbf{A}, the \textbf{nonzero rows} of the \textbf{reduced row-echelon form} of \textbf{A} form a \textbf{basis} for the row space of \textbf{A}.
}

\theo{(Row operations preserve linear relations between columns)\br
Let $\Bf{A}=(\Bf{a}_{1}\q\Bf{a}_{2}\q\dots\q\Bf{a}_{n})$ and $\Bf{B}=(\Bf{b}_{1}\q\Bf{b}_{2}\q\dots\q\Bf{b}_{n})$ be row equivalent $m\times n$ matrices, where $\Bf{a}_{i}$ and $\Bf{b}_{i}$ is the $i$-th column of \textbf{A} and \textbf{B}, respectively, for $i=1,\dots,n$. Then for any coefficients $c_{1},c_{2},\dots,c_{n}$,
\[c_{1}\Bf{a}_{1}+c_{2}\Bf{a}_{2}+\dots+c_{n}\Bf{a}_{n}=\Bf{0}\]
if and only if 
\[c_{1}\Bf{b}_{1}+c_{2}\Bf{b}_{2}+\dots+c_{n}\Bf{b}_{n}=\Bf{0}\]
}

\theo{(Basis for column space)\br
Suppose \textbf{R} is the reduced row-echelon form of a matrix \textbf{A}. Then the columns of \textbf{A} corresponding to the pivot columns in \textbf{R} form a basis for the column space of \textbf{A}.\br
The column space is the set of vectors \textbf{v} such that \textbf{Ax = v} is consistent, or the set of vectors \textbf{v} such that \textbf{v = Au} for some \textbf{u},
\[
Col(\Bf{A})=\{\Bf{v = Au}\;|\;\Bf{u}\in\mathbb{R}^{k}\}=\{\Bf{v}\;|\;\Bf{Ax = v}\text{ is consistent}\}.
\]
}

\defn{

The \textbf{nullspace} of a $m\times n$ matrix \textbf{A} is the solution space to the homogeneous system \textbf{Ax = 0} with coefficient matrix \textbf{A}. It is denoted as 
\[
Null(\textbf{A})=\{\Bf{v}\in\mathbb{R}^{n}\;|\;\Bf{Av = 0}\}.
\]

Whenever we come across a subspace, we are interested in its dimensions.\br
The \textbf{nullity} of \textbf{A} is the dimension of the nullspace of \textbf{A}, denoted as
\[
nullity(\Bf{A})=dim(Null(\Bf{A}))
\]
}

\sectiontitle{4.2 Rank}

\theo{Let \textbf{A}
 be a $m\times n$
 matrix and \textbf{R}
 its reduced row-echelon form.
\begin{flalign*}
\text{dim(Col(\textbf{A}))}&=\text{\# of pivot columns in RREF of \textbf{A},}\\
&=\text{\# of leading entries in RREF of \textbf{A},}\\
&=\text{\# of nonzero rows in RREF of \textbf{A} = dim(Row(\textbf{A}))}\\
\end{flalign*}
}

\defn{
Define the \textbf{rank} of \textbf{A} to be the dimension of its column space or row space
\[ rank(\Bf{A})=dim(Col(\Bf{A}))=dim(Row(\Bf{A}))\]
}

\theo{
Rank is invariant under transpose, 
\[rank(\Bf{A})=rank(\Bf{A}^{T})\]
}

\theo{
The linear system \textbf{Ax = b}
 is \textbf{consistent} if and only if 
the rank of \textbf{A}
 is equal to the rank of the augmented matrix $(\Bf{A}\;|\;\textbf{b})$,
\[
rank(\Bf{A})=rank((\Bf{A\;|\;\Bf{b}})).
\]
}

\lemma{
Let \textbf{A} be a $m\times n$ matrix and \textbf{B} a $n\times p$ matrix. The column space of the product \textbf{AB} is a subspace of the column space of \textbf{A},\[\text{Col}(\Bf{AB})\subseteq \text{Col} (\Bf{A})\]
}

\theo{
Let \textbf{A} be a $m\times n$ matrix and \textbf{B} a $n\times p$ matrix. Then
\[
rank(\Bf{AB})\leq \text{min}\{rank(\Bf{A}),rank(\Bf{B})\}
\]
}

\theo{
If \textbf{A} and \textbf{B} are \textbf{row equivalent} matrices, then $rank(\Bf{A})=rank(\Bf{B})$.
}

\theo{(Rank-Nullity Theorem)\br
Let \textbf{A} be a $m\times n$  matrix. The sum of its rank and nullity is equal to the number of columns,
\[rank(\Bf{A})+nullity(\Bf{A})=n\]
}

\theo{
(Equivalent Statements of Invertibility)
\begin{enumerate}
	\setcounter{enumi}{11}
	\item \textbf{A} is of full rank, $rank(\Bf{A})=n$.
	\item $nullity(\Bf{A})=0$.
\end{enumerate}
}

\theo{(Full Rank Equals Number of Columns)\br
Suppose \textbf{A} is a $m\times n$ matrix. The following statements are equivalent.
\begin{enumerate}
	\item \textbf{A} is full rank, where the rank is equal to the number of columns, $rank(\Bf{A})=n$.
	\item The rows of \textbf{A} spans \R{n}, $\text{Row(\textbf{A})=}$ \R{n}.
	\item The columns of \textbf{A} are linearly independent.
	\item The homogeneous system \textbf{Ax = 0} has only the trivial solution, that is, $Null(\Bf{A})=\{\Bf{0}\}$.
	\item $\Bf{A}^{T}\Bf{A}$ is an invertible matrix of order $n$.
	\item \textbf{A} has a left inverse.
\end{enumerate} 
The reduced row-echelon form of \textbf{A} is 
\[
\Bf{R}=\begin{pmatrix} \Bf{I}_{n} \\ \Bf{0}_{(m-n)\times n} \end{pmatrix}
\]
}

\theo{(Full Rank Equals Number of Rows)\br
Suppose \textbf{A} is a $m\times n$ matrix. The following statements are equivalent.
\begin{enumerate}
	\item \textbf{A} is full rank, where the rank is equal to the number of columns, $rank(\Bf{A})=m$.
	\item The columns of \textbf{A} spans \R{m}, $\text{Col(\textbf{A})=}$ \R{m}.
	\item The rows of \textbf{A} are linearly independent.
	\item The linear system \textbf{Ax = b} is consistent for every \textbf{b} $\in$ \R{m}.
	\item $\Bf{A}\Bf{A}^{T}$ is an invertible matrix of order $m$.
	\item \textbf{A} has a right inverse.
\end{enumerate} 
The reduced row-echelon form of \textbf{A} is 
\[
\Bf{R}=
\begin{pmatrix} 
1 & \dots & 0 & \dots & 0 & \dots & 0 & \dots \\
0 & \dots & 1 & \dots & 0 & \dots & 0 & \dots \\
0 & \dots & 0 & \dots & 1 & \dots & 0 & \dots \\
\vdots & \dots & \vdots & \dots & \vdots & \dots & \vdots & \dots \\
0 & \dots & 0 & \dots & 0 & \dots & 1 & \dots \\
\end{pmatrix}
\]
}

\note{
If \textbf{A} and \textbf{B} are order $n$ square matrices and $\Bf{AB=0}$, then rank(\textbf{A}) + rank(\textbf{B}) $\leq n$.\br
$\Bf{AB}=\Bf{A}(\Bf{b}_{1}\q \Bf{b}_{2} \q\dots\q \Bf{b}_{n})=\Bf{0}$ $\Rightarrow$ Col(\textbf{B}) $\in$ Null(\textbf{A}) $\Rightarrow$ nullity(\textbf{A}) $\geq$ rank(\textbf{B}) $\Rightarrow$ $n - \text{rank}(\Bf{A}) \geq$ rank(\textbf{B}) $\Rightarrow$ rank(\textbf{A}) + rank(\textbf{B}) $\leq n$
}

%:[HIDE] Ch 4
%\end{comment} 

%:[HIDE] Ch 5
%\begin{comment}
% ========================== Chapter 5 ==========================
\chapter{Chapter 5: Orthogonality and Least Square Solution}

\sectiontitle{5.1 Orthogonality}

\defn{Two vectors $\Bf{u}, \Bf{v}$ in \R{n} are \textbf{orthogonal} if $\Bf{u}\cdot \Bf{v}=0$.\br In this case, either one of the vectors is the zero vector, or that they are \textbf{perpendicular}. }

\defn{
A set $S=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{k}\}$ of vectors is \textbf{orthogonal} if $\Bf{v}_{i}\cdot \Bf{v}_{j}=0$ for every $i\not=j$, that is, vectors in $S$ are \textbf{pairwise orthogonal}.\br A set $S=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{k}\}$  of vectors is \textbf{orthonormal} if for all $i,j=1,\dots,k$, \[\Bf{v}_{i}\cdot \Bf{v}_{j}=\begin{cases}0\q \text{ if }i \not= j, \\ 1\q \text{ if }i = j. \end{cases}\] That is, $S$ is \textbf{orthogonal}, and all the vectors are \textbf{unit vectors}.}

\note{\textbf{Orthogonal} set can contain zero vector $\Bf{0}$.\br \textbf{Orthonormal} set cannot contain $\Bf{0}$.}

\defn{Let $V$ be a subspace of \R{n}. A vector $n\in$ \R{n} is \textbf{orthogonal} to $V$ if for every $\Bf{v}$ in $V$, $\Bf{n}\cdot\Bf{v}=0$, that is, $\Bf{n}$ is \textbf{orthogonal} to every vector in $V$. We will denote it as $\Bf{n}\perp\Bf{V}$.}

\theo{
Let $V$ be a subspace of \R{n} and $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a spanning set for $V$, span($S$)=$V$. Then a vector $\Bf{w}$ is \textbf{orthogonal} to $V$ if and only if $\Bf{w}\cdot\Bf{u}_{i}=0$ for all $i=1,\dots,k$. 
}

\theo{
Let $V$ be a subspace of \R{n} and $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a spanning set for $V$. Then $\Bf{w}$ is \textbf{orthogonal} to $V$ if and only if $\Bf{w}$ is in the nullspace of $\Bf{A}^{T}$, where $\Bf{A}=(\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{k})$; \[ \Bf{w}\perp V \q\Leftrightarrow\q \Bf{w}\in Null(\Bf{A}^{T})\]
}

\defn{
Let $V$ be a subspace of \R{n}. The \textbf{orthogonal complement} of $V$ is the set of all vectors that are \textbf{orthogonal} to $V$, and is denoted as \[ V^{\perp}=\{ \Bf{w}\in\mathbb{R}^{n}\;|\;\Bf{w}\cdot\Bf{v}=0 \text{ for all }\Bf{v} \text{ in } V\} \]
}

\theo{
Let $V$ be a subspace of \R{n} and $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a spanning set for $V$. Let $\Bf{A}=(\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{k})$. Then the \textbf{orthogonal complement} of $V$ is the nullspace of $\Bf{A}^{T}$, \[ V^{\perp}=Null(\Bf{A}^{T})\]
}

\note{
Let $\Bf{A}$ be a $m\times n$ matrix. The nullspace of $\Bf{A}$ is the orthogonal complement of the row space of $\Bf{A}$, \[ Row(\Bf{A})^{\perp}=Null(\Bf{A})\]
}

\sectiontitle{5.2 Orthogonal and Orthonormal Bases}

\defn{
Suppose $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is an \textbf{orthogonal set} of \textbf{nonzero} vectors. Then $S$ is linearly independent.
}

\theo{
Every \textbf{orthonormal set} is \textbf{linearly independent}.
}

\defn{
Let $V$ be a subspace of \R{n}. A set $S\subseteq V$ is an \textbf{orthogonal basis} (resp, \textbf{orthonormal basis}) of $V$ if $S$ is a basis of $V$ and $S$ is an \textbf{orthogonal} (resp, \textbf{orthonormal}) set.
}

\theo{
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be an orthogonal basis for a subspace $V$ of \R{n}. Then for any $\Bf{v}\in V$, \[ \Bf{v} = \big(  \frac{\mathbf{v}\cdot \mathbf{u}_1}{  \parallel \mathbf{u}_1 \parallel^2 }  \big)\mathbf{u}_1 + \big(  \frac{\mathbf{v}\cdot \mathbf{u}_2}{  \parallel \mathbf{u}_2 \parallel^2 }  \big)\mathbf{u}_2 + \dots + \big(  \frac{\mathbf{v}\cdot \mathbf{u}_k}{  \parallel \mathbf{u}_k \parallel^2 }  \big)\mathbf{u}_k \]
If further $S$ is an \textbf{orthonormal basis}, then \[ \Bf{v}=(\Bf{v}\cdot\Bf{u}_{1})\Bf{u}_{1}+(\Bf{v}\cdot\Bf{u}_{2})\Bf{u}_{2}+\dots+(\Bf{v}\cdot\Bf{u}_{k})\Bf{u}_{k} \] that is, $S$ orthogonal, $[\Bf{v}]_{s}=\begin{pmatrix} \frac{\mathbf{v}\cdot \mathbf{u}_1}{  \parallel \mathbf{u}_1 \parallel^2 }  \\ \ \\ \frac{\mathbf{v}\cdot \mathbf{u}_2}{  \parallel \mathbf{u}_2 \parallel^2 }\\ \ \\ \vdots\\ \ \\ \frac{\mathbf{v}\cdot \mathbf{u}_k}{  \parallel \mathbf{u}_k \parallel^2 } \end{pmatrix} $, $S$ orthonormal, $[\Bf{v}]_{S}\begin{pmatrix} \mathbf{v}\cdot \mathbf{u}_1  \\ \mathbf{v}\cdot \mathbf{u}_2 \\ \vdots \\ \mathbf{v}\cdot \mathbf{u}_k  \end{pmatrix} $.\br Note that this only works if $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is an orthogonal or orthonormal basis.
}

\note{
Let $V$ be a subspace of \R{n} and $S$ an \textbf{orthonormal basis} of $V$. For any $\Bf{u},\Bf{v}\in V$, \begin{enumerate}
\item $\Bf{u}\cdot\Bf{v}=[\Bf{u}]_{S}\cdot[\Bf{v}]_{S}$
\item $\parallel \Bf{u-v} \parallel \;=\; \parallel [\Bf{u}]_{S}-[\Bf{v}]_{S} \parallel$
\end{enumerate}
}

\defn{
A $n\times n$ square matrix $\Bf{A}$ is \textbf{orthogonal} if $\Bf{A}^{T}=\Bf{A}^{-1}$, equivalently, $\Bf{A}^{T}\Bf{A}=\Bf{I}=\Bf{A}\Bf{A}^{T}$.
}

\theo{
Let \textbf{A} be a square matrix of order $n$. The following statements are equivalent.
\begin{enumerate}
\item \textbf{A} is an \textbf{orthogonal matrix}.
\item The \textbf{columns} of \textbf{A} form an \textbf{orthonormal basis} for \R{n}.
\item The \textbf{rows} of \textbf{A} form an \textbf{orthonormal basis} for \R{n}.
\end{enumerate}
}

\note{The term 'orthonormal matrix' is not used.}

\qn{
Let $W$ be a subspace of dimension 3. We can never find an orthonormal subset of $W$ containing 4 vectors. (\textbf{T})\br Orthonormal set is linearly independent and if $W$ contains a set of 4 linearly independent vectors, then $3=\text{dim}(W)\geq 4$, a contradiction. An orthonormal set is linearly independent. Also, if $U$ and $V$ are subspaces such that $U\subseteq V$, then $dim(U)\leq dim(V)$.
}

\qn{
Which is true regarding an orthogonal set $S$ containing 3 non-zero vectors in \R{3}?
\begin{enumerate}
\item The set $S$ must be linearly independent \true
\item $S$ is a basis for \R{3} \true
\item Each pair of vectors in $S$ are perpendicular to each other \true
\item The set $S$ must span \R{3} \true
\end{enumerate} 
Nonzero orthogonal vectors are perpendicular to each other, and is thus linearly independent.
}

\qn{
An orthogonal set must be linearly independent. \true \br Orthogonal set can contain the zero vector, which makes the set linearly dependent.
}

\qn{
Let $S=\left\{  \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \end{pmatrix}, \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \\ 0 \end{pmatrix} \right\}$ be a basis for a subspace $V$ in \R{3}. Let $\Bf{v}=\begin{pmatrix}3\\ 4\\ 0\end{pmatrix}$. What is the norm of $[\Bf{v}]_{S}$, $\parallel [\Bf{v}]_{S} \parallel$?\br $\sqrt{3^{2}+4^{2}}=5$. If $S$ is an orthonormal basis for $V$, then for any vector $v\in V$, $\parallel \Bf{v} \parallel \;=\; \parallel [\Bf{v}]_{S} \parallel$.
}

\qn{
A square matrix \textbf{A} of order $n$ is orthogonal if the columns or rows of \textbf{A} form an orthogonal basis for \R{n}. \false \br
The columns and/or columns need to form an orthonormal basis, not an orthogonal basis, in order for \textbf{A} to be orthogonal.
}

\note{
Let \textbf{A} be an orthogonal matrix of order $n$ and \textbf{u, v} be any two vectors in \R{n}. Then \br
1. $\parallel\Bf{u}\parallel=\parallel\Bf{Au}\parallel$\\
Proof: $(\Bf{Au})\cdot(\Bf{Av})=\Bf{u}^{T}\Bf{A}^{T}\Bf{Av}=\Bf{u}^{T}\Bf{v}=\Bf{u}\cdot\Bf{v}$. Hence $\parallel\Bf{Au}\parallel=\sqrt{(\Bf{Au})\cdot(\Bf{Au})}=\sqrt{\Bf{u}\cdot\Bf{u}}=\parallel\Bf{u}\parallel$\br
2. $d(\Bf{u,v})=d(\textbf{Au},\Bf{Av})$\\
Proof: $d(\Bf{Au},\Bf{Av})=\parallel\Bf{Au}-\Bf{Av}\parallel=\parallel\Bf{A}(\Bf{u-v})\parallel=\parallel\Bf{u-v}\parallel=d(\Bf{u,v})$\br
3. the angle between \textbf{u} and \textbf{v} is equal to the angle between \textbf{Au} and \textbf{Av}\\
Proof: $\frac{\Bf{Au}\cdot\Bf{Av}}{||\Bf{Au}||\;||\Bf{Av}||}=\frac{\Bf{u}\cdot\Bf{v}}{||\Bf{u}||\;||\Bf{v}||}$. Taking cosine on both sides will give the same angle.
}

\note{
Let \textbf{A} be an orthogonal matrix of order $n$. Let $S=\{\Bf{u}_{1},\Bf{u}_{2},\dots,\Bf{u}_{k}\}$ and define $T=\{\Bf{Au}_{1},\Bf{Au}_{2},\dots,\Bf{Au}_{k}\}$. \br
If $S$ is orthogonal, then $T$ is orthogonal.\\
Proof: If \textbf{A} is an orthogonal matrix, then $\Bf{Au}\cdot\Bf{Av}=\Bf{u}^{T}\Bf{A}^{T}\Bf{Av}=\Bf{u}\cdot\Bf{v}$. So for any two vectors in $T$, $\Bf{Au}_{i}\cdot\Bf{Au}_{j}=\Bf{u}_{i}\cdot\Bf{u}_{j}=0$ for $i\not=j$. So $T$ is an orthogonal set.\br
If $S$ is orthonormal, then $T$ is orthonormal. \\
Proof: $\Bf{Au}_{i}\cdot\Bf{Au}_{j}=\Bf{u}_{i}\cdot\Bf{u}_{j}$. So if $i=j$, $\Bf{Au}_{i}\cdot\Bf{Au}_{j}=\Bf{u}_{i}\cdot\Bf{u}_{j}=1$
}

\sectiontitle{5.3 Orthogonal Projection}

\theo{Orthogonal projection theorem\br
Let $V$ be a subspace of \R{n}. Every vector $\Bf{w}$ in \R{n} can be decomposed \textbf{uniquely} as a sum \[\Bf{w}=\Bf{w}_{p}+\Bf{w}_{n}\]where $\Bf{w}_{n}$ is orthogonal to $V$ and  $\Bf{w}_{p}$ is in $V$. Moreover, if $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is an \textbf{orthogonal basis} for $V$, then \[\Bf{w}_{p}=\frac{\Bf{w}\cdot\Bf{u}_{1}}{\Bf{u}_{1}\cdot\Bf{u}_{1}}\Bf{u}_{1}+\frac{\Bf{w}\cdot\Bf{u}_{2}}{\Bf{u}_{2}\cdot\Bf{u}_{2}}\Bf{u}_{2}+\dots+\frac{\Bf{w}\cdot\Bf{u}_{k}}{\Bf{u}_{k}\cdot\Bf{u}_{k}}\Bf{u}_{k}.\]
}

\defn{
Define the vector $\Bf{w}_{p}$ in the theorem above as the \textbf{orthogonal projection} (or just \textbf{projection}) of $\Bf{w}$ onto the subspace $V$.
}

\theo{Best Approximation Theorem\br
Let $V$ be a subspace of \R{n} and $\Bf{w}$ a vector in \R{n}. Let $\Bf{w}_{p}$ be the projection of $\Bf{w}$ onto $V$. Then $\Bf{w}_{p}$ is a vector in $V$ closest to $\Bf{w}$; that is, \[\parallel \Bf{w-w_{p}}\parallel \;\leq\; \parallel \Bf{w-v} \parallel\] for all $\Bf{v}$ in $V$.
}

\defn{Gram-Schmidt Orthogonalization\br
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a linearly independent set. Let \begin{flalign*}
&\Bf{v}_{1}=\Bf{u}_{1}\\
&\Bf{v}_{2}=\Bf{u}_{2}-\left(\frac{\Bf{v}_{1}\cdot\Bf{u}_{2}}{\parallel\Bf{v}_{1}\parallel^{2}}\right)\Bf{v}_{1}\\
&\Bf{v}_{3}=\Bf{u}_{3}-\left(\frac{\Bf{v}_{1}\cdot\Bf{u}_{3}}{\parallel\Bf{v}_{1}\parallel^{2}}\right)\Bf{v}_{1}-\left(\frac{\Bf{v}_{2}\cdot\Bf{u}_{3}}{\parallel\Bf{v}_{2}\parallel^{2}}\right)\Bf{v}_{2}\\
&\q\vdots\\
&\Bf{v}_{3}=\Bf{u}_{3}-\left(\frac{\Bf{v}_{1}\cdot\Bf{u}_{k}}{\parallel\Bf{v}_{1}\parallel^{2}}\right)\Bf{v}_{1}-\left(\frac{\Bf{v}_{2}\cdot\Bf{u}_{k}}{\parallel\Bf{v}_{2}\parallel^{2}}\right)\Bf{v}_{2}-\dots-\left(\frac{\Bf{v}_{k-1}\cdot\Bf{u}_{k}}{\parallel\Bf{v}_{k-1}\parallel^{2}}\right)\Bf{v}_{k-1}\\
\end{flalign*}

Then $V=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{k}\}$ is an \textbf{orthogonal} set (of nonzero vectors), and hence, \[ \left\{ \frac{\Bf{v}_{ 1 }}{\parallel \Bf{v}_{1} \parallel},\frac{\Bf{v}_{ 2 }}{\parallel \Bf{v}_{2} \parallel},\dots,\frac{\Bf{v}_{ k }}{\parallel \Bf{v}_{k} \parallel} \right\} \] is an \textbf{orthonormal set} such that $\text{span}\{\Bf{v}_{1},\dots,\Bf{v}_{k}\}=\text{span}\{\Bf{u}_{1},\dots,\Bf{u}_{k}\}$. 
}

\sectiontitle{5.4 QR Factorization}

\defn{
Suppose now \textbf{A} is a $m\times n$ matrix with linearly independent columns, i.e. $rank(\Bf{A})=n$. Write \[ \Bf{A}=(\Bf{a}_{1}\q\Bf{a}_{2}\q\dots\q\Bf{a}_{n}). \] Since the set $S=\{\mathbf{a}_{1},\mathbf{a}_{2},\dots,\mathbf{a}_{n}\}$ is \textbf{linearly independent} we may apply the \textbf{Gram-Schmidt process} on $S$ to obtain an \textbf{orthonormal set} $\{\mathbf{q}_{1},\mathbf{q}_{2},\dots,\mathbf{q}_{n}\}$. Set \[ \Bf{Q}=(\Bf{q}_{1}\q\Bf{q}_{2}\q\dots\q\Bf{q}_{n}). \] Recall that for any $j=1,2,\dots,n$, $\text{span}\{\Bf{a}_{1},\Bf{a}_{2},\dots,\Bf{a}_{j}\}=\text{span}\{\Bf{q}_{1},\Bf{q}_{2},\dots,\Bf{q}_{j}\}$. In particular, $\Bf{a}_{j}$ is in $\text{span}\{\Bf{q}_{1},\Bf{q}_{2},\dots,\Bf{q}_{j}\}$. Thus we may write \[ \Bf{a}_{j}= r_{1j}\Bf{q}_{1}+r_{2j}\Bf{q}_{2}+\dots+r_{jj}\Bf{q}_{j}+0\Bf{q}_{j+1}+\dots+0\Bf{q}_{n} = (\Bf{q}_{1}\q\dots\q\Bf{q}_{j}\q\dots\q\Bf{q}_{n}) \begin{pmatrix}r_{1j} \\ \vdots \\ r_{jj} \\ 0 \\ \vdots \\ 0 \end{pmatrix} \]
Explicitly,
\begin{flalign*}
& \Bf{a}_{1}=r_{11}\Bf{q}_{1}= (\Bf{q}_{1}\q\dots\q\Bf{q}_{n}) \begin{pmatrix}r_{11} \\ 0 \\ \vdots \\ 0 \end{pmatrix} \\
& \Bf{a}_{2}=r_{12}\Bf{q}_{1} + r_{22}\Bf{q}_{2} = (\Bf{q}_{1}\q\dots\q\Bf{q}_{n}) \begin{pmatrix}r_{12} \\ r_{22} \\ \vdots \\ 0 \end{pmatrix} \\
& \q\vdots \\
& \Bf{a}_{n}=r_{1n}\Bf{q}_{1} + r_{2n}\Bf{q}_{2} + \dots + r_{nn}\Bf{q}_{n} = (\Bf{q}_{1}\q\dots\q\Bf{q}_{n}) \begin{pmatrix}r_{1n} \\ r_{2n} \\ \vdots \\ r_{nn} \end{pmatrix} \\
\end{flalign*}
Thus, we may write
\begin{flalign*}
A &= (\Bf{a}_{1}\q\Bf{a}_{2}\q\dots\q\Bf{a}_{n}) \\
&= (\Bf{q}_{1}\q\Bf{q}_{2}\q\dots\q\Bf{q}_{n}) \begin{pmatrix} r_{11} & r_{12} & \dots & r_{1n}\\ 0 & r_{22} & \dots & r_{2n}\\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & r_{nn} \end{pmatrix} \\
&= \Bf{QR}\\
\end{flalign*} 
for some $m\times n$ matrix \textbf{Q} with \textbf{orthonormal columns}, and a \textbf{upper triangular} $n\times n$  matrix \textbf{R}.
}

\note{
$\Bf{Q}^{T}\textbf{Q}=\Bf{I}_{n}$.\br
The diagonal entries of \textbf{R} are positive, $r_{ii}>0$ for all $i=1,2,\dots,n$.\br
The upper triangular matrix \textbf{R} is invertible.
}

\theo{(QR Factorization)\br
Suppose \textbf{A} is a $m\times n$ matrix with \textbf{linearly independent} columns. Then \textbf{A} can be written as \textbf{A = QR} for some $m\times n$ matrix \textbf{Q} such that $\Bf{Q}^{T}\textbf{Q}=\Bf{I}_{n}$ and \textbf{invertible} upper triangular matrix \textbf{R} with \textbf{positive} diagonal entries.
}

\defn{The decomposition given in the theorem above is called a \textbf{QR factorization} of \textbf{A}.}

\algo{to QR Factorization}{
Let \textbf{A} be a $m\times n$ matrix with \textbf{linearly independent} columns.
\begin{enumerate}
\item Perform Gram-Schmidt on the columns of $\Bf{A} = (\Bf{a}_{1}\q\Bf{a}_{2}\q\dots\q\Bf{a}_{n})$ to obtain an \textbf{orthonormal set} $\{\mathbf{q}_{1},\mathbf{q}_{2},\dots,\mathbf{q}_{n}\}$.
\item Set $\Bf{Q}=\{\mathbf{q}_{1},\mathbf{q}_{2},\dots,\mathbf{q}_{n}\}$.
\item Compute $\Bf{R}=\Bf{Q}^{T}\Bf{A}$. 
\end{enumerate}
}

\corr{
Suppose \textbf{A} is a $m\times n$ matrix with \textbf{linearly independent} columns, i.e. $rank(\Bf{A})=n$. Then $\Bf{A}^{T}\Bf{A}$ is invertible, and \textbf{A} has a \textbf{left inverse}; that is, there is a \textbf{B} such that $\Bf{BA}=\Bf{I}_{n}$
}

\sectiontitle{5.5 Least Square Approximation}

\defn{
Let \textbf{A} be a $m\times n$ matrix and \textbf{b} a vector in \R{m}. A vector \textbf{u} in \R{n} is a \textbf{least square solution} of \textbf{Ax = b} if for every vector $\Bf{v}\in\R{n}$, \[ \parallel \Bf{Au-b} \parallel\; \leq\; \parallel \Bf{Av-b} \parallel  \]
}

\theo{Let \textbf{A} be a $m\times n$ matrix and \textbf{b} a vector in \R{m}. A vector \textbf{u} in \R{n} is a \textbf{least square solution} to \textbf{Ax = b} if and only if \textbf{u} is the \textbf{projection} of \textbf{b} onto the column space of \textbf{A}, $Col(A)$.
}

\theo{Let \textbf{A} be a $m\times n$ matrix and \textbf{b} a vector in \R{m}. A vector \textbf{u} in \R{n} is a \textbf{least square solution} to \textbf{Ax = b} if and only if \textbf{u} is a solution to $\Bf{A}^{T}\Bf{Ax=A}^{T}\Bf{b}$.
}

\note{Least square solutions are not unique, but projection is unique.}

\note{
Let \textbf{A} be a $m\times n$ matrix and \textbf{b} a vector in \R{n}. For any choice of \textbf{least square solution u}, that is, for any solution \textbf{u} of $\Bf{A}^{T}\Bf{Ax=A}^{T}\Bf{b}$., the projection \textbf{Au} is unique.
}

\theo{Let \textbf{V} be a subspace of \R{n} and $S = \{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{k}\}$ be a \textbf{spanning set} for $V$.  Set $\Bf{A} = (\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{k})$. Let \textbf{w} be a vector in \R{n}, and \textbf{u} be a least square solution to \textbf{Ax = w}. Then $\Bf{w}_{p}=\Bf{Au}$ is the \textbf{orthogonal projection} of a vector \textbf{w} onto $V$.\br In particular, if $S = \{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{k}\}$ is a \textbf{basis} for $V$, then the \textbf{orthogonal projection} of a vector \textbf{w} onto $V$ is \[ \Bf{w}_{p}=\Bf{A}(\Bf{A}^{T}\Bf{A})^{-1}\Bf{A}^{T}\Bf{w}. \]
}

\note{For any $m\times n$ matrix \textbf{A} and $\Bf{b}\in$ \R{n}, $\Bf{A}^{T}\Bf{Ax=A}^{T}\Bf{b}$ is always consistent. (The rank of $\Bf{A}^{T}\Bf{A}$ is always equal to ($\Bf{A}^{T}\Bf{A}\q\Bf{A}^{T}\Bf{b}$))}

\qn{
Let \textbf{u}
 be a solution to $\Bf{A}^{T}\Bf{Ax=A}^{T}\Bf{b}$
. Which of the following statement is false?
\begin{itemize}
\item \textbf{u} is a least square solution to \textbf{Ax = b} \true
\item \textbf{Au - b} is orthogonal to the column space of \textbf{A} \true 
\item \textbf{u} is the unique solution to $\Bf{A}^{T}\Bf{Ax=A}^{T}\Bf{b}$ \false
\item For any vector $\Bf{v}\in$ \R{n}, $\parallel \Bf{Au-b} \parallel\; \leq\; \parallel \Bf{Av-b} \parallel$ \true
\item \textbf{Au} is the projection of \textbf{b} onto the column space of \textbf{A} \true 
\end{itemize}
}

\qn{If \textbf{u} is a solution to \textbf{Ax = b}, then \textbf{u} is a least square solution to \textbf{Ax = b}. \true}

\qn{
Suppose $rank(\Bf{A})=$ number of columns of \textbf{A}. Which of the statements is always true?
\begin{itemize}
\item $\Bf{A}(\Bf{A}^{T}\Bf{A})^{-1}\Bf{A}^{T}\Bf{b}$ is the unique least square solution to \textbf{Ax = b} \false
\item $(\Bf{A}^{T}\Bf{A})^{-1}\Bf{A}^{T}\Bf{b}$ is the unique least square solution to \textbf{Ax = b} \true
\item $(\Bf{A}^{T}\Bf{A})^{-1}\Bf{A}^{T}\Bf{b}$ is the unique solution to \textbf{Ax = b} \false
\end{itemize}
Full rank implies $\Bf{A}^{T}\Bf{A}$ is invertible. 
}

%\end{comment}
%:[HIDE] Ch5
%:[HIDE] Ch6
%\begin{comment}

\chapter{6 Eigenanalysis}
\sectiontitle{6.1 Eigenvalues and Eigenvectors}

\defn{
Let \textbf{A} be a \textbf{square} matrix of order $n$. A real number $\lambda$ is an \textbf{eigenvalue} of \textbf{A} if there is a \textbf{nonzero} vector \textbf{v} in \R{n}, $\Bf{v}\not=0$, such that \[\Bf{Av}=\lambda\Bf{v}.\] In this case, the nonzero vector \textbf{v} is called an \textbf{eigenvector} associated to $\lambda$. Let \textbf{A} be a \textbf{square} matrix of order $n$, the \textbf{characteristic polynomial} of \textbf{A}, denoted as $\text{char}(\Bf{A})$, is the degree $n$ polynomial \[ \text{det}(x\Bf{I}-\Bf{A}).\]
}

\theo{
Let \textbf{A} be a \textbf{square} matrix of order $n$. $\lambda\in$ \R{n} is an \textbf{eigenvalue} of \textbf{A} if and only if the homogeneous system $(\lambda\Bf{I}-\Bf{Ax=0})$ has \textbf{nontrivial} solutions.}

\theo{Let \textbf{A} be a \textbf{square} matrix of order $n$. $\lambda$ is an \textbf{eigenvalue} of \textbf{A} if and only if $\lambda$ is a \textbf{root} of the \textbf{characteristic polynomial} $\text{det}(x\Bf{I}-\Bf{A}).$.
}

\theo{(Equivalent statements for invertibility)\br 14. A \textbf{square} matrix \textbf{A} is \textbf{invertible} if and only if $\lambda=0$ is \textbf{not} an \textbf{eigenvalue} of \textbf{A}.} 

\defn{
Let $\lambda$ be an eigenvalue of \textbf{A}. The \textbf{algebraic multiplicity} of $\lambda$ is the \textbf{largest integer} $r_{\lambda}$ such that \[\text{det}(x\Bf{I}-\Bf{A})=(x-\lambda)^{r_{\lambda}}p(x)\] for some polynomial $p(x)$. Alternatively, $r_{\lambda}$ is the \textbf{positive integer} such that in the above equation, $\lambda$ is \textbf{not a root} of $p(x)$. Suppose \textbf{A} is an order $n$ square matrix such that $\text{det}(x\Bf{I}-\Bf{A})$ can be \textbf{factorized} into \textbf{linear factors completely}.\br Then we can write \[\text{det}(x\Bf{I}-\Bf{A})=(x-\lambda_{1})^{r_{1}}(x-\lambda_{2})^{r_{2}}\dots(x-\lambda_{k})^{r_{1k}}\] where $r_{1}+r_{2}+\dots+r_{k}=n$, and $\lambda,\lambda_{2},\dots,\lambda_{k}$ are the \textbf{distinct eigenvalues} of \textbf{A}.\br Then the \textbf{algebraic multiplicity} of $\lambda_{i}$ is $r_{i}$ for $i=1,\dots,k$.
}

\theo{The \textbf{eigenvalues} of a \textbf{triangular matrix} are the \textbf{diagonal entries}. The \textbf{algebraic multiplicity} of the eigenvalue is the number of times it appears as a diagonal entry of \textbf{A}.}

\defn{
The \textbf{eigenspace} associated to an eigenvalue $\lambda$ of \textbf{A} is \[E_{\lambda}=\{\Bf{v}\in\mathbb{R}^{n}\;|\;\Bf{Av}=\Bf{\lambda v}\}=Null(\Bf{\lambda I-A}).\] The \textbf{geometric multiplicity} of an eigenvalue $\lambda$ is the \textbf{dimension} of its associated eigenspace, \[dim(E_{\lambda})=nullity(\Bf{\lambda I-A})\]
}

\note{
If \textbf{A} and \textbf{B} are row equivalent order $n$ square matrices, if $\lambda$ is an eigenvalue of \textbf{A}, it is not guaranteed to be an eigenvalue of \textbf{B}. If \textbf{v} is an eigenvector of \textbf{A}, it is not guaranteed to be an eigenvector of \textbf{B}.\br This is because row operations affect the determinant of the matrix, so eigenvalues and eigenvectors are not preserved. 
}

\note{
Let \textbf{A} be a $n\times n$ matrix. The characteristic polynomial of \textbf{A} is equal to the characteristic polynomial of $\Bf{A}^{T}$. Hence \textbf{A} and $\Bf{A}^{T}$ has the same eigenvalues.\br Let $\lambda$ be an eigenvalue of \textbf{A}. The geometric multiplicity of $\lambda$ as an eigenvalue of \textbf{A} is equal to its geometric multiplicity as an eigenvalue of $\Bf{A}^{T}$.
}

\note{Let \textbf{A} be a matrix such that $\Bf{A}^{T}=-\Bf{A}$. \br(a) If $\lambda$ is an eigenvalue of \textbf{A}, then $\lambda = 0$.\br
$\Bf{A}^{T}\Bf{v}=-\Bf{Av}\Rightarrow \text{ (Take transpose of both sides) } \Bf{v}^{T}\Bf{A}=-\Bf{v}^{T}\Bf{A}^{T}\Rightarrow \text{ (Post multiply both sides by \textbf{v}) } \Bf{v}^{T}\Bf{Av}=-\Bf{v}^{T}\Bf{A}^{T}\Bf{v} \Rightarrow \Bf{v}^{T}(\lambda\Bf{v})=-\Bf{v}^{T}(\lambda\Bf{v})\Rightarrow \lambda\parallel\Bf{v}\parallel^{2}=-\lambda\parallel\Bf{v}\parallel^{2}\Rightarrow \lambda = 0$ \br
(b) \textbf{A} is diagonalizable iff \textbf{A = 0}.\br
($\Rightarrow$) If \textbf{A} is diagonalizable, $\Bf{A}=\Bf{PDP}^{-1}=\Bf{P0P}^{-1}=\Bf{0}$ since eigenvalues of \textbf{A} can only be 0. \\
($\Leftarrow$) If \textbf{A = 0}, then $\Bf{A}=\Bf{I0I}^{-1}$, so \textbf{A} is diagonalizable.
}

\note{
$\lambda$ is an eigenvalue of \textbf{A} iff it is an eigenvalue of $\Bf{A}^{T}$. \\
Proof: $\text{det(}x\Bf{I-A}\text{)}=\text{det(}(x\Bf{I-A})^{T}\text{)}=\text{det(}(x\Bf{I})^{T}-\Bf{A}^{T})=\text{det}(x\Bf{I}-\Bf{A}^{T})$\br

If \textbf{A} is diagonalizable, then $\Bf{A}^{T}$ is diagonalizable. \\ 
Proof: $\Bf{A=PDP}^{-1}\Rightarrow\Bf{A}^{T}=(\Bf{PDP}^{-1})^{T}=(\Bf{P}^{-1})^{T}\Bf{D}^{T}\Bf{P}^{T}=\Bf{QDQ}^{-1}$\br

If \textbf{v} is an eigenvector of \textbf{A} associated to eigenvalue $\lambda$, then \textbf{v} is an eigenvector of $\Bf{A}^{k}$ associated to eigenvalue $\lambda^{k}$ for any $k\in\mathbb{Z}^{+}$.\\
Proof: $\Bf{A}^{k}\Bf{v}=\Bf{A}^{k-1}\Bf{Av}=\lambda\Bf{A}^{k-2}\Bf{Av}=\dots=\lambda^{k-1}\Bf{Av}=\lambda^{k}\Bf{v}$. Since $\Bf{v}\not=\Bf{0}$, $\lambda^{k}$ is an eigenvalue of $\Bf{A}^{k}$.\br

If \textbf{A} is invertible, \textbf{v} is an eigenvector of $\Bf{A}^{k}$ associated to eigenvalue $\lambda^{k}$ for any negative integer $k$.\\
Proof: $\Bf{Av}=\lambda\Bf{v}\Rightarrow \text{ (premulitply by inverse) } \Bf{A}^{-1}\Bf{Av}=\Bf{A}^{-1}\lambda\Bf{v}\Rightarrow \Bf{v}=\lambda\Bf{A}^{-1}\Bf{v}\Rightarrow\frac{1}{\lambda}\Bf{v}=\Bf{A}^{-1}\Bf{v}$, so $\lambda^{-1}$ is an eigenvalue of $\Bf{A}^{-1}$. Then for any negative $k$, let $k=-m$, $\Bf{A}^{k}\Bf{v}=(\Bf{A}^{-1})^{m}\Bf{v}=\lambda^{-m}\Bf{v}$ \br

A square matrix is \emph{nilpontent} iff there is a positive integer $k$ such that $\Bf{A}^{k}=\Bf{0}$. If \textbf{A} is nilpotent, then 0 is the only eigenvalue.\\
Proof: If $\Bf{A}^{k}=\Bf{0}$ then $\Bf{A}^{k}\Bf{v}=\Bf{0v}=\Bf{0}$. Since \textbf{v} is a non-zero vector, the only eigenvalue is 0.\br

Let $\Bf{A}_{n\times n}$ have only one eigenvalue $\lambda$ with algebraic multiplicity $n$. Then \textbf{A} is diagonalizable iff \textbf{A} is a scalar matrix, $\Bf{A}=\lambda\Bf{I}$.\\
Proof: If \textbf{A} is diagonalizable, then $\Bf{A}=\Bf{PDP}^{-1}$, and the entries of \textbf{D} are all $\lambda$, i.e. $\Bf{D}=\lambda\Bf{I}$. Hence $\Bf{A}=\Bf{P}\lambda\Bf{I}\Bf{P}^{-1}=\lambda\Bf{PIP}^{-1}=\lambda\Bf{I}$.\br

The only diagonalizable nilpotent matrix is the zero matrix. \\
Proof: Let \textbf{A} be a nilpotent matrix. Then 0 is the only eigenvalue. If \textbf{A} is diagonalizable, then $\Bf{A}=\Bf{P}\text{diag}(0,0,\dots,0)\Bf{P}^{-1}=\Bf{0}$.
}

\note{
If \textbf{A} and \textbf{B} are similar matrices, i.e. $\Bf{A}=\Bf{PBP}^{-1}$ for some invertible \textbf{P}, then the characteristic polynomials of \textbf{A} and \textbf{B} are equal.\br
Proof: $\text{det}(x\Bf{I-A})=\text{det}(\Bf{P})\text{det}(\Bf{P})^{-1}\text{det}(x\Bf{I-A})=\text{det}(\Bf{P})\text{det}(x\Bf{I-A})\text{det}(\Bf{P})^{-1}=\text{det}(\Bf{P}(x\Bf{I-A})\Bf{P}^{-1})=\text{det}(x\Bf{PIP}^{-1}-\Bf{PAP}^{-1})=\text{det}(x\Bf{I-B})$
}

\note{If the characteristic polynomial of \textbf{A} and \textbf{B} are equal, we cannot conclude that \textbf{A} and \textbf{B} are similar matrices.\br
E.g. [1 1; 0 1] and [1 0; 0 1] have the same characteristic polynomial $(x-1)^{2}$ but for any invertible matrix \textbf{P}, $\Bf{PIP}^{-1}=\Bf{I}\not=$[1 1; 0 1]
}

\note{If \textbf{A} and \textbf{B} are $n\times n$ matrices with the same determinant, it is not true that their characteristic polynomials are equal. \\
E.g. A=[1 0; 0 0] and B=[2 0; 0 0] have the same determinant 0, but their characteristic polynomials are different.}

\note{
If \textbf{A} is a $2\times 2$ matrix, its characteristic polynomial is $x^{2}-tr(\Bf{A})+\text{det}(\Bf{A})$ where $tr(\Bf{A})$ is the sum of diagonal entries of \textbf{A}.\br
Proof: Write \textbf{A}=[a b; c d]. Then $tr(\Bf{A})=a+d$ and det(\textbf{A}) = $ad-bc$. $\text{det}(x\Bf{I-A})=(x-a)(x-d)-bc=x^{2}-(a+d)x+ad-bc=x^{2}-tr(\Bf{A})x+\text{det}(\Bf{A})$\br

If \textbf{A} is an $n\times n$ matrix and $p(x)=x^{n}+a_{n-1}x^{n-1}+\dots+a_{1}x+a_{0}$, then $a_{0}=p(0)=\text{det}(0\Bf{I-A})=\text{det}(-\Bf{A})=(-1)^{n}\text{det}(\Bf{A})$
}

\note{
If \textbf{AB = BA}, and \textbf{v} is an eigenvector of \textbf{B}, then \textbf{Av} is also an eigenvector of \textbf{B}.\br
Proof: $\Bf{Bv}=\lambda\Bf{v}\Rightarrow \Bf{ABv}=\Bf{B(Av)}=\lambda(\Bf{Av})$
}

\sectiontitle{6.2 Diagonalizaton}

\defn{
A square matrix \textbf{A} of order $n$ is \textbf{diagonalizable} if there exists an \textbf{invertible} matrix \textbf{P} such that \[\Bf{P}^{-1}\Bf{AP}=\Bf{D}\] is a \textbf{diagonal} matrix, OR \[\Bf{A}=\Bf{P}\Bf{DP}^{-1}\]
}

\defn{
A $n\times n$ square matrix \textbf{A} is \textbf{diagonalizable} if and only if \textbf{A} has $n$ \textbf{linearly independent eigenvectors}. That is, \textbf{A} is \textbf{diagonalizable} if and only if \[ \Bf{P}=(\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{n}),\text{ and } \Bf{D}=\begin{pmatrix} \mu_{1} & 0 & \dots & 0\\ 0 & \mu_{2} & \dots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & \mu_{n} \end{pmatrix}, \] where $\mu_{i}$ is the \textbf{eigenvalue} associated to \textbf{eigenvector} $\Bf{u}_{i}$, $i=1,2,\dots,n$,$\Bf{Au}_{i}=\mu_{i}\Bf{u}_{i}$, and $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{n}\}$ is a \textbf{basis} for \R{n}. 
}

\theo{(Eigenspaces are linearly independent)\br
Let \textbf{A} be a $n\times n$ \textbf{square} matrix. Let $\lambda_{1}$  and $\lambda_{2}$ be \textbf{distinct eigenvalues} of \textbf{A}, $\lambda_{1}\not=\lambda_{2}$. Suppose $\{\mathbf{u}_{1},\dots,\mathbf{u}_{k}\}$ is a \textbf{linearly independent} subset of eigenspace associated to eigenvalue $\lambda_{1}$, and $\{\mathbf{v}_{1},\dots,\mathbf{v}_{m}\}$ is a \textbf{linearly independent} subset of of eigenspace associated to eigenvalue $\lambda_{2}$. Then the union $\{\mathbf{u}_{1},\dots,\mathbf{u}_{k},\mathbf{v}_{1},\dots,\mathbf{v}_{m}\}$ is \textbf{linearly independent}.
 }
 
\theo{(Geometric Multiplicity is no greater than Algebraic multiplicity)\br
The \textbf{geometric multiplicity} of an \textbf{eigenvalue} $\lambda$  of a square matrix \textbf{A} is \textbf{no greater} than the \textbf{algebraic multiplicity}, that is,\[1\leq \text{dim}(E_{\lambda})\leq r_{\lambda}\]
}

\theo{(Equivalent Statements for Diagonalizability)\br
Let \textbf{A} be a square matrix of order $n$. The following statements are equivalent.
\begin{enumerate}
\item \textbf{A} is diagonalizable.
\item There exists a \textbf{basis} $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{n}\}$ of \R{n} of \textbf{eigenvectors} of \textbf{A}.
\item The \textbf{characteristic polynomial} of \textbf{A} \textbf{splits} into \textbf{linear factors}, \[ \text{det}(x\Bf{I}-\Bf{A})=(x-\lambda_{1})^{r_{\lambda_{1}}}(x-\lambda_{2})^{r_{\lambda_{2}}}\dots(x-\lambda_{k})^{r_{\lambda_{k}}} \] where $r_{\lambda_{i}}$ is the \textbf{algebraic multiplicity} of $\lambda_{i}$, for $i=1,\dots,k$, and the \textbf{eigenvalues} are \textbf{distinct}, $\lambda_{i}\not=\lambda_{j}$ for all $i\not=j$, and the \textbf{geometric multiplicity} is equal to the \textbf{algebraic multiplicity} for each eigenvalue $\lambda_{i}$, \[ \text{dim}(E_{\lambda_{i}})=r_{\lambda_{i}} \]
\end{enumerate}
}

\defn{
A \textbf{square} matrix \textbf{A}
 of order $n$
 is \textbf{not diagonalizable} if either
\begin{enumerate}
\item the characteristic polynomial $ \text{det}(x\Bf{I}-\Bf{A}) $ does not split into linear factors, or
\item there exists an eigenvalue $\lambda$ such that $\text{dim}(E_{\lambda}) < r_{\lambda}$.
\end{enumerate}
}

\algo{to Diagonalization}{
Let \textbf{A}
 be an order $n$
 square matrix.
\begin{enumerate}
\item Compute the characteristic polynomial. \textbf{If the characteristic polynomial does not split into linear factors, A is not diagonalizable.} Otherwise, write \[\text{det}(x\Bf{I}-\Bf{A})=(x-\lambda_{1})^{r_{\lambda_{1}}}(x-\lambda_{2})^{r_{\lambda_{2}}}\dots(x-\lambda_{k})^{r_{\lambda_{k}}}\]
\item For each eigenvalue $\lambda_{i}$ of \textbf{A}, $i=1,\dots,k$, find a basis $S_{\lambda_{i}}$  for the eigenspace, that is, find a basis $S_{\lambda_{i}}$  for the solution space of the following linear system, \[(\lambda_{i}\Bf{I}-\Bf{A})\Bf{x=0}\]
\textbf{Compute first the eigenspace associated to eigenvalues with algebraic multiplicity greater than 1. If} $\text{dim}(E_{\lambda_{i}}) < r_{\lambda}$ \textbf{, A is not diagonalizable.}
\item Let $S=\bigcup_{i=1}^{k}{S_{\lambda_{i}}} $. Then $S = \{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{n}\}$ is a basis for \R{n} consisting of eigenvectors of \textbf{A}.
\item Let $\Bf{P} = (\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{n})$, and $\Bf{D}=diag(\mu_{1},\mu_{2},\dots,\mu_{n})$, where $\mu_{i}$ is the eigenvalue associated to $\Bf{u}_{i}$, $i=1,\dots,n$, \[\Bf{Au}_{i}=\mu_{i}\Bf{u}_{i}\] 
Then \[ \Bf{A} = \Bf{PDP}^{-1} = (\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{n}) \begin{pmatrix} \mu_{1} & 0 & \dots & 0\\ 0 & \mu_{2} & \dots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & \mu_{n} \end{pmatrix} (\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{n})^{-1}
\]
\end{enumerate}
}

\qn{
Suppose \textbf{A} is diagonalizable. Which of the following statement(s) is/are true?\br
If the diagonal matrix \textbf{D} is fixed, then the invertible matrix \textbf{P} is fixed. \false \br
If the invertible matrix \textbf{P} is fixed, then the diagonal matrix \textbf{D} is fixed. \true
}

\note{If \textbf{A} is diagonalizable, then there exists a \textbf{B} such that $\Bf{B}^{3}=\Bf{A}$.\br
Proof: $\Bf{A}=\Bf{PDP}^{-1}$. Let \textbf{Q} be the diagonal matrix \textbf{D} but with all the entries cube rooted. Then $\Bf{A}=\Bf{PQ}^{3}\Bf{P}^{-1}=\Bf{B}^{3}$, $\Bf{B}=\Bf{PQP}^{-1}$
}

\sectiontitle{6.3 Orthogonally Diagonalizable}

\defn{An order $n$ square matrix \textbf{A} is \textbf{orthogonally diagonalizable} if \[\Bf{A}=\Bf{PDP}^{T}\]for some \textbf{orthogonal matrix} \textbf{P} and \textbf{diagonal} matrix \textbf{D}.}

\theo{(The Spectral Theorem)\br
Let \textbf{A} be a $n\times n$ square matrix. \textbf{A} is \textbf{orthgonally diagonalizable} if and only if \textbf{A} is \textbf{symmetric}.}

\theo{(Equivalent statements for orthogonally diagonalizable)\br
Let \textbf{A} be a square matrix of order $n$. The following statements are equivalent.
\begin{enumerate}
\item \textbf{A} is orthogonally diagonalizable.
\item There exists an orthonormal basis $\{\Bf{u}_{1},\Bf{u}_{2},\dots,\Bf{u}_{n}\}$ of \R{n} of \textbf{eigenvectors} of \textbf{A}.
\item \textbf{A} is a \textbf{symmetric} matrix.
\end{enumerate}
\textbf{A} is \textbf{orthogonally diagonalizable} if and only if
\[\Bf{P}=(\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{n}),\text{ and }\Bf{D}=\begin{pmatrix} \mu_{1} & 0 & \dots & 0\\ 0 & \mu_{2} & \dots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & \mu_{n} \end{pmatrix} \] where $\mu_{i}$ is the \textbf{eigenvalue} associated to \textbf{eigenvector} $\Bf{u}_{i}$, $i=1,\dots,n$ , $\Bf{Au}_{i}=\mu_{i}\Bf{u}_{i}$, and $\{\Bf{u}_{1},\Bf{u}_{2},\dots,\Bf{u}_{n}\}$ is an \textbf{orthonormal basis} for \R{n}.
}

\theo{(Eigenspaces of a symmetric matrix is orthogonal)\br
If \textbf{A} is a \textbf{symmetric} matrix, then the \textbf{eigenspaces} are \textbf{orthogonal} to each other. That is, suppose $\lambda_{1}$ and $\lambda_{2}$ are \textbf{distinct eigenvalues} of a \textbf{symmetric matrix A}, $\lambda_{2}\not=\lambda_{2}$, and $\Bf{v}_{i}$ is an eigenvector associated to eigenvalue $\lambda_{i}$, for $i=1,2$. Then $\Bf{v}_{1}\cdot\Bf{v}_{2}=0$.
}

\algo{to Orthogonal Diagonalization}{
Let \textbf{A}
 be an order $n$
 symmetric matrix. Since \textbf{A}
 is symmetric, it is orthogonally diagonalizable.
\begin{enumerate}
\item Compute the characteristic polynomial \[\text{det}(x\Bf{I}-\Bf{A})=(x-\lambda_{1})^{r_{\lambda_{1}}}(x-\lambda_{2})^{r_{\lambda_{2}}}\dots(x-\lambda_{k})^{r_{\lambda_{k}}}\]
\item For each eigenvalue $\lambda_{i}$ of \textbf{A}, $i=1,\dots,k$, find a basis $S_{\lambda_{i}}$ for the eigenspace, that is, find a basis $S_{\lambda_{i}}$ for the solution space of the following linear system, \[(\lambda_{i}\Bf{I}-\Bf{A})\Bf{x}=0.\]
\item Apply Gram-Schmidt process to each basis $S_{\lambda_{i}}$ of the eigenspace $E_{\lambda_{i}}$ to obtain an orthonormal basis $T_{\lambda_{i}}$. Let $T=\bigcup_{i=1}^{k}{T_{\lambda_{i}}} $. Then $T=\{\Bf{u}_{1},\Bf{u}_{2},\dots,\Bf{u}_{n}\}$ is an orthonormal basis for \R{n}.
\item Let $\Bf{P}=(\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{n})$, and $\Bf{D}=diag(\mu_{1},\mu_{2},\dots,\mu_{n})$, where $\mu_{i}$ is the eigenvalue associated to $\Bf{u}_{i}$, $i=1,\dots,n$, $\Bf{Au}_{i}=\mu_{i}\Bf{u}_{i}$. Then \textbf{P} is an orthogonal matrix, and \[\Bf{A}=\Bf{PDP}^{T}=(\Bf{u}_{1}\q\Bf{u}_{2}\q\dots\q\Bf{u}_{n})\begin{pmatrix} \mu_{1} & 0 & \cdots & 0\\ 0 & \mu_{2} & \cdots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & \mu_{n} \end{pmatrix}\begin{pmatrix}\Bf{u}_{1}^{T}\\ \Bf{u}_{2}^{T} \\ \vdots \\ \Bf{u}_{n}^{T} \end{pmatrix}.\]
\end{enumerate}
}

\sectiontitle{6.4 Application of Diagonalization: Markov Chain}

\theo{
Suppose $\Bf{A}=\Bf{PDP}^{-1}$. Then $\Bf{A}^{m}=\Bf{PD}^{m}\Bf{P}^{-1}$.
}

\theo{(Powers of diagonal matrices)\br
Let $\Bf{D}=\begin{pmatrix} d_{1} & 0 & \cdots & 0\\ 0 & d_{2} & \cdots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & d_{n} \end{pmatrix}$ be a diagonal matrix. Then for any positive integer $m$, $\Bf{D}^{m}=\begin{pmatrix} d_{1}^{m} & 0 & \cdots & 0\\ 0 & d_{2}^{m} & \cdots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & d_{n}^{m}\end{pmatrix}$.}

\corr{(Powers of diagonalizable matrices)\br
Suppose \textbf{A} is  \textbf{diagonalizable}. Write $\Bf{A}=\Bf{P}\begin{pmatrix} \mu_{1} & 0 & \cdots & 0\\ 0 & \mu_{2} & \cdots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & \mu_{n} \end{pmatrix}\Bf{P}^{-1}$. Then for any positive integer $k>0$, \[ \Bf{A}^{k} = \Bf{P}\begin{pmatrix} \mu_{1}^{k} & 0 & \cdots & 0\\ 0 & \mu_{2}^{k} & \cdots & 0\\ \vdots & \ & \ddots & \vdots \\ 0 & 0 & \dots & \mu_{n}^{k} \end{pmatrix}\Bf{P}^{-1}\] Moreover, if \textbf{A} is \textbf{invertible}, then the identity above holds for any integer $k\in\mathbb{Z}$.
}

\defn{
\begin{enumerate}
\item A vector $\Bf{v}=(v_{i})_{n}$ with \textbf{nonnegative} coordinates that add up to 1, $\sum_{i=1}^{n}v_{i}=1$, is called a \textbf{probability vector}.
\item A \textbf{stochastic matrix} is a square matrix whose columns are \textbf{probability vectors}.
\item A \textbf{Markov chain} is a sequence of \textbf{probability vectors} $\Bf{x}_{0},\Bf{x}_{1},\dots,\Bf{x}_{k},\dots$, together with a \textbf{stochastic matrix P} such that \[ \Bf{x}_{1}=\Bf{Px}_{0},\q\Bf{x}_{2}=\Bf{Px}_{1},\q\dots\q,\q\Bf{x}_{k}=\Bf{Px}_{k-1},\dots\]
\end{enumerate}
A \textbf{steady-state vector}, or \textbf{equilibrium vector} for a \textbf{stochastic matrix P} is a \textbf{probability vector} that is an \textbf{eigenvector} associated to eigenvalue 1. 
}

\theo{
Let \textbf{P} be a $n\times n$ \textbf{stochastic matrix} and \[ \Bf{x}_{0},\q\Bf{x}_{1}=\Bf{Px}_{0},\q\Bf{x}_{2}=\Bf{Px}_{1},\q\dots\q,\q\Bf{x}_{k}=\Bf{Px}_{k-1}\] be a \textbf{Markov chain} for some probability vector $\Bf{x}_{0}$. If the Markov chain \textbf{converges}, it will converge to an \textbf{equilibrium vector}. 
}

\defn{(Google PageRank Algorithm)\br
Suppose the set $S$ contains $n$ sites.\br We define the \textbf{adjacency matrix} for $S$ for be the order $n$ square matrix $\Bf{A}=(a_{ij})$ where \[ a_{ij}=\begin{cases} 1\q\text{ if site \emph{j} has an outgoing link to site \emph{i};} \\ 0\q\text{ if site \emph{j} does not have an outgoing link to site \emph{i}.}\end{cases} \] From the \textbf{adjacency matrix} \textbf{A}, we define the \textbf{probability transition matrix} $\Bf{P}=(p_{ij})$ by dividing each entry of \textbf{A} by the sum of the entries in the same column; that is \[ p_{ij}=\frac{a_{ij}}{\sum_{k=1}^{n}a_{kj}}\]
}

\defn{
A stochastic matrix is \textbf{regular} if for some positive integer $k>0$, the matrix power $\Bf{P}^{k}$ has positive entries, \[\Bf{P}^{k}=(a_{ij})_{n},\q a_{ij}>0 \text{ for all }i,j=1,\dots,n.\]
}

\theo{
Suppose \[ \Bf{x}_{1}=\Bf{Px}_{0},\q\Bf{x}_{2}=\Bf{Px}_{1},\q\dots\q,\q\Bf{x}_{k}=\Bf{Px}_{k-1},\dots\]is a Markov chain and \textbf{P} is a \textbf{regular stochastic matrix}. Then The Markov chain \textbf{will converge} to the \textbf{unique equilibrium vector}.
}

\algo{to Computing Equilibrium vector}{
Let \textbf{P} be a $n\times n$ stochastic matrix.
\begin{enumerate}
\item Find an eigenvector \textbf{u} associate to eigenvalue $\lambda=1$, that is, find a nontrivial solution to the homogeneous system  $\Bf{(I-P)x=0}$.
\item Write $\Bf{u}=(u_{i})$. Then
\end{enumerate}
\[ \Bf{v}=\frac{1}{\sum_{k=1}^{n}u_{k}}\Bf{u} \] will be an equilibrium vector. Indeed, the $i$-th coordinate of \textbf{v} is $\frac{u_{i}}{\sum_{k=1}^{n}u_{k}}$ and hence, the sum of the coordinates of \textbf{v} is \[\sum_{i=1}^{n}\frac{u_{i}}{\sum_{k=1}^{n}u_{k}}=\frac{\sum_{k=1}^{n}u_{i}}{\sum_{k=1}^{n}u_{k}}=1 \] Alternatively, the equilibrium eigenvectors are solutions to the equation \[ \begin{pmatrix} \Bf{P-I}_{n} \\ 1\q1\q\dots\q1 \end{pmatrix}\Bf{x}=\begin{pmatrix}0\\ \vdots \\ 0 \\ 1 \end{pmatrix}\] where $\Bf{I}_{n}$ is the $n\times n$ identity matrix. Here $\begin{pmatrix} \Bf{P-I}_{n} \\ 1\q1\q\dots\q1 \end{pmatrix}$ is the $(n+1)\times n$ matrix whose first $n$ rows are the matrix $\Bf{P-I}_{n}$, and the last row has all entries 1.
}

\sectiontitle{6.5 Application of Orthogonal Diagonalization: Singular Value Decomposition}

\theo{
(Singular value decomposition)\br
Let \textbf{A} be a $m\times n$ matrix. Then \[\Bf{A}=\Bf{U\Sigma V}^{T}\]where \textbf{U} is an order $m$ orthogonal matrix, \textbf{V} an order $n$ orthogonal matrix, and the matrix $\Bf{\Sigma}$ has the form \[\Bf{\Sigma}=\begin{pmatrix}\Bf{D} & \Bf{0}_{r\times(n-r)}\\ \Bf{0}_{(m-r)\times r} & \Bf{0}_{(m-r)\times(n-r)}\end{pmatrix}\] for some diagonal matrix \textbf{D} of order $r$, where $r\leq \text{min}\{m,n\}$.
}

\algo{to Singular Value Decomposition}{
Let \textbf{A} be a $m\times n$ matrix with $rank(\Bf{A})=r$.
\begin{enumerate}
\item The matrix $\Bf{A}^{T}\Bf{A}$ is a symmetric matrix, and is thus orthogonally diagonalizable. Find the eigenvalues of $\Bf{A}^{T}\Bf{A}$. Arrange the nonzero eigenvalues in descending order (counting multiplicity) \[ \mu_{1}\geq \mu_{2}\geq\dots\geq\mu_{r}>0=\mu_{r+1}=\dots=\mu_{n} \]
\item Let $\sigma_{i}=\sqrt{\mu_{i}},i=1,\dots,r$, \[ \sigma_{1}=\sqrt{\mu_{1}} \geq \sigma_{2}=\sqrt{\mu_{2}} \geq\dots\geq \sigma_{r}=\sqrt{\mu_{r}} \] These are the positive \textbf{singular values} of \textbf{A}. Set \[ \Bf{\Sigma} = \begin{pmatrix} \begin{matrix} \sigma_{1} & 0 & \cdots & 0\\ 0 & \sigma_{2} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \sigma_{r} \end{matrix}  & \Bf{0}_{r\times(n-r)} \\ \Bf{0}_{(m-r)\times r} & \Bf{0}_{(m-r)\times(n-r)} \end{pmatrix}\]
\item Proceed to find an orthonormal basis $\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{n}\}$ of \R{n} consisting of eigenvectors of $\Bf{A}^{T}\Bf{A}$ (section 6.3) such that $\Bf{v}_{i}$ is an eigenvector associated to $\mu_{i}$. Set \[\Bf{V}=(\mathbf{v}_{1}\q\mathbf{v}_{2}\q\dots\q\mathbf{v}_{n}).\]
\item Let $\Bf{u}_{i}=\frac{1}{\sigma_{i}}\Bf{Av}_{i}$ for $i=1,\dots,r$. Then the set $\{\mathbf{u}_{1},\dots,\mathbf{u}_{r}\}$ is an orthonormal basis for the column space of \textbf{A}. If $r=m$, set $\Bf{U}=(\mathbf{u}_{1}\q\mathbf{u}_{2}\q\dots\q\mathbf{u}_{r})$.\br 
Otherwise, extend $\{\mathbf{u}_{1},\dots,\mathbf{u}_{r}\}$ to an orthonormal basis for \R{m} as such. Find a basis for the solution space of \[ (\mathbf{u}_{1}\q\dots\q\mathbf{u}_{r})^{T}\Bf{x}=\Bf{0} \]\\ Perform Gram-Schmidt process on the basis found to obtain an orthonormal set $\{\mathbf{u}_{r+1},\dots,\mathbf{u}_{m}\}$. This set is an orthonormal basis for the orthogonal complement of the column space of \textbf{A}.\br
Then $\{\mathbf{u}_{1},\dots,\mathbf{u}_{r},\mathbf{u}_{r+1},\dots,\mathbf{u}_{m}\}$ is an orthonormal basis for \R{m}. Set \[\Bf{U}=(\mathbf{u}_{1}\q\mathbf{u}_{2}\q\dots\q\mathbf{u}_{m}).\]
\item Then \[ \Bf{A}=\Bf{U\Sigma V}^{T}=(\mathbf{u}_{1}\q\mathbf{u}_{2}\q\dots\q\mathbf{u}_{m})\begin{pmatrix} \begin{matrix} \sigma_{1} & 0 & \cdots & 0\\ 0 & \sigma_{2} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \sigma_{r} \end{matrix}  & \Bf{0}_{r\times(n-r)} \\ \Bf{0}_{(m-r)\times r} & \Bf{0}_{(m-r)\times(n-r)} \end{pmatrix}\begin{pmatrix}\Bf{v}_{1}^{T}\\ \Bf{v}_{2}^{T} \\ \vdots \\ \Bf{v}_{n}^{T} \end{pmatrix} \]
\end{enumerate}
}

\note{
\[\Bf{\Sigma}^{T}\Bf{\Sigma}=\begin{pmatrix}
\mu_{1} & 0 & \dots & 0 & \dots & 0 \\
0 & \mu_{2} & \dots & 0 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \mu_{r} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & 0 & \dots & \mu_{n} \\
\end{pmatrix}\] where $\mu_{i},i=1,\dots,n$ is the eigenvalues of $\Bf{A}^{T}\Bf{A}$; that is $\Bf{A}^{T}\Bf{A}=\Bf{P\Sigma}^{T}\Bf{\Sigma P}^{T}$, where $\Bf{P}=(\mathbf{v}_{1}\q\mathbf{v}_{2}\q\dots\q\mathbf{v}_{n})$ and $\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{n}\}$ be an orthonormal basis for \R{n} of eigenvectors of $\Bf{A}^{T}\Bf{A}$.
}

\note{
$\Bf{Av}_{i}\not=\Bf{0}$ for all $i\leq r$ and $\Bf{Av}_{i}=\Bf{0}$ for all $i>r$.
}

\qn{
$rank(\Bf{A})=n$ if and only if all the singular values of \textbf{A} are positive. \br
$rank(\Bf{A})=m$  if and only if all the singular values of $\Bf{A}^{T}$ are positive.
}

\note{
If \textbf{A} is a symmetric matrix, then the singular values of \textbf{A} are the absolute value of the eigenvalues of \textbf{A}.
}

%\end{comment}
%:[HIDE] Ch6
%:[HIDE] Ch7
%\begin{comment}

\chapter{7 Linear Transformation}

\sectiontitle{7.1 Introduction to Linear Transformation}

\defn{
A mapping (function) $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$, is a \textbf{linear transformation} if for all vectors \textbf{u, v} in \R{n}, and scalars $\alpha, \beta$
, \[ T(\alpha\Bf{u}+\beta\Bf{v})=\alpha T(\Bf{u})+\beta T(\Bf{v})\] The Euclidean space \R{n} is called the \textbf{domain} of the mapping, and the Euclidean space \R{m} is called the \textbf{codomain} of the mapping. \br Equivalently, a mapping $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$, is a \textbf{linear transformation} if it satisfies the following properties.
\begin{enumerate}
\item For any vector \textbf{u} in \R{n} and scalar $\alpha$, \[ T(\alpha \Bf{u})=\alpha T(\Bf{u}) \]
\item For any vectors \textbf{u, v} in \R{n}, \[ T(\Bf{u+v})=T(\Bf{u})+T(\Bf{v}) \]
\end{enumerate} 
By induction, we have that for any vectors $\Bf{u}_{1},\Bf{u}_{2}\dots,\Bf{u}_{k}$ in \R{n} and scalars $c_{1},c_{2},\dots,c_{k}$, \[ T(c_{1}\Bf{u}_{1}+c_{2}\Bf{u}_{2}+\dots+c_{k}\Bf{u}_{k}) = c_{1}T(\Bf{u}_{1})+ c_{2}T(\Bf{u}_{2})+\dots+ c_{k}T(\Bf{u}_{k}) \]
Any $m\times n$ matrix \textbf{A} defines a linear transformation $T_{\Bf{A}}:\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ by multiplication, \[ T_{\Bf{A}}(\Bf{u})=\Bf{Au} \text{ for any } \Bf{u}\in \mathbb{R}^{n} \]
A mapping  $\Bf{T}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is \textbf{not a linear transformation} if any of the following statements hold.
\begin{enumerate}
\item \textbf{T} does not map the zero vector to the zero vector, $\Bf{T(0)}\not=\Bf{0}$.
\item There is a scalar $\alpha$ and a vector \textbf{u} in \R{n} such that $\Bf{T}(\alpha\Bf{u})\not=\alpha\Bf{T(u)}$.
\item There are vectors \textbf{u, v} in \R{n} such that $\Bf{T(u+v)\not=T(u)+T(v)}$.
\end{enumerate}
}

\theo{(Standard matrix of linear transformation)\br
A mapping $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is a \textbf{linear transformation} if and only if there is a \textbf{unique} $m\times n$ matrix \textbf{A} such that \[ T(\Bf{u})=\Bf{Au}\q\text{for all vectors \textbf{u} in }\mathbb{R}^{n}.\]
The matrix \textbf{A} is given by \[ \Bf{A}=(T(\Bf{e}_{1})\q T(\Bf{e}_{2})\q\dots\q T(\Bf{e}_{n}))\] where $E=\{\Bf{e}_{1},\Bf{e}_{2},\dots,\Bf{e}_{n}\}$ is the \textbf{standard basis} for \R{n}. That is, the $i$-th column of \textbf{A} is $T(\Bf{e}_{1})$, for $i=1,\dots,n$.
}

\defn{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. The unique $m\times n$ matrix \textbf{A} such that \[ T(\Bf{u})=\Bf{Au}\q\text{for all \textbf{u} in }\mathbb{R}^{n}\] is called the \textbf{standard matrix}, or \textbf{matrix representation} of $T$.
}

\defn{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation} and $S = \{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{n}\}$ be a basis for \R{n}. The \textbf{representation of} $T$ \textbf{with respect to basis} $S$ , denoted as $[T]_{S}$, is defined to be the $m\times n$ matrix \[[T]_{S}=(T(\Bf{u}_{1})\q T(\Bf{u}_{2})\q\dots\q T(\Bf{u}_{n}))\]
We are only able to find the standard matrix or the formula of a linear transformation $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ if and only if we are given the image of $T$ on a basis of \R{n}.
}

\theo{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a linear transformation and $S = \{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{n}\}$ be a basis for \R{n}. Then for any vector \textbf{v} in \R{n} , \[ T(\Bf{v})=[T]_{S}[\Bf{v}]_{S}\]
that is, the image $T(\Bf{v})$ is the product of the representation of $T$ with respect to basis $S$ with the coordinates \textbf{v} with respect to basis $S$.\br
Moreover, if \textbf{P} is the transition matrix from the standard basis $E$ of \R{n} to basis $S$, then the standard matrix \textbf{A} of $T$ is given by \[ \Bf{A} = [T]_{S}\Bf{P} \]
}

\sectiontitle{7.2 Range and Kernel of Linear Transformation}

\defn{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. The \textbf{range} of $T$ is \[\text{R}(T)=T(\mathbb{R}^{n})=\{\;\Bf{v}\in\mathbb{R}^{m}\;|\;\Bf{v}=T(\Bf{u})\text{ for some }\Bf{u}\in\mathbb{R}^{n}\;\}\]
}

\theo{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. The \textbf{range} of $T$ is a subspace.\br Let \textbf{A} be the standard matrix of $T$. Then the range of $T$ is the column space of \textbf{A}, \[ \text{R}(T)=\{\;\Bf{v}=T(\Bf{u})\;|\;\Bf{u}\in\mathbb{R}^{n}\;\}=\{\; \Bf{v}=\Bf{Au}\;|\;\Bf{u}\in\mathbb{R}^{n}\;\}=\text{Col}(\Bf{A})\]
}

\defn{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. The \textbf{rank} of $T$ is the dimension of the range of $T$ \[\text{rank}(T)=\text{dim(R(}T))\] Let \textbf{A} be the standard matrix of $T$. Then the rank of $T$ is equal to the rank of \textbf{A}, \[\text{rank}(T)=\text{dim(R(}T))=\text{dim(Col(}\Bf{A}))=\text{rank}(\Bf{A})\]
}

\defn{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. The set of all vectors in \R{n} that maps to the zero vector \textbf{0} by $T$ is called the \textbf{kernel} of $T$, and is denoted as \[\text{Ker}(T)=\{\;\Bf{u}\in\mathbb{R}^{n}\;|\;T(\Bf{u})=\Bf{0}\;\}\]
}

\theo{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. The \textbf{kernel} of $T$ is a subspace.\br Let \textbf{A} be the standard matrix of $T$. Then the kernel of $T$ is the nullspace of \textbf{A}, \[ \text{ker}(T)=\{\;\Bf{u}\in\mathbb{R}^{n}\;|\;\Bf{Au}=T(\Bf{u})=\Bf{0}\;\}=\text{Null}(\Bf{A}).\]
}

\defn{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. The \textbf{nullity} of $T$ is the \textbf{dimension} of the kernel of $T$, \[ \text{nullity}(T)=\text{dim(Ker}(T))\] 
Let \textbf{A} be the standard matrix of $T$. Then the nullity of $T$ is equal to the nullity of \textbf{A}, \[ \text{nullity}(T)=\text{dim(Ker}(T))=\text{dim(Null}(\Bf{A}))=\text{nullity}(\Bf{A}) \]
}

\defn{
A mapping $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is \textbf{injective}, or \textbf{one-to-one} if for every vector \textbf{v} in the range of $T$, $\Bf{v}\in\text{R}(T)$, there is a \textbf{unique u} in \R{n} such that $T(\Bf{u})=\Bf{v}$. \br Alternatively, $T$ is injective if whenever $T(\Bf{u}_{1})=T(\Bf{u}_{2})$, then $\Bf{u}_{1}=\Bf{u}_{2}$.
}

\theo{
A \textbf{linear transformation} $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is \textbf{injective} if and only if the kernel is \textbf{trivial}, $\text{ker}(T)=\{\Bf{0}\}$.\br Let \textbf{A} be the standard matrix of $T$. Then $T$ is injective if and only if the homogeneous system \textbf{Ax = 0} has only the trivial solution.
}

\theo{(Full Rank Equals Number of Columns)\br
Suppose \textbf{A} is a $m\times n$ matrix. The following statements are equivalent.
\begin{enumerate}
	\item \textbf{A} is full rank, where the rank is equal to the number of columns, $rank(\Bf{A})=n$.
	\item The rows of \textbf{A} spans \R{n}, $\text{Row(\textbf{A})=}$ \R{n}.
	\item The columns of \textbf{A} are linearly independent.
	\item The homogeneous system \textbf{Ax = 0} has only the trivial solution, that is, $Null(\Bf{A})=\{\Bf{0}\}$.
	\item $\Bf{A}^{T}\Bf{A}$ is an invertible matrix of order $n$.
	\item \textbf{A} has a left inverse.
	\item The linear transformation $T_{\Bf{A}}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$, $T_{\Bf{A}}(\Bf{v})=\textbf{Av}$ defined by \textbf{A} is injective.
\end{enumerate}
}

\note{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a linear transformation. If $T$ is injective, then necessarily $n\leq m$.
}

\defn{
A \textbf{linear transformation} $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is \textbf{surjective} or \textbf{onto} if for every \textbf{v} in the codomain \R{m}, \textbf{there exists} a \textbf{u} in the domain \R{n} such that $T(\Bf{u})=\Bf{v}$. Or equivalently, $T$ is surjective if the range is the codomain, $\text{R}(T)=$ \R{m}.\br Let \textbf{A} be the standard matrix of $T$. Then $T$ is surjective if and only if the column space of \textbf{A} is equal to \R{m}. This means that the rank of \textbf{A} is equal to the number of rows.
}

\theo{(Full Rank Equals Number of Rows)\br
Suppose \textbf{A} is a $m\times n$ matrix. The following statements are equivalent.
\begin{enumerate}
	\item \textbf{A} is full rank, where the rank is equal to the number of rows, $rank(\Bf{A})=m$.
	\item The columns of \textbf{A} spans \R{m}, $\text{Col(\textbf{A})=}$ \R{m}.
	\item The rows of \textbf{A} are linearly independent.
	\item The linear system \textbf{Ax = b} is consistent for every \textbf{b} $\in$ \R{m}.
	\item $\Bf{A}\Bf{A}^{T}$ is an invertible matrix of order $m$.
	\item \textbf{A} has a right inverse.
	\item The linear transformation $T_{\Bf{A}}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$, $T_{\Bf{A}}(\Bf{v})=\textbf{Av}$ defined by \textbf{A} is surjective.
\end{enumerate} 
}

\note{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a linear transformation. If $T$ is surjective, then necessarily $n\geq m$.
}

\theo{
Let $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ be a \textbf{linear transformation}. Then $T$ is both \textbf{injective} and \textbf{surjective} if and only if $n=m$ and the matrixrepresentation of $T$ is \textbf{invertible}.
}

\theo{(Equivalent statements for invertibility)\br
15. The \textbf{linear transformation} $T_{\Bf{A}}:\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$, $T_{\Bf{A}}(\Bf{v})=\Bf{Av}$ defined by \textbf{A} is \textbf{injective}.\br
15. The \textbf{linear transformation} $T_{\Bf{A}}:\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$, $T_{\Bf{A}}(\Bf{v})=\Bf{Av}$ defined by \textbf{A} is \textbf{surjective}.
}

\note{
A linear transformation $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is \textbf{bijective} if it is both \textbf{injective} and \textbf{surjective}. $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is bijective if and only if there is a linear transformation $S:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ such that \[T(S(\Bf{x}))=\Bf{x}\q\text{and}\q S(T(\Bf{x}))=\Bf{x}\q\text{for all }\Bf{x}\in\mathbb{R}^{n}\]
}
%:[HIDE] Ch 7
%\end{comment}

\end{document}


