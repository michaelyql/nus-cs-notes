\documentclass[11pt]{article}

%:packages
\usepackage{geometry}                	                  
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts} 	  % \mathbb commands
\usepackage{enumitem} 	  % control description indentation
\usepackage{tcolorbox}
\usepackage{mathtools}
\tcbuselibrary{theorems}
\tcbuselibrary{skins} 	  % enable enhanced tcb option
\tcbuselibrary{breakable} % allow boxes to break to next page

%:cover page
\begin{document}
\title{MA1522 Notes (AY24/25 Sem1)}
\author{Michael Yang}
\date{September 3, 2024}							
\maketitle

%:colors
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{BabyBlue}{rgb}{0, 0.5, 1}
\definecolor{Amber}{rgb}{1.0, 0.6, 0.4}
\definecolor{CadmiumRed}{rgb}{0.89, 0.0, 0.13}
\definecolor{Silver}{rgb}{0.65, 0.65, 0.65}

%:commands
\newcommand{\R}[1]{\mbox{$\mathbb{R}^{#1}$}}  % R^n
\newcommand{\Bf}[1]{\mathbf{#1}}
\newcommand{\Bb}[1]{\mathbb{#1}}
\newcommand{\q}{\quad}
\newcommand{\qq}{\qquad}
\newcommand{\constants}{c_{1}, c_{2}, \dots, c_{k} \in \mathbb{R}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\purple}[1]{{\color{violet}#1}}
\newcommand{\orange}[1]{{\color{orange}#1}}
\newcommand{\brown}[1]{{\color{brown}#1}}
\newcommand{\red}[1]{{\color{red}#1}}

% Usage: \createNewTcbBox{name}{displayName}{prefix}{colour}
\newcommand{\createNewTcbBox}[4]{
    \NewTcbTheorem{#1}{#2}{%
    	enhanced,
    	separator sign none,
    	frame empty,
    	colback=#4!5!white,
    	colbacktitle=#4!25!white,
    	colframe=#4!80!white,
    	coltitle=#4!50!black,
    	borderline={0.5mm}{0mm}{#4!10!white},
    	borderline={0.5mm}{0mm}{#4!60!white},
    	attach boxed title to top left={yshift=-2mm, xshift=2mm},
    	boxed title style={boxrule=0.4pt},
    	fonttitle=\bfseries,
    	theorem name,
		breakable,
    	}{#3}
}
\createNewTcbBox{definitionBox}{Definition}{def}{BabyBlue}
\createNewTcbBox{theoremBox}{Theorem}{theo}{ForestGreen}
\createNewTcbBox{corollaryBox}{Corollary}{corr}{Amber}
\createNewTcbBox{algorithmBox}{Algorithm}{algo}{CadmiumRed}
\createNewTcbBox{propertiesBox}{Properties}{props}{Silver}
	
\newcommand{\defn}[1]{% Definition
\begin{definitionBox}{}{}
#1
\end{definitionBox}%
\vspace{0.1cm}
}
\newcommand{\theo}[1]{% Theorem
\begin{theoremBox}{}{} 
#1
\end{theoremBox}%
\vspace{0.1cm}
}
\newcommand{\corr}[1]{% Corollary
\begin{corollaryBox}{}{}
#1
\end{corollaryBox}%
\vspace{0.1cm}
}
\newcommand{\algo}[2]{% Algorithm
\begin{algorithmBox}{#1}{}
#2
\end{algorithmBox}%
\vspace{0.1cm}
}
\newcommand{\props}[2]{% Properties
\begin{propertiesBox}{#1}{}
#2
\end{propertiesBox}%
\vspace{0.1cm}
}
\newcommand{\sectiontitle}[1]{% Section title
\begin{flushleft}\large{\textbf{#1}}\end{flushleft}} 
\newcommand{\chapter}[1]{% New chapter
\newpage
\section*{#1}
\hrule
\vspace{0.3cm}
}

% Margins
\newgeometry{left=1cm, right=1cm, top=1cm, bottom=2cm}

% ========================== Chapter 1 ==========================
\chapter{Chapter 1: Linear Systems}

% ========================== Chapter 2 ==========================
\chapter{Chapter 2: Matrix Algebra}

\sectiontitle{2.1 Definition and Special types of Matrices}
\sectiontitle{2.2 Matrix Algebra}
\sectiontitle{2.3 Linear System and Matrix Equation}
\sectiontitle{2.4 Inverse of Matrices}
\sectiontitle{2.5 Elementary Matrices}
\sectiontitle{2.6 Equivalent Statements for Invertibility}
\sectiontitle{2.7 LU Factorization}
\sectiontitle{2.8 Determinant by Cofactor Expansion}


\sectiontitle{2.9 Determinant by Reduction}

\theo{Suppose \textbf{B} is obtained from \textbf{A} by a \red{single} \orange{elementary row operation}, $\Bf{A}\xrightarrow{r} \Bf{B}$.\\
Then the \purple{determinant} of \textbf{B} is obtained from the \purple{determinant} of \textbf{A} as such.
\begin{itemize}
\item If $r=R_{i} + aR_{j}$, then det(\textbf{B}) = det(\textbf{A});
\item If $r=cR_{i}$, then det(\textbf{B}) = $c$ det(\textbf{A});
\item If $r=R_{i} \leftrightarrow R_{j}$, then det(\textbf{B}) = $-$det(\textbf{A}).
\end{itemize}
}

\corr{The determinant of an elementary matrix \textbf{E} is given as such.
\begin{itemize}
\item If \textbf{E} corresponds to $R_{i}+aR_{j}$, then det(\textbf{E}) = 1.
\item If \textbf{E} corresponds to $cR_{i}$, then det(\textbf{E}) = $c$.
\item If \textbf{E} corresponds to $R_{i}+aR_{j}$, then det(\textbf{E}) = $-$1.
\end{itemize}
}

\theo{Let \textbf{A} and \textbf{R} be square matrices such that \[\Bf{R}=\Bf{E}_{k}\dots\Bf{E}_{2}\textbf{E}_{1}\Bf{A}\] for some elementary matrices $\mathbf{E}_{1},  \mathbf{E}_{2},\dots, \mathbf{E}_{k}$. Then \[ \text{det}(\Bf{R}) = \text{det}(\Bf{E}_{k})\dots\text{det}(\Bf{E}_{2})\text{det}(\Bf{E}_{1})\text{det}(\Bf{A}).  \]
}

\corr{Let \textbf{A} be a $n\times n$ square matrix.\\
Suppose $\Bf{A}\xrightarrow{r_{1}}\xrightarrow{r_{2}}\dots\xrightarrow{r_{k}}\Bf{R}=\left( 
\begin{matrix} 
d_{1} & * & \dots & * \\ 
0 & d_{2} & \dots & * \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_{n} \\
\end{matrix} \right)$
, where \textbf{R} is the reduced row-echelon form of \textbf{A}. Let $\Bf{E}_{1}$ be the elementary matrix corresponding to the elementary row operation $r_{i}$, for $i=1,\dots,k$. Then \[ 
\text{det}(\Bf{A})=\frac{d_{1}d_{2}\dots d_{n}}{\text{det}(\Bf{E}_{k})\dots\text{det}(\Bf{E}_{2})\text{det}(\Bf{E}_{1})}.
\]
}

\sectiontitle{2.10 Properties of Determinant}

\theo{(Determinant of product is the product of determinant)\\\;\\
Let \textbf{A} and \textbf{B} be square matrices of the same size. Then \[\text{det}(\Bf{AB})=\text{det}(\Bf{A})\text{det}(\Bf{B}).\] By induction, for square matrices $\mathbf{A}_{1},  \mathbf{A}_{2},\dots, \mathbf{A}_{k}$ of the same size, \[\text{det}(\mathbf{A}_{1}  \mathbf{A}_{2}\dots \mathbf{A}_{k})=\text{det}(\Bf{A}_{1})\text{det}(\Bf{A}_{2})\dots\text{det}(\Bf{A}_{k}).\]
}

\theo{(Determinant of inverse is the inverse of determinant)\\\;\\
If \textbf{A} is \purple{invertible}, then \[\text{det}(\Bf{A}^{-1})=\text{det}(\Bf{A})^{-1}\]
}

\theo{(Determinant of scalar multiplication)\\\;\\
For any square matrix \textbf{A} of order $n$ and scalar $c$, \[\text{det}(c\Bf{A})=c^{n}\;\text{det}(\Bf{A}).\]
}

\defn{Let \textbf{A} be a $n\times n$ square matrix. The \blue{adjoint} of \textbf{A}, denoted as  \textbf{adj(A)}, is the $n\times n$ square matrix whose $(i, j)$ entry is the $(j, i)$-cofactor of \textbf{A},\[ \Bf{adj(A)}=\left( \begin{matrix}A_{11} & A_{12} & \dots & A_{1n}\\ A_{21} & A_{22} & \dots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \dots & A_{nn} \end{matrix} \right)^{T} = \left( \begin{matrix}A_{11} & A_{21} & \dots & A_{n1}\\ A_{12} & A_{22} & \dots & A_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ A_{1n} & A_{2n} & \dots & A_{nn} \end{matrix} \right) \]}

\theo{(Adjoint formula)\\\;\\
Let \textbf{A} be a \orange{square} matrix and \textbf{adj(A)} be its \purple{adjoint}. Then\[ \Bf{A(adj(A))}=\text{det}(\Bf{A})\Bf{I},\] where \textbf{I} is the identity matrix.}

\corr{(Adjoint formula for inverse)\\\;\\
Let \textbf{A} be an \purple{invertible} matrix. Then the \orange{inverse} of \textbf{A} is given by \[\textbf{A}^{-1}=\frac{1}{\text{det}(\Bf{A})}\Bf{adj(A)}\]}

% ========================== Chapter 3 ==========================
\chapter{Chapter 3: Euclidean Vector Spaces}

\sectiontitle{3.1 Euclidian Vector Spaces}

\defn{A (real) \emph{n}-\textbf{vector} is a collection of \emph{n} ordered real numbers, \[ \mathbf{v}\;=\; \begin{pmatrix} v_{1} \\ v_{2} \\ \vdots \\ v_{n} \end{pmatrix} ,where\text{ } v_{i} \in \mathbb{R} \text{ \emph{for} } i = 1, \dots, n. \] The real number $v_{i}$ is called the \emph{i}-th coordinate of the vector \textbf{v}. The \textbf{Euclidiean} \emph{n}-\textbf{space}, denoted \R{n}, is the collection of all \emph{n}-vectors \[ \mathbb{R}^{n} = \left\{ v =  \begin{matrix}   \begin{pmatrix}  v_{1} \\ v_{2} \\ \vdots \\ v_{n} \end{pmatrix} \end{matrix} \middle| v_{i} \in \mathbb{R} \text{ \emph{for} } i = 1, \dots, n.\right\} \]}

\props{of Vector Addition and Scalar Multiplication}{
Since vectors are matrices (column vectors are $n \times 1$  matrices and row vectors are $1 \times n$ matrices), the properties of matrix addition and scalar multiplication holds for vectors. For any vectors $\mathbf{u}, \mathbf{v}, \mathbf{w}$ and scalars $a, b\in \mathbb{R}$ , \begin{enumerate}
\item The sum $\Bf{u} + \Bf{v} $ is a vector in \R{n}
\item (Commutative) $\Bf{u} + \Bf{v} = \Bf{v} + \Bf{u}$
\item (Associative) $\Bf{u} + (\Bf{v} + \Bf{w}) = (\Bf{u} + \Bf{v}) +\Bf{w} $.
\item (Zero vector) $\Bf{0} + \Bf{v} = \Bf{v}$.
\item The negative $-\Bf{v}$ is a vector in \R{n} such that $\Bf{v}-\Bf{v}=\Bf{0}$.
\item (Scalar multiple) $a\Bf{v}$  is a vector in \R{n}.
\item (Distribution) $a(\Bf{u} + \Bf{v})= a\Bf{u} + a\Bf{v}$.
\item (Distribution) $(a+b)\Bf{u} = a\Bf{u} + b\Bf{u}$.
\item (Associativity of scalar multiplication) $(ab)\textbf{u} = a(b\Bf{u})$.
\item If $a\Bf{u} = \Bf{0}$, then either $a=0$ or $\Bf{u} = \Bf{0}$.
 \end{enumerate}
}

\defn{A \textbf{linear combination} of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k} \in $ \R{n} is $c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}$ for some $c_{1},  c_{2},\dots, c_{k} \in$  \R{k}.}

\defn{A set $V$ equipped with \textbf{addition} and \textbf{scalar multiplication} is said to be a \textbf{vector space} over \R{}  if it satisfies the following axioms. 
\begin{enumerate}
\item For any vectors $\Bf{u, v}$ in $V$, the sum $\Bf{u} + \Bf{v}$ is in $V$. 
\item (Commutative) For any vectors $\Bf{u, v}$ in $V$, $\Bf{u} + \Bf{v} = \Bf{v} + \Bf{u}$
\item (Associative) For any vectors $\Bf{u, v, w}$ in $V$, $\Bf{u} + (\Bf{v} + \Bf{w}) = (\Bf{u} + \Bf{v}) +\Bf{w} $
\item (Zero vector) There is a vector $\Bf{0}$ in $V$ such that $\Bf{0} + \Bf{v} = \Bf{v}$ for all vectors $\Bf{v}$ in $V$. 
\item (Negative) For any vector $\Bf{u}$ in $V$, there exists a vector $-\Bf{u}$ in $V$ such that 
$\Bf{u} + (-\Bf{u}) = \Bf{0}$.
\item For any scalar $a$ in \R{} and vector $\Bf{v}$ in $V$, $a\Bf{v}$  is a vector in $V$.
\item (Distribution) For any scalar $a$ in \R{} and vector $\Bf{u,v}$ in $V$, $a(\Bf{u} + \Bf{v})= a\Bf{u} + a\Bf{v}$.
\item (Distribution) For any scalars  $a, b$ in \R{} and vector $\Bf{u}$ in $V$, $(a + b)\Bf{u} = a\Bf{u} + b\Bf{u}$.
\item (Associativity of scalar multiplication) For any scalars $a, b$ in \R{} and vector $\Bf{u}$ in $V$, $a(b\Bf{u}) = (ab)\Bf{u}$.
\item For any vector $\Bf{u}$ in $V$, $1\Bf{u} = \Bf{u}$.
\end{enumerate}
}

\sectiontitle{3.2 Dot Product, Norm, Distance} 

\defn{The inner product (or dot product) of vectors $\textbf{u} = (u_{i})$
 and  $\textbf{v} = (v_{i})$ 
in \R{n}
 is defined to be \[ \Bf{u}\cdot \Bf{v} = {u}_{1} v_{1} +  {u}_{2} v_{2}, + \dots + {u}_{n}v_{n}.  \]  
\\ Define the \textbf{norm} of a vector $\Bf{u} \in$ \R{n}, $\Bf{u}=(u_{i})$, to be the square root of the inner product of $\Bf{u}$ with itself, and is denoted as $||\Bf{u}||$, \[ ||u|| = \sqrt{\Bf{u} \cdot \Bf{u}}=\sqrt{{u}^{2}_{1}+{u}^{2}_{2}+\dots+{u}^{2}_{n}}.\]
\\ This is also known as the \textbf{length} or \textbf{magnitude} of the vector.
}

\props{of inner product and norm}{Let $\Bf{u, v} \in$ \R{n} be vectors and $a,b,c\in \mathbb{R}$ be real numbers.
\begin{enumerate}
\item Inner product is \textbf{symmetric}, \[ \Bf{u}\cdot\Bf{v} = \Bf{v}\cdot\Bf{u}. \]
\item Inner product \textbf{commutes} with scalar multiple, \[c\Bf{u}\cdot\Bf{v}=(c\Bf{u})\cdot\Bf{v}=\Bf{u}\cdot(c\Bf{v}).\]
\item Inner product is \textbf{distributive}, \[ \Bf{u}\cdot(a\Bf{v} + b\Bf{w})  = a\Bf{u}\cdot\Bf{v} + b\Bf{u}\cdot\Bf{w} \]
\item Inner product is \textbf{positive definite}, $\Bf{u}\cdot\Bf{u}\geq 0$  with equality if and only if $\Bf{u} = \Bf{0}$.
\item $||c\Bf{u}||=|c| \;||\Bf{u}||$.
\end{enumerate}
}

\defn{A vector $\Bf{u} \in$ \R{n} is a \textbf{unit vector} if its norm is 1, \[||\Bf{u}||=1 \]  \textbf{Normalizing a vector} \\
Let $\Bf{u}$ be a nonzero vector $\Bf{u} \neq \Bf{0}$. By multiplying by the reciprocal of the norm, we get a unit vector, \[ \Bf{u} \longrightarrow \frac{\Bf{u}}{||\Bf{u}||} \]
This is called \textbf{normalizing} \textbf{u}.
}

\defn{The \textbf{distance} between two vectors \textbf{u} and \textbf{v}, denoted as $d(\Bf{u}, \Bf{v})$, is defined to be \[ d(\Bf{u}, \Bf{v}) = ||\Bf{u}-\Bf{v}||. \] 
Define the angle $\theta$ between two nonzero vectors, $\Bf{u, v}\neq\Bf{0}$ to be such that \[ \frac{\Bf{u}\cdot\textbf{v}}{||\Bf{u}||\;||\Bf{v}||} \]
}


\sectiontitle{3.3 Linear Combinations and Linear Spans}

\defn{
A linear combination of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k} \in$\R{n} is \[  c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}\text{, for some } c_{1}, c_{2},\dots c_{k} \in \mathbb{R}. \]
The scalars $c_{1}, c_{2},\dots c_{k}$ are called \textbf{coefficients}.\\
\\ Let $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}$ be vectors in \R{n}. The \textbf{span} (or \textbf{linear span}) of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}$ is the subset of \R{n} containing all the linear combinations of $\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}$, \[ \text{span}\{\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k}\}=\{ c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}\;|\; \constants  \}. \] 
}

\algo{to Check for Linear Combination}{
\begin{enumerate}
\item Form the $n\times k$ matrix $\Bf{A}=(\mathbf{u}_{1}\q \mathbf{u}_{2}\q\dots\q \mathbf{u}_{k})$ whose columns are the vectors in $S$.
\item Then a vector \textbf{v} in \R{n} is in $span\{ \mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k} \}$  if and only if the system $\Bf{Ax=v}$ is consistent.
\item If the system is consistent, then the solutions to the system are the possible coefficients of the linear combination. That is, if $\Bf{u}=\left( \begin{matrix} c_{1}\\c_{2}\\ \vdots \\c_{k} \end{matrix} \right)$ is a solution to $\Bf{Ax=v}$, then \[ \Bf{v}=c_{1}\mathbf{u}_{1} + c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k}. \]
\end{enumerate}
}

\algo{to Check if $span(S)=$ \R{n}}{ 
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a set of vectors in \R{n}. \begin{enumerate}
\item Form the $n\times k$ matrix $\Bf{A}=(\mathbf{u}_{1},  \mathbf{u}_{2},\dots, \mathbf{u}_{k})$ whose columns are the vectors in $S$.
\item Then $span(S)=$\R{n} if and only if the system $\Bf{Ax=v}$ is consistent for all \textbf{v}.
\item This is equivalent to the reduced row-echelon form of \textbf{A} having no zero rows.
\end{enumerate}
}

\props{of linear span}{
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a finite set of vector. The span of $S$, $span(S)$ has the following properties.
\begin{enumerate}
\item The span of $S$ \textbf{contains the origin}, \[ \Bf{0} \in span(S). \]
\item The span of $S$ is \textbf{closed under vector addition}, for any \textbf{u, v} $\in span(S)$, and real number $\alpha \in$ \R{}, \[\Bf{u} + \Bf{v} \in span(S) \]
\item The span $S$ is \textbf{closed under scalar multiplication}, for any $\Bf{u}\in span(S)$ and real number $\alpha\in$\R{}, \[ \alpha\Bf{u} \in span(S). \]
Properties (ii) and (iii) can be combined together into one property (ii'): The span is closed under linear combinations, that is, if \textbf{u, v} are vectors in $span(S)$ and $\alpha, \beta$ are any scalars, then the linear combination $\alpha\Bf{u} + \beta\Bf{v}$ is a vector in $span(S)$.
\end{enumerate}
}

\theo{(Linear span is closed under linear combinations) \\ \\
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a set of vectors in \R{n}. For any vectors $\mathbf{v}_{1}, \mathbf{v}_{2},\dots, \mathbf{v}_{m}$ in $span(S)$, the span of $\mathbf{v}_{1}, \mathbf{v}_{2},\dots, \mathbf{v}_{m}$ is a subset of $span(S)$, \[ span\{\mathbf{v}_{1},  \mathbf{v}_{2},\dots, \mathbf{v}_{m}\} \subseteq span(S). \]
}

\algo{to check for Set Relations between Spans}{
Suppose we are given 2 sets of vectors $T=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{m}\}$ and $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$. \begin{enumerate}
\item By the corollary, if $\Bf{v}_{i} \in span(S)$ for $i = 1,\dots,m$, we can conclude that $span(T) \subseteq span(S)$.
\item Recall that to check if $\Bf{v}_{i} \in span(S)$, we check that the system $(\;\textbf{u}_{1}\;\textbf{u}_{2}\;\dots\;\Bf{u}_{k} \;|\; \Bf{v}_{i}\;)$ is consistent for all $i = 1,\dots,m$.
\item There are in total $m$ such linear systems to check. However, since they have the same coefficient matrix, we may combine and check them together, that is, check that \[ (\;\textbf{u}_{1}\;\textbf{u}_{2}\;\dots\;\Bf{u}_{k} \;|\; \Bf{v}_{1}\ \;|\; \Bf{v}_{2} \;|\; \dots \;|\; \Bf{v}_{m}\; ) \] is consistent.
\end{enumerate}
}

\theo{(Algorithm to check for set relations between spans) \\ \\
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ and $T=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{m}\}$ be sets of vectors in \R{n}. Then $span(T) \subseteq span(S)$ if and only if $(\;\textbf{u}_{1}\;\textbf{u}_{2}\;\dots\;\Bf{u}_{k} \;|\; \Bf{v}_{1}\ \;|\; \Bf{v}_{2} \;|\; \dots \;|\; \Bf{v}_{m}\; )$ is consistent. 
}
 
\sectiontitle{3.4 Subspaces} 

\defn{The set of solutions to a linear system \textbf{Ax = b} can be expressed \textbf{implicitly} as \[ V = \{\Bf{u}\in \mathbb{R}^{n} \;|\; \Bf{Au=b}\} \] or \textbf{explicitly} as \[ V=\{ \Bf{u} + s_{1}\Bf{v}_{1} +  s_{2}\Bf{v}_{2} + \dots + s_{k}\Bf{v}_{k} \;|\; s_{1},s_{2},\dots,s_{k} \in \mathbb{R} \}, \] where $\Bf{u} + s_{1}\Bf{v}_{1} +  s_{2}\Bf{v}_{2} + \dots + s_{k}\Bf{v}_{k},\; s_{1},s_{2},\dots,s_{k} \in \mathbb{R}$ is the general solution.
}

\defn{A subset $V$ of \R{n} is a \textbf{subspace} if it satisfies the following properties.
\begin{enumerate}
\item $V$ \textbf{contains the zero vector}, $\textbf{0} \in V$.
\item $V$  is \textbf{closed under scalar multiplication}. For any vector. $v$ in $V$ and scalar $\alpha$, the vector $\alpha\Bf{v}$ is in $V$.
\item $V$ is \textbf{closed under addition}. For any vectors \textbf{u, v} in $V$, the sum \textbf{u + v} is in $V$.
\end{enumerate}
Property (i) can be replaced with property (i'): $V$ is \textbf{nonempty}.\\ \\
Properties (ii) and (iii) is equivalent to property (ii'):
$V$ is \textbf{closed under linear combination}. For any \textbf{u, v} in $V$, and scalars $\alpha, \beta$, the linear combination $\alpha\Bf{u} + \beta\Bf{v}$ is in $V$.
}

\theo{(Solution set of a homogeneous system is a subspace) \\ \\
The solution set $V=\{\textbf{u}\;|\;\Bf{Au=b}\}$ to a linear system \textbf{Ax = b} is a \textbf{subspace} if and only if \textbf{b = 0}, that is, the system is \textbf{homogeneous}.
}

\defn{The \textbf{solution set} to a \textbf{homogeneous system} is call a \textbf{solution space.}}

\theo{(Subspaces are equivalent to linear spans) \\ \\
A subset $V\subseteq$ \R{n} is a subspace if and only if it is a linear span, $V=span(S)$, for some finite set $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$.\\ \\
\textbf{Check if a set is a subspace} \\
To show that a set $V$ is a subspace, we can either
\begin{itemize}
\item find a spanning set, that is, find a set $S$  such that $V=span(s)$, or
\item show that $V$ satisfies the 3 conditions of being a subspace.
\end{itemize}
To show that a subset $V$ is not a subspace, we can either
\begin{itemize}
\item show that it does not contain the zero vector, $\Bf{0} \not\in V$,
\item find a vector $\Bf{v}\in V$ and a scalar $\alpha\in$ \R{} such that $\alpha\Bf{v}\not\in V$, or
\item find vectors $\Bf{u, v} \in V$ such that the sum is not in $V$, $\Bf{u + v} \not\in V$.
\end{itemize}
}

\theo{(Affine spaces)\\ \\
The solution set $W=\{\;\mathbf{w}\;|\;\mathbf{Aw=b}\;\}$ of a non-homogeneous linear system \textbf{Ax = b}, $\Bf{b}\not =0$, is given by \[ \Bf{u} + V := \{\; \Bf{u+v} \;|\;\Bf{v}\in V\; \} \] where $V=\{\;v\;|\;\Bf{Av=0}\;\}$ is the solution space to the associated homogeneous system and \textbf{u} is a particular solution, \textbf{Au = b}. \\ \\
That is, vectors in \textbf{u} + $V$ are of the form \textbf{u + v} for some \textbf{v} in $V$.
}

\sectiontitle{3.5 Linear Independence} 

\defn{A set $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is \blue{linearly independent} if the \orange{only coefficients} $\constants$ satisfying the equation \[ c_{1}\mathbf{u}_{1} +  c_{2}\mathbf{u}_{2} +\dots + c_{k}\mathbf{u}_{k} = \Bf{0} \] are $c_{1} = c_{2} = \dots = c_{k} = 0$. Otherwise, we say that the set is \blue{linearly dependent}.}

\algo{to Check for Linear Independence}{
Let $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a set of vectors in \R{n}. \begin{itemize}
\item $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is \orange{linearly independent} if and only if the \blue{homogeneous system} $(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)\Bf{x} = \Bf{0}$ has only the \orange{trivial solution}.
\item The homogeneous system has only the \orange{trivial solution} if and only if the \orange{reduced row-echelon form} of $(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)$ has \orange{no non-pivot column}.
\end{itemize}
}

\theo{(Solution set of a homogeneous system is a subspace) \\ \\
A subset $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ of \R{n} is \orange{linearly independent} if and only if the \purple{reduced row-echelon form} of $\Bf{A}=(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)$ has \orange{no non-pivot columns}.
}

\sectiontitle{3.6 Basis and Coordinates} 

\defn{Let $V$ be a subspace of \R{n}. A set $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is a \textbf{basis} for $V$ if \begin{itemize}
\item $span(S)=V$ and
\item $S$ is linearly independent.
\end{itemize}
}

\theo{Suppose $S\;\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is a basis for $V$. Then every vector \textbf{v} in the subspace $V$ \orange{can be written} as a linear combination of vectors in $S$ \orange{uniquely}.}

\theo{(Basis for Solution Set of Homogeneous System) \\ \\ 
Let $V=\{\Bf{u}|\Bf{Au=0}\}$ be the solution space to some homogeneous system. Suppose \[s_{1}\mathbf{u}_{1} +  s_{1}\mathbf{u}_{2} + \dots + s_{k}\mathbf{u}_{k}, s_{1},s_{2},\dots,s_{k}\in\mathbb{R}\] is a general solution to the homogeneous system $\Bf{Ax = 0}$. \\
Then $\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ is a basis for the subspace $V=\{\Bf{u}|\Bf{Au=0}\}$.
}

\theo{Basis for the zero space $\{\Bf{0}\}$ of \R{n} is the \orange{empty set} $\{\}$ or $\emptyset$.}

\theo{A $n\times n$ square matrix \textbf{A} is invertible if and only if the columns are linearly independent.}

\theo{A $n\times n$ square matrix \textbf{A} is invertible if and only if the columns spans \R{n}.}


\corr{Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$  be a subset of \R{n}  containing $n$  vectors. Then $S$ is linearly independent if and only if $S$ spans \R{n}.
}

\corr{Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$  be a subset of \R{n} and  $\Bf{A}=(\;\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;)$ be the matrix whose columns are vectors in $S$. Then $S$ is a \blue{basis} for \R{n} if and only if \purple{$k=n$} and \textbf{A} is an \orange{invertible matrix}.
}

\theo{A $n\times n$ \orange{square} matrix \textbf{A} is invertible if and only if the \purple{columns} of  \textbf{A} form a basis for \orange{basis} for \R{n}.
}

\theo{A $n\times n$ \orange{square} matrix \textbf{A} is invertible if and only if the \purple{row} of  \textbf{A} form a basis for \orange{basis} for \R{n}.
}

\theo{(Equivalent Statements for Invertibility) \\ \\ 
Let \textbf{A} be a square matrix of order $n$. The following statements are equivalent.
\begin{enumerate}
\item \textbf{A} is \blue{invertible}.
\item $\Bf{A}^{T}$ is \blue{invertible}.
\item \textbf{A} has a \blue{left-inverse}, that is, there is a matrix \textbf{B} such that \textbf{BA = I}.
\item \textbf{A} has a \blue{right-inverse}, that is, there is a matrix \textbf{B} such that \textbf{AB = I}.
\item The \orange{reduced row-echelon form} of \textbf{A} is the \orange{identity matrix}.
\item \textbf{A} can be expressed as a \orange{product} of \orange{elementary matrices}.
\item The \purple{homogeneous system} \textbf{Ax = 0} has \brown{only the trivial solution}.
\item For \orange{any} \textbf{b}, the system \textbf{Ax = b} is \purple{consistent}.
\item The \purple{determinant} of \textbf{A} is \orange{nonzero}, $\text{det}(\Bf{A})\not = 0$.
\item The \purple{columns/rows} of \textbf{A} are \orange{linearly independent} for \R{n}.
\item The \purple{columns/rows} of \textbf{A} \orange{spans} \R{n}.
\end{enumerate}
}

\defn{(Coordinates Relative to a Basis) \\ \\ 
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a basis for a subspace $V$ of \R{n}.\\
Then given any vector $\Bf{v} \in V$, we can write \textbf{v} unique as \[ c_{1}\mathbf{u}_{1},  c_{2}\mathbf{u}_{2},\dots, c_{k}\mathbf{u}_{k}. \]
The coordinates of \textbf{v} relative to the basis $S$ is defined to be the vector \[ [\Bf{v}]_{s} = \left(\; \begin{matrix} c_{1}\\c_{2}\\ \vdots \\ c_{k} \end{matrix} \;\right). \]
}

\algo{for Computing Relative Coordinate}{
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u_{k}}\}$ be a basis for a subspace $V$ of \R{n}.\\ \\
For $\Bf{v}\in V$, find real numbers $\constants \in$ \R{} such that \[ c_{1}\mathbf{u}_{1} +  c_{2}\mathbf{u}_{2} + \dots + c_{k}\mathbf{u}_{k} = \Bf{v}. \] 
That is, we are solving for \[ (\Bf{u}_{1}\;\Bf{u}_{2}\;\dots\;\Bf{u}_{k}\;|\;\Bf{v}). \]
}

\theo{Let $V$ be a subspace of \R{n} and $B$ a basis for $V$.
\begin{enumerate} 
\item For any vectors \textbf{u, v}$\in V$, \textbf{u = v} if and only if $[\Bf{u}]_{B}=[\Bf{v}]_{B}$.
\item For any $\mathbf{v}_{1},  \mathbf{v}_{2},\dots, \mathbf{v}_{m}\in V$, \[ [c_{1}\mathbf{v}_{1} +  c_{2}\mathbf{v}_{2} + \dots + c_{m}\mathbf{v}_{m}]_{B} = c_{1}[\mathbf{v}_{1}]_{B} +  c_{2}[\mathbf{v}_{2}]_{B} + \dots + c_{m}[\mathbf{v}_{m}]_{B}. \]
\end{enumerate}
}

\sectiontitle{3.7 Dimensions}

\theo{Let $V$ be a subspace of \R{n} and $B$ a basis for $V$. Suppose $B$ contains $k$ vectors, $|B|=k$. Let $v_{1},v_{2},\dots,v_{k}$ be vectors in $V$. Then \begin{enumerate} 
\item $\mathbf{v}_{1},  \mathbf{v}_{2},\dots, \mathbf{v}_{k}$ is linearly independent if and only if [$\mathbf{v}_{1}]_{B},  [\mathbf{v}_{2}]_{B},\dots, [\mathbf{v}_{k}]_{B}$ is linearly independent (respectively, dependent) in \R{k}; and
\item \{$\mathbf{v}_{1}, \mathbf{v}_{2},\dots, \mathbf{v}_{k}$\} spans $V$ if and only if [$\mathbf{v}_{1}]_{B},  [\mathbf{v}_{2}]_{B},\dots, [\mathbf{v}_{k}]_{B}$ spans \R{k}.
\end{enumerate}}

\corr{Let $V$ be a subspace of \R{n} and $V$ a basis for $B$. Suppose $B$ contains $k$ vectors, $|B|=k$.\begin{enumerate}
\item If $S=\{\mathbf{v}_{1},\mathbf{v}_{2}\dots,\mathbf{v}_{m}\}$ is a subset of $V$ with $m > k$, then $S$ is \purple{linearly dependent}.
\item If $S=\{\mathbf{v}_{1},\mathbf{v}_{2}\dots,\mathbf{v}_{m}\}$ is a subset of $V$ with $m < k$, then $S$ \orange{cannot span} $V$.
\end{enumerate}}

\corr{Suppose $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ and $T=\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{m}\}$ are bases for a subspace $V\subseteq$\;\R{n}. Then $k=m$.}

\defn{Let $V$ be a subspace of \R{n}. The \purple{dimension} of $V$, denoted by $\text{dim}(V)$, is defined to be the \purple{number of vectors} in any \orange{basis} of $V$.}

\theo{(Dimension of solution space) \\ \\ 
Let \textbf{A} be a $m \times n$ matrix. The \purple{number of non-pivot columns} in the reduced row-echelon form of $A$ is the \orange{dimension} of the solution space \[V=\{ \mathbf{u}\in \text{\R{n}}\; |\; \mathbf{Au=0} \}. \]}

\theo{(Spanning Set Theorem) \\ \\
Let $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ be a subset of vectors in \R{n}, and let $V=span(S)$. Suppose $V$ is not the zero space, $V\neq \{\mathbf{0}\}$. Then there must be a subset of $S$ that is a basis for $V$.}

\theo{(Linear Independence Theorem) \\ \\ 
Let $V$ be a subspace of \R{n} and $S=\{\mathbf{u}_{1},\mathbf{u}_{2},\dots,\mathbf{u}_{k}\}$ a linearly independent subset of $V$, $S\subseteq V$. Then there must be a set $T$ containing $S$, $S\subseteq T$ such that $T$ is a basis for $V$.
}

\theo{Let $U$ and $V$ be \orange{subspaces} of \R{n}.
\begin{enumerate}
\item If \blue{$U\subseteq V$}, then \purple{$dim(U) \leq dim(V)$}.
\item If \blue{$U\subseteq V$}, and \blue{$U\not = V$}, then \purple{$dim(U) < dim(V)$}
That is, $U\subseteq V$, then \purple{$dim(U) \leq dim(V)$} with \purple{equality} if and only if \purple{$U=V$}.
\end{enumerate}
}

\theo{(B1) \\ 
Let $V$ be a $k$-dimensional subspace of \R{n}, \purple{$dim(V)=k$}. Suppose $U\subseteq V$ is a \orange{linearly independent} subset containing \purple{$k$} vectors, \purple{$|S|=k$}. Then $S$ is a \blue{basis} for $V$.
\begin{enumerate}
\item $|S|=dim(V)$
\item $S\subseteq V$
\item $S$ is linearly independent
\end{enumerate}
}

\theo{(B2) \\ 
Let $V$ be a $k$ dimensional subspace of \R{n}, \purple{$dim(V)=k$}. Suppose $S$ is a set containing \purple{$k$} vectors, \purple{$|S|=k$}, such that \orange{$V\subseteq span(S)$}. Then $S$ is a \blue{basis} for $V$.
\begin{enumerate}
\item $|S|=dim(V)$
\item $V\subseteq span(S)$
\end{enumerate}
}


\sectiontitle{3.8 Transition Matrices}

\defn{Let $V$ be a subspace of \R{n}. Suppose $S=\{u_{1},\dots,u_{k}\}$ and $T=\{v_{1},\dots,v_{k}\}$ are \textbf{basis} for the subspace $V$. Define the \blue{transition matrix} from \orange{$T$ to $S$} to be \[\mathbf{P}=(\;[\mathbf{v}_{1}]_{S}\q[\mathbf{v}_{2}]_{S}\q\dots\q[\mathbf{v}_{k}]_{S}\;),\]the matrix whose columns are the coordinates of the vectors in $T$ relative to the basis $S$.}

\theo{(Transition Matrix) \\ \\
Let $V$ be a subspace of \R{n}. Suppose $S =  \{  u_{1}  ,\dots, u_{k}    \}$ are \textbf{bases} for the subspace $V$. Let \textbf{P} be the transition matrix from \purple{$T$ to $S$}. Then for any vector $w$ in $V$, \[ [\mathbf{w}]_{S} = \mathbf{P}[\mathbf{w}]_{T}. \] }

\algo{to find Transition Matrix}{ Let $S=\{u_{1},\dots,u_{k}\}$ and $T=\{v_{1},\dots,v_{k}\}$ be a basis for a subspace $V$ in \R{n}. To find \textbf{P}, the transition matrix from $T$ to $S$, \[(``S''|``T'') = (u_{1}\q u_{2}\q \dots u_{k}\q|\q v_{1}\q v_{2}\q\dots v_{k} ) \xrightarrow{\text{rref}} \left(\q \begin{matrix}\mathbf{I}_{k}\\ \mathbf{0}_{(n-k)\times k}  \end{matrix}\q \middle|\q \begin{matrix}\mathbf{P}\\ \mathbf{0}_{(n-k)\times k}  \end{matrix}\q \right)  \]}

\theo{(Inverse of Transition Matrix) \\ \\ 
Suppose $S = \{ u_{1},\dots, u_{k}\}$ and $T = \{ v_{1},\dots, v_{k} \}$ are \orange{bases} for a subspace $V$ of \R{n}. Let \textbf{P} be the \orange{transition matrix from $T$ to $S$}. Then $P^{-1}$ is the \orange{transition matrix from $S$ to $T$}.}

% ========================== Chapter 4 ==========================
\chapter{Chapter 4: Subspaces Associated to a Matrix}

\sectiontitle{4.1 Column Space, Row Space, and Nullspace}

\sectiontitle{4.2 Rank}


\end{document}


