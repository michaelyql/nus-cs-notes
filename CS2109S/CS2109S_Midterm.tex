\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\usepackage{geometry}
\newgeometry{left=0.25cm, right=0.25cm, top=0.8cm, bottom=0.5cm} 

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{setspace}
\usepackage{fontawesome}
\usepackage[hidelinks]{hyperref}
\usepackage{newtxmath}
\usepackage{titlesec}

\begin{document}

% \singlespacing
\singlespacing % 1.5 line spacing

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[C]{\vspace{-0.8cm}\faGithub\;\href{https://github.com/michaelyql}{michaelyql}}


\titlespacing*{\subsubsection}{0pt}{0pt}{0pt}

\begin{multicols*}{3}

\subsubsection*{\underline{1 Intelligent Agents}}
\textbf{PEAS}: Performance Measure, Environment, Actuator, Sensors\\
\textbf{Properties of Task Environment}: \\
(1) Fully (Sensors give access to complete state of environment) vs partially observable (2) Deterministic (next state is completely determined by current state and action taken) vs stochastic (3) Strategic: if the environment depends on actions of other agents, unless the other agents are predictable (unintelligent) (4) Episodic vs sequential, (5) Static (environment is unchanging while agent thinks) vs dynamic (environment changes with time but agent's score does not), (6) Discrete (limited number of percepts and actions) vs continuous, (7) Single vs multi-agent \\
\textbf{Agent Function}: Maps from percept histories $P$ to actions $A$, $f:P\to A$. An agent program is the implementation of an agent function. An agent is \textbf{completely} specified by the agent function. \\
\textbf{Agent Structures}: 1. \textit{Simple Reflex}: if-else actions, 2. \textit{Goal based}, 3. \textit{Utility Based}, 4. \textit{Learning Agent}\\
\textbf{Exploration vs Exploitation}: Learn about the world vs maximize gain based on current knowledge
\subsubsection*{\underline{2 Uninformed/Informed Search}}
\textbf{Completeness}: Search always find a solution. \textbf{Optimal}: If the search produces a solution, it is the best one\\
\textbf{Uninformed Search}: No clue how good a state is i.e. how close to goal \\
\textbf{Algo}: Create frontier, insert initial state to frontier. While frontier is not empty, pop node from frontier. If node is goal, return solution. Else, for each available action of current state, generate next state using transition model and add that to the frontier. If the frontier becomes empty and the goal is not reached, return failure \\
\textbf{BFS}: FIFO Queue. \textbf{Uniform Cost Search (UCS)}: priority queue. \textbf{DFS}: stack.\\
\textbf{Search with Visited Memory}: Maintain set of visited states. When a node is popped, if it is in visited memory, continue. Else, add it to the visited states. \\
\textbf{Depth Limited Search (DLS)}: Limit depth to $l$, backtrack when hit. Can be used with BFS/UCS/DFS. \\
\textbf{Iterative Deepning Search (IDS)}: Search with depth = $0,1,\dots$, return solution if found \\
\begin{center}
\vspace{-0.8cm}
\hspace*{-\leftskip}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Name & Time & Space & Complete & Opt \\ \hline
    BFS & Exp & Exp & Y & Y \\  \hline
    UCS & Exp & Exp & Y & Y \\ \hline
    DFS & Exp & Poly & N & N \\ \hline
    DLS & Exp & Poly* & N* & N* \\ \hline
    IDS & Exp & Poly* & Y & Y \\
    \hline
    \end{tabular}
\end{center}
* If used with DFS\\
\textbf{Informed Search}: has a clue how good the state is using a heuristic function \\
\textbf{Best-first search}: Priority queue but using $f(n)$ to evaluate cost \\
\textbf{A* search}: $f(n)=g(n)+h(n)$ where $g(n)$ is the cost to reach $n$ and $h(n)$ is the estimate distance from $n$ to the goal\\
\textbf{Admissible}: iff for every node $n$, $h(n)\leq h^*(n)$ where $h^*(n)$ is the optimal path cost to reach the goal state from $n$. If $h(n)$ is admissible, A* search without visited memory is optimal. \\
\textbf{Consistent}: iff for every node $n$, every successor $n'$ generated by any action $a$, $h(n)\leq c(n,a,n')+h(n')$ and $h(G)=0$. If $h(n)$ is consistent, A* search with visited memory is optimal \\
\textbf{Dominance}: if $h_1(n)\geq h_2(n)$ for all $n$, then $h_1(n)$ dominantes $h_2(n)$. $h_1(n)$ is better for search if it is admissible. 
\subsubsection*{\underline{3 Local/Adversarial Search}}
\textbf{Local Search}: For problems with too big search space. Search local neighbour states using perturbation or construction. Typically incomplete and suboptimal. \textbf{Any-time} property: longer runtime, better solution. Can obtain ``good enough'' solution \\
\textbf{Pertubative search}: Search space = complete solutions, search step = modification of solution. \\
\textbf{Constructive search}: search space = partial solutions, search step = extend solution. \\
\textbf{Problem Formulation}: States (may not be actual states), Initial State, Goal Test (optional), Successor function (generate neighbours) \\
\textbf{Evaluation function}: calculate quality of current state\\
\textbf{Hill Climbing}: Generate neighbours of current state and pick the one with highest evaluation score. If all neighbours score are lower than current state's score, return current state. Else, update current state to highest score neighbour. Repeat until current state has the highest score\\
\textbf{Adversarial Search â€“ Multi-agent problems}: Cooperative (agents work towards same goal), Competitive (agents have opposing goals), Mixed-motive (some cooperation and competition)\\
\textbf{Adversarial Games}: Players compete against each other. Two player zero-sum games: only one winner \\
\textbf{Problem Formulation}: States, Initial State, Terminal State (win/lose/draw), Actions, Transition, Utility Function (value of a state from one agent's perspective) \\
\textbf{Minimax}: Algorithm for two-player zero sum game. Player A: max player (try to maximize score). B: min player\\
\vspace{-0.6cm}
\begin{verbatim}
def minimax(state):
  v = max_value(state)
  return action in expand(state) 
  with value v
def max_value(state, a, b):
  if is_terminal(state): 
    return utility(state) v = -inf 
  for next_state in expand(state):
    v = max(v, min_value(next_state)) 
    a = max(a, v)
    if v >= b:
      return v
  return v
def min_value(state, a, b):
  if is_terminal(state): 
    return utility(state) v = inf 
  for next_state in expand(state):
    v = min(v, max_value(next_state)) 
    b = min(b, v)
    if v <= a:
      return v
  return v    
\end{verbatim}
\vspace{-0.2cm}
Time: Exponential $O(b^m)$, branching factor $b$, max depth $m$. Space: polynomial. Complete: Yes, if tree is finite. Optimal: Yes, against optimal opponent. If B is not optimal, A's action never has an utility lower than the utility against an optimal opponent, but a faster path can be taken to win, hence A's actions might be suboptimal\\
\textbf{AB Pruning}: $\alpha$ = largest value found so far for MAX, $\beta$ = smallest value found so far for MIN. Pruning doesn't affect the final result, but brings the TC down to $O(b^{m/2})$.
\subsubsection*{\underline{4 Machine Learning}}
\textbf{Supervised Learning}: Agent learns from labelled data (input-output pairs) and aims to learn mapping from inputs to outputs.\\
\textbf{Unsupervised Learning}: Agent learns from unlabelled data (input only) and aims to find patterns or structure.\\
\textbf{Regression Error}: The difference between the predicted ($\hat{y}$) and true value ($y$), where $\hat{y}^{(i)}=h(x^{(i)})$\\
\textbf{Mean Squared Error}:\\ $\text{MSE}=\frac{1}{N}\sum_{i=1}^N (\hat{y}^{(i)}-y^{(i)})^2$ \\
\textbf{Mean Absolute Error}:\\ $\text{MAE} = \frac{1}{N}\sum_{i=1}^N |\hat{y}^{(i)}-y^{(i)}| $ \\
\textbf{Classification Correctness/Accuracy}: Correct when $\hat{y}=y$. Accuracy = average correctness $=\frac{1}{N}\sum_{i=1}^N\vmathbb{1}_{\hat{y}^{(i)}=y^{(i)}}$ (1 if predicted = true value)\\
\textbf{Entropy}: Given outcomes $v_1,\dots,v_k$ and their probabilities $P(v_1),\dots,P(v_k)$, $I(P(v_1),\dots,P(v_k))=-\sum_{i=1}^kP(v_i)\text{log}_2P(v_i)$ For $p$ positive and $n$ negatives, $P(pos)=\frac{p}{p+n}$, $P(neg)=\frac{n}{p+n}$, $I(P(pos),P(neg))=-\frac{p}{p+n}\text{log}_2\frac{p}{p+n}-\frac{n}{p+n}\text{log}_2\frac{n}{p+n}$ \\
\textbf{Information Gain}: If attribute $A$ divides the training set into $E_1,\dots,E_v$ subsets, $IG(A)=I(\frac{p}{p+n}, \frac{n}{p+n})-remainder(A)$ where $remainder(A)=\sum_{i=1}^v\frac{p_i+n_i}{p+n}I(\frac{p_i}{p_i+n_i}, \frac{n_i}{p_i+n_i})$\\
\textbf{Decision Tree}: Basically, nested if-else. Used for both classification and regression. Greedily pick attribute with maximal IG at each node. \textbf{Overfitting}: DT captures noise, performs perfectly on training data but worse on test data. \textbf{Occam's Razor}: prefer short/simple hypothesis (more likely) over long/complex hypothesis. \textbf{Pruning}: Min-sample leaf (min. number of samples to be a leaf node), Max-depth (limit depth of DT)\\
\textbf{Data Preprocessing}: Continuous values (partition values into intervals), Missing values: assign most common values, assign probabilities to values, drop rows/attributes
\subsubsection*{\underline{5 Linear Regression}}
\textbf{Regression}: Given a data point $x$ and \textbf{no} target, find a function that \textbf{predicts} the target $y$ for that $x$.\\
\textbf{Linear Model}: The \textit{hypothesis class} of linear models is defined as the set of functions $h_w(x)=w_0x_0+w_1x_1+\dots+w_dx_d$, where $w_0,\dots,w_d$ are \textbf{parameters/weights} and $x_0=1$ is a dummy variable. Shorthand: $h_w(x)=w^Tx$. To measure the `fit' of a hypothesis, use a loss function to calculate the difference in predicted vs actual value. We want to find a $w$ that minimizes $h$. \\
\textbf{Normal Equation}: Gives the best weights $w$ that minimizes $J_{MSE}$, assuming $X^TX$ is invertible i.e. of full rank. If not invertible $\to$ 1. regularize, or 2. compute approximation using Moore-Penrose (least squares) inverse \\
\textbf{Problems with Normal Equation}: Cost of interving matrix is high ($d^3$): does not scale well for large matrices + Will not work for non-linear models + Assumes invertibility. \\
\textbf{Partial Derivative of Linear Model}: $\frac{\partial}{\partial w_j}h_w(x^{(i)})=\frac{\partial}{\partial w_j}(w^Tx^{(i)})=x_j^{(i)}$\\
\textbf{Partial Derivative of MSE}:
$\frac{\partial J_{MSE}(w)}{\partial w_j} =\frac{1}{2N}\frac{\partial}{\partial w_j}\sum^N_{i=1}(h_w(x^{(i)})-y^{(i)})^2 =\frac{1}{2N}\sum^N_{i=1}\frac{\partial}{\partial w_j}(h_w(x^{(i)})-y^{(i)})^2 
=\frac{1}{N}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}
$. Minimum when $\frac{\partial J_{MSE}(w)}{\partial w_j}=0$, i.e. $w=(X^TX)^{-1}X^TY$. (Note: $X$ has a bias column)\\
\textbf{Gradient Descent}: Move in the opposite direction of gradient i.e. towards lower loss. \\
\textbf{Theorem}: MSE loss function is \textbf{convex} for linear regression (and polynomial regression, with feature transformations). \\
\textbf{Features of Different Scales}: Normalize/Standardize - $x_j\leftarrow\frac{x_j-\mu_j}{\sigma_j}$, where $\sigma_j$ is the std deviation of the feature $j$ across the training data. 
Alternatives: min-max scaling, robust scaling, different learning rates $\gamma_j$ for each parameter.\\
\textbf{Variants of GD}: \textit{Mini-batch}: use a subset of whole training data per epoch, \textit{Stochastic}: use a random point per epoch. Faster, more randomness, may escape local minima. \\
\textbf{GD vs NE}: GD - Need to choose learning rate $\gamma$, many iterations, performs well for large $d$, may need feature scaling. NE - slow for large $d$, $X^TX$ must be invertible, no iterations or $\gamma$, no feature scaling needed.
\subsubsection*{\underline{6 Logistic Regression}}
\textbf{Classification}: Given a new data point $x\in \mathbb{R}^d$ and \textbf{no} target, based on existing dataset, find a function that predicts the target $y\in\{0, 1\}$ for that $x$ \\
\textbf{Squashing Models - Sigmoid Function} Maps a real number to a value between 0 and 1. $\sigma(x)=\frac{1}{1+e^{-x}}$. Derivative: $\sigma'(x)=\sigma(x)(1-\sigma(x))$ a.k.a the \textbf{logistic function} \\
The curve of $\sigma(c+x)$ where $c>0$ is a shift left\\
\textbf{Logistic Regression Model}: $h_w(x)=\sigma(w_0x_0+w_1x_1+\dots+w_dx_d)=\sigma(w^Tx)$ where $w_0,\dots,w_d$ are weights, and $x_0=1$ is a dummy variable. \\
The output of this model can be treated as the \textbf{probability of an input to be of class 1}. To decide whether an input belongs to a certain class, compare the probability to a \textbf{decision threshold}, e.g. 0.5. \\
\textbf{Decision Boundary}: the surface (or line, hyperplane) that separates different classes in the feature space \\
\textbf{Non-linearly Separable Data}: when a single linear decision boundary e.g. straight line cannot effectively separate the classes \\
\textbf{Theorem}: MSE loss function is \textbf{non-convex} for logistic regression, so it can't be used. \\
\textbf{Binary Cross Entropy} (BCE): Given the probability value $y\in[0,1]$ and $\hat{y}\in[0,1]$, $BCE(y,\hat{y})=-y\log(\hat{y})-(1-y)log(1-\hat{y})$ \\
A.k.a logistic loss / log loss. If $y$ and $\hat{y}$ are close, BCE loss is small. Else it is large. \\
\textbf{Mean BCE}: \\$J_{BCE}(w)=\frac{1}{N}\sum_{i=1}^N BCE(y^{(i)}, h_w(x^{(i)}))$\\ 
\textbf{Theorem}: BCE loss function is \textbf{convex} for logistic regression \\
\textbf{Logistic Regression with Gradient Descent} \\
\textbf{Hypothesis}: $h_w(x)=\sigma(w_0+w_1x_1+w_2x_2)$\\
\textbf{Loss function}: \\$J_{BCE}(w)=\frac{1}{N}\sum^N_{i=1}BCE(y^{(i)}, h_w(x^{(i)}))$\\
\textbf{Weight Update}:\\ $w_j \leftarrow w_j-\gamma\frac{\partial J_{BCE}(w_0, w_1,\dots)}{\partial w_j}$\\
\textbf{Derivative}: \\ $
    \frac{\partial J_{BCE}(w)}{\partial w_j}=\frac{\partial}{\partial w_j}\frac{1}{N}\sum^N_{i=1}BCE(y^{(i)}, h_w(x^{(i)})) 
    =\frac{1}{N}\sum^N_{i=1}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}
$ (same as LR) \\
\textbf{Multi-class Classification}: Given $N$ data points, predict the target $y\in\{1,2,\dots,C\}$ where $C$ is the number of classes. \\
\textbf{One-vs-One}: A separate binary classifier is trained for \textbf{every pair of classes}. For $C$ classes, this results in $\frac{C(C-1)}{2}$ classifiers. E.g. $(1, 2), (1,3), (2,3),\dots$ During prediction, each classifier votes for a class; the class with the most votes is selected \\
\textbf{One-vs-Rest}: A separate binary classifier is trained for each class, \textbf{treating all other classes as a single combined class}. For $C$ classes, this results in $C$ classifiers. E.g. 1 and not 1, 2 and not 2. During prediction, the classifier with \textbf{highest confidence score} (probability output) determines the class \\
\textbf{Generalization}: The ability to perform well on \textbf{unseen data}. Generalization error: error on unseen data. Two factors affect generalization: 1. \textit{Dataset quality and quantity}, 2. \textit{Model complexity} \\
\textbf{Dataset Quality}: 1. \textit{Relevance} - data should be relevant for the problem, 2. \textit{Noise} - incorrect data can hinder model's learning, 3. \textit{Balance} â€“ each class should be adequately represented \\
\textbf{Dataset Quantity}: More data is typically better. \textbf{Extreme case}: if the dataset contains every possible data point, the model can simply \textit{memorize} all the data \\
\textbf{Model Complexity}: Refers to the \textit{size} and \textit{expressiveness} of the hypothesis class. Higher complexity allows for more sophisticated modeling e.g. polynomial vs linear. Low complexity: $h_1(x)=\sigma(w_0+w_1x)$. Medium complexity: $h_2(x)=\sigma(w_0+w_1x+w_2x^2)$. High complexity: $h_3(x)=\sigma(w_0+w_1x+w_2x^2+w_3x^3+\dots)$ \\
\textbf{Underfitting}: Low complexity model cannot capture data\\
\textbf{Overfitting}: High complexity model may capture noise of the data \\
\textbf{Bias}: High bias = more data points does not lead to a different model. Low bias = more data points leads to a more complex model \\
\textbf{Variance}: Low variance = retraining model with different training set based on the same ground truth leads to the same model. High variance = retraining a complex model with a different training set based on the same ground truth leads to a very different model \\
\textbf{Model Complexity vs Error}: High complexity: Error on unseen data decreases with more data points. Low complexity: error on unseen data remains roughly the same regardless of data quantity. \textbf{Overparameterized} (deep neural networks): Error on unseen data decreases initially, peaks once before decreasing again \\
\textbf{Hyperparameters}: The settings that control the behaviour of the training algorithm and model. They are not learned. They \textbf{need to be set before the training process}. E.g. learning rate, feature transformations, batch size and no. of iterations \\
\textbf{Parameters}: learned \textbf{during} training e.g. weights in a linear model\\
\textbf{Hyperparameter Tuning}: Optimizing the hyperparameters to improve model performance, a.k.a hyperparameter search. Techniques: \textit{Grid (exhaustive) search}, \textit{Random search}, \textit{Local search (hill climbing)}, \textit{Successive halving}, \textit{Bayesian optimization}\\

\end{multicols*}
\end{document}

