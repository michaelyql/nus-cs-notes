\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\usepackage{geometry}
\newgeometry{left=0.25cm, right=0.25cm, top=0.8cm, bottom=0.5cm} 

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{setspace}
\usepackage{fontawesome}
\usepackage[hidelinks]{hyperref}
\usepackage{newtxmath}
\usepackage{titlesec}

\begin{document}

% \singlespacing
\singlespacing % 1.5 line spacing

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[C]{\vspace{-0.8cm}\faGithub\;\href{https://github.com/michaelyql}{michaelyql}}


\titlespacing*{\subsubsection}{0pt}{0pt}{0pt}

{\scriptsize
\begin{multicols*}{3}

\subsubsection*{\underline{1 Intelligent Agents}}
\textbf{PEAS}: Performance Measure, Environ- ment, Actuator, Sensors\\
\textbf{Properties of Task Environment}: Fully vs partially observable, Deterministic vs stochastic, Strategic (other intelligent agents), Episodic vs Sequential, Static vs dynamic (environment changes over time), Discrete vs continuous, Single vs multi-agent \\
\textbf{Agent Structures}: Simple Reflex (if-else), Goal based, Utility Based, Learning Agent\\ 
\textbf{Exploration vs Exploitation}: Learn about the world vs maximize gain based on current knowledge
\subsubsection*{\underline{2 Uninformed/Informed Search}}
\textbf{Complete}: Search always find a solution. \textbf{Optimal}: If the search produces a solution, it is the best one\\
\textbf{Uninformed Search}: No clue how good a state is i.e. how close to goal. Algorithm: BFS – FIFO queue, Uniform Cost Search (UCS) – priority queue, DFS – stack\\
\textbf{Search with Visited Memory}: Maintain set of visited states.\\
\textbf{Depth Limited Search (DLS}): Limit depth to $l$, backtrack when hit.\\
\textbf{Iterative Deepening Search (IDS)}: Search with depth $=0,1,\dots$\\
\textbf{Informed Search}: has a clue how good the state is using a heuristic function\\
\textbf{Best-first search}: Priority queue but using $f(n)$ to evaluate cost\\
\textbf{A* search}: $f(n)=g(n)+h(n)$ where $g(n)$ is the cost to reach $n$ and $h(n)$ is the estimate distance from $n$ to the goal.\\
\textbf{Admissible}: iff for every node $n$, $h(n) \leq h^*(n)$ where $h^*(n)$ is the optimal path cost to reach the goal state from $n$. If $h(n)$ is admissible, A* search without visited memory is optimal.\\
\textbf{Consistent}: iff for every node $n$, every successor $n'$ generated by any action $a$, $h(n)\leq c(n,a,n')+h(n')$ and $h(G)=0$. If $h(n)$ is consistent, A* search with visited memory is optimal\\
\textbf{Dominance}: if $h_1(n)\geq h_2(n)$ for all $n$, then $h_1$ dominates $h_2$. $h_1$ is better for search if it is admissible.
\begin{center}
\vspace{-0.2cm}
\hspace*{-\leftskip}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Name & Time & Space & Complete & Opt \\ \hline
    BFS & Exp & Exp & Y & Y \\  \hline
    UCS & Exp & Exp & Y & Y \\ \hline
    DFS & Exp & Poly & N & N \\ \hline
    DLS & Exp & Poly* & N* & N* \\ \hline
    IDS & Exp & Poly* & Y & Y \\
    \hline
    \end{tabular}
\end{center}
\vspace{-0.2cm}* If used with DFS
\subsubsection*{\underline{3 Local/Adversarial Search}}
\textbf{Local Search}: For problems with too big search space. Search local neighbour states using perturbation or construction. Typically incomplete and suboptimal. \textbf{Any-time property}: longer runtime, better solution. Can obtain “good enough” solution\\
\textbf{Pertubative search}: Search space = complete solutions, search step = modification of solution\\
\textbf{Constructive search}: search space = partial solutions, search step = extend solution\\
\textbf{Hill Climbing}: Generate neighbours and pick the one with highest score. \\
\textbf{Adversarial Search}: Cooperative (same goal), Competitive (opposing goals), Mixed-motive\\
\textbf{Adversarial Games}: Players compete against each other. Two player zero-sum games: only one winner\\
\textbf{Minimax}: Max player picks biggest value out of all states returned by min (vice versa) \\
\textbf{AB Pruning}: $\alpha$ = largest value so far, $\beta$ =  smallest value so far. Pruning doesn’t affect the final result, but brings the TC down to $O(b^{m/2})$.
\subsubsection*{\underline{4 Machine Learning}}
\textbf{Mean Squared Error}: $\frac{1}{N}\sum_{i=1}^N(\hat{y}^{(i)}-y^{(i)})^2$ \\
\textbf{Mean Absolute Error}: $\frac{1}{N}\sum_{i=1}^N|\hat{y}^{(i)}-y^{(i)}|$ \\
\textbf{Entropy}: $I(P(pos),P(neg))=-\frac{p}{p+n}\log_2\frac{p}{p+n}-\frac{n}{p+n}\log_2\frac{n}{p+n}$\\
\textbf{Information Gain}: $IG(A)=I(\frac{p}{p+n},\frac{n}{p+n})-remainder(A)$, $remainder(A)=\sum_{i=1}^v\frac{p_i+n_i}{p+n}I(\frac{p_i}{p_i+n_i}, \frac{n_i}{p_i+n_i})$ \\
\textbf{Decision Tree}: Pick attribute with maximal IG at each node. Occam’s Razor: prefer simpler hypothesis over complex one. Pruning: Min-sample leaf, Max-depth\\
\textbf{Data Preprocessing}: partition continuous values into intervals, assign most common values/assign probabilities to missing values or drop rows/attributes
\subsubsection*{\underline{5 Linear Regression}}
\textbf{Regression}: Predict a value for a data point\\
\textbf{Hypothesis Class}: The set of functions $h_w(x)=w_0x_0+\dots+w_dx_d=w^Tx$, $x_0=1$ \\ 
\textbf{Normal Equation}: $w=(X^TX)^{-1}X^TY$, assuming invertible. If not invertible, regularize, or compute approximation using Moore-Penrose (least squares) inverse \\
\textbf{Problems with Normal Equation}: Cost of inverting matrix is high $O(d^3)$: does not scale well for large matrices + Will not work for non-linear models + Assumes invertibility.\\
\textbf{Derivative}: $\frac{\partial J_{MSE}}{\partial w_j}=\frac{1}{2N}\sum_{i=1}^N(h_w(x^{(i)})-y^{(i)})x^{(i)}_j$ \\
\textbf{Theorem}: MSE loss function is convex for linear regression (and polynomial regression, with feature transformations)\\
\textbf{Features of Different Scales}: Normalize $x_j\leftarrow\frac{x_j-\mu_j}{\sigma}$. Alternatives: min-max scaling, robust scaling, different learning rates $\gamma_j$ for each parameter\\
\textbf{Variants}: \textit{Mini-batch}: use subset of training data, \textit{Stochastic}: use a random point. Faster, more randomness, may escape local minima. \\
\textbf{GD vs NE}: GD - Need to choose learning rate $\gamma$, many iterations, performs well for large $d$, may need feature scaling. NE - slow for large $d$, $X^TX$ must be invertible, no iterations or $\gamma$, no feature scaling needed.
\subsubsection*{\underline{6 Logistic Regression}}
\textbf{Sigmoid}: $\sigma(x)=\frac{1}{1+e^{-x}}$, $\sigma'(x)=\sigma(x)(1-\sigma(x))$. Squashes values between 0 and 1. $\sigma(x+c)$ shifts the curve left. \\
\textbf{Logistic Regression Model}: $h_w(x)=\sigma(w^Tx)=\sigma(w_0x_0+\dots+w_dx_d)$, $w_0=1$. Decision threshold: whether to predict class 0 or 1. \\
\textbf{Theorem}: MSE loss function is non-convex for logistic regression so it can't be used for gradient descent\\
\textbf{Binary Cross Entropy (BCE)}: i.e. log loss. $BCE(y,\hat{y})=-y\log(\hat{y})-(1-y)\log(1-\hat{y})$. If $y$ and $\hat{y}$ are close, BCE is small (and vice versa), hence it is a good loss function.\\
\textbf{Mean BCE}: $J_{BCE}(w)=\frac{1}{N}\sum_{i=1}^NBCE(y^{(i)},h_w(x^{(i)}))$\\
\textbf{Theorem}: BCE loss is convex for logistic regression. There is only one minimum (global minimum) \\
\textbf{Weight Update}: $w_j\leftarrow w_j+\gamma\frac{\partial J_{BCE}(w)}{\partial w_j}$\\
\textbf{Derivative}: $\frac{\partial J_{BCE}(w)}{\partial w_j}=\frac{1}{N}\sum_{i=1}^N(h_w(x^{(i)})-y^{(i)})x_j^{(i)}$\\
\textbf{Multi-class Classification}: Out of $C$ classes, predict which one it is. \\
\textbf{One-to-one}: Binary classifier for each pair of classes. At the end, the class with most votes is selected. Results in $\frac{C(C-1)}{2}$ classifiers\\
\textbf{One-vs-rest}: Binary classifier for one class vs all others, for each class, resulting in $C$ classifiers. The classifier with highest confidence (probability output) determines the class.\\
\textbf{Generalization}: ability to perform on unseen data. Generalization error: error on unseen data. Generalization is affected by \textit{dataset quality and quantity}, and \textit{model complexity}. \\
\textbf{Dataset Quality}: Relevance, Noise, Balance (equal repr). \textbf{Dataset Quantity}: Generally more is better, except when it contains all possible points, then model just memorizes the answer\\
\textbf{Model Complexity}: Size + Expressiveness of the hypothesis. Higher complexity = higher polynomial. Low complexity model may underfit, high complexity model may overfit. \\
\textbf{Bias}: High bias = more data points does not lead to a different model. Low bias = more data points leads to a more complex model\\
\textbf{Variance}: Low variance = retraining model with different training set based on the same ground truth leads to the same model. High variance = retraining a complex model with a different training set based on the same ground truth leads to a very different model\\
\textbf{Model Complexity vs Error}: High complexity: Error on unseen data decreases with more data points. Low complexity: error on unseen data remains roughly the same regardless of data quantity. \textit{Overparameterized} (deep neural networks): Error on unseen data decreases initially, peaks once before decreasing again\\
\textbf{Hyperparameters}: Settings that control the behaviour of the model. They are not learned; need to be set before the training process. E.g. learning rate, feature transformations, batch size and no. of iterations. \textbf{Parameters}: learned during training e.g. weights 
\subsubsection*{\underline{7 Regularization, Kernels, SVM}}
\textbf{Overfitting}: Complex model includes noise, performs poorly on unseen data. Often associated with large weights. To prevent, keep weights small by putting a cost (penalty) on large weights. Given a loss function $J(w)$, add a penalty function/regularizer $P(w)$ with a penalty strength $\lambda\geq0$: $J_{reg}(w)=J(w)+\lambda P(w)$. Optimization goal: $\text{min}_wJ_{reg}(w)$. \\
\textbf{Penalty Functions}: Square $P(w)=\sum^d_{j=0}\frac{1}{2}w_j^2$ (L2/Ridge, make all weights small). Absolute $P(w)=\sum^d_{j=0}|w_j|$ (L1/Lasso, zero out some weights). Max-norm $P(w)=\text{max}(w_0,w_1,w_2,\dots)$\\
\textbf{Gradient descent}: $\frac{\partial}{\partial w_j}J_{reg}(w)=\frac{\partial}{\partial w_j}J(w)+\lambda \frac{\partial}{\partial w_j}P(w)$\\
% Ridge regression for normal equation: $w=(X^TX)^{-1}X^TY$, $L2=||w||^2=w_0^2+w_1^2+\dots+w_d^2$. Residual sum of squares, $RSS=\sum_{i=1}^n(y_i-\hat{y})^2$, $RSS_{L2}=RSS+\lambda\sum_{j=1}^dw_j^2$
\textbf{Normal Equation with Reg}: $\frac{\partial J_{reg}(w)}{\partial w_j}=\frac{1}{N}\sum^N_{i=1}(w^Tx^{(i)}-y^{(i)})x_j^{(i)}+\lambda w_j=0$, $w=(X^TX+\lambda I)^{-1}X^TY$. NE with regularization works no matter if there is linear dependency or insufficient observations. \\
\textbf{Theorem}: For all $\lambda>0$, the matrix $X^TX+\lambda I$ is invertible.\\
\textbf{Weights and Training Data}: $w=(X^TX)^{-1}X^TY=(X^TX)^{-1}\sum^n_{j=1}y^{(j)}x^{(j)}=\sum^n_{j=1}\alpha_jx^{(j)}$. $\alpha$ is a $n$-dimension vector. \\
\textbf{Linear Model Dual Formulation}: $h_w(x)=w^Tx=\sum^n_{j=1}\alpha_jx^{(j)^T}x=h_\alpha(x)$. $h_\alpha(x)$ is a \textbf{dual hypothesis} which contains a sum of dot products between all $x^{(j)}$ and $x$, and weights $\alpha_j$. \\
\textbf{Dual Formulation and Kernel}: Let $k(u,v)=u^Tv$. Then $h_\alpha(x)=\sum^N_{j=1}\alpha_jk(x^{(j)},x)$. Other functions can be used to obtain different linear models.\\
\textbf{Kernel Trick}: Replace dot product with a similarity function $k(u,v)$ i.e. a kernel. \\
\textbf{Kernel Machine}: A hypothesis that uses kernel trick. \textit{Valid} Kernel: continuous symmetric positive-definite kernel \\
\textbf{Single variable transformed features}: $x_j\to x_j$ and $x_j^5$, $x_j\to \log(x_j)$ and $x_j$, $x_j\to e^{(x_j)}$ and $x_j$. \\
\textbf{Multi-variable transformed features}: $d$-dimension to $M$-dimension feature vector. In general, $x\in \mathbb{R}^d\to\phi(x)\in \mathbb{R}^M$. Usually $M\geq d$. $\phi$ is a \textit{feature map}, and $\phi(x)$ is called \textit{transformed features}. E.g. $x=\begin{bmatrix}x_1,x_2,x_3\end{bmatrix}^T$,$\phi_{M2}(x)=\begin{bmatrix}x_1,x_2,x_3,x_1^2,x_2^2,x_3^2,x_1,x_2,x_1x_3,x_2x_3\end{bmatrix}^T$ where $M2$ transforms all monomials of degree up to 2.\\
\textbf{Hypothesis Class}: Given a $d$ dimension input vector $x$ and a $M$ dimension feature map $\phi(x)$, the hypothesis class of linear models with transformed features is the set of functions $h_w^\phi(x)=w_0\phi(x)_0+w_1\phi(x)_1+\dots+w_M\phi(x)_M$, where $w_0,\dots,w_M$ are weights, with dummy feature $\phi(x)_0=1$. Shorthand: $h_w(x)=w^T\phi(x)$. \\
\textbf{Dual Hypothesis with Transformed Features}: $h_\alpha^\phi(x)=\sum^N_{j=1}\alpha_j\phi^T(x^{(j)})\phi(x)$. The dot product between $\phi^T(x^{(j)})\phi(x)$ defines a new valid kernel function $k_\phi(u,v):=\phi^T(u)\phi(v)$.\\
\textbf{Dual Hypothesis with Kernel}: $h_\alpha^\phi(x)=\sum^N_{j=1}\alpha_jk_\phi(x^{(j)},x)$. Don't have to compute $\phi(x)$ explicitly. $k_{Ps}(u,v)=\phi_{Ps}(u)^T\phi_{Ps}(v)=(u^Tv)^s$ for polynomial degree $s$ with $d^s$ terms\\
\textbf{Theorem}: For every valid kernel $k(u,v)$ there exists a corresponding feature transformation $\phi$ (which may be infinite-dimensional) where $k(u,v)=\phi^T(u)\phi(v)$. Conversely, every $\phi$ induces a valid kernel $k$.\\
\textbf{Support Vector Machine}: a classifier (can also be used for regression). Construct an `optimal' separating decision boundary. Fat margin maximizes performance on unseen data and is robust to noise. Kernels can be applied naturally to lead to non-linear boundaries. \\
\textbf{Hyperplane}: defined by $w$, contains all vectors $u$ for which $w^Tu=0$. Sign of $w^Tu$ is the side a point lies. Distance of a point: $|w^Tu|/||w||$. Support vectors lie on the edge of the margin.\\
$SVM_\alpha(x)=\text{sign}(\sum_{i=1}^Na^{(i)}k(x^{(i)},x))$ \\
\textbf{Sparsity}: Many $\alpha^{(i)}$ will be zero, only few are support vectors, others can be thrown out. 
\subsubsection*{\underline{8 Unsupervised Learning}}
Learn patterns in the data.\\
\textbf{Types}: Clustering (identify clusters), Dimensionality reduction (find a lower dimensional representation of the data)\\
\textbf{Clustering}: group similar data points together. The no. of clusters is not predefined by the data. Common applications: data segmentation, anomaly detection.\\
\textbf{Dimensionality Reduction}: Reduce the no. of features while retaining as much relevant information as possible. Common applications: visualizations, feature extraction \\
\textbf{K-Means Clustering}: E.g. produce clothes of $K$ distinct sizes such that the fit for everyone is pretty good. \\
\textbf{Centroid}: The average of a set of points $x^{(i)}$, $\mu=\frac{1}{N}\sum_{i=1}^Nx^{(i)}$. A centroid defines a cluster.\\
\textbf{Algorithm}: 1. Randomly initialize $K$ centroids $\mu_1,\dots,\mu_k$. 2. Repeat until convergence: 2.1 For $i=1,\dots,N$, set $c^{(i)}\leftarrow$ index of centroid closest to $x^{(i)} $, 2.2 For $k=1,\dots,K$, $\mu_k\leftarrow$ centroid of $x^{(i)} $ assigned to cluster $k$. Convergence = no more changes \\
\textbf{Measuring Goodness of Clusters}: Distortion (average distance of each sample to its centroid) $J(c^{(1)},\dots,c^{(N)},\mu_1,\dots,\mu_k)=\frac{1}{N}\sum^N_{i=1}||x^{(i)}-\mu_{c^{(i)}}||^2$ \\
\textbf{Theorem}: Every step in K-Means never increases distortion.\\
\textbf{Picking No. of Clusters}: Elbow method – pick $k$ where the graph of $J$ against $K$ forms an elbow. \\
\textbf{Variants of K-Means}: K-Medoids – initialize $K$ random centroids, then pick data points closest to centroids and use them as centroids (`snap' centroids to nearest data point) \\
\textbf{Clustering vs Classification}: Classification: supervised, uses input-output pairs, hyperplane based (SVM), probability based (logistic model), no. of classes is defined by the dataset. Clustering: unsupervised, input only, distance based, no. of classes is set by us\\
\textbf{Curse of dimensionality}: No. of samples $N$ needed to learn a hypothesis class increases \textit{exponentially} with no. of features $d$, $N=O(2^d)$. Models that take in high dimensional features may suffer. \\
\textbf{Singular Value Decomposition}: Redundant features can be removed. Features can be made redundant by a change of basis. $A=U\Sigma V^T$ \\
\textbf{Theorem}: WLOG, let $d>N$. For any $d\times N$ matrix $A$, there exists a factorization $A=X^T=U\Sigma V^T$ such that $U$ is $d\times d$ and has $d$ orthonormal columns, $\Sigma$ is $N\times N$ and is a diagonal matrix with $\sigma_j\geq 0$, $V$ is $N\times N$ and has $N$ orthonormal columns and rows. $U$ can be thought of as the new basis, $\Sigma$ as the importance, and $V$ as the linear combination of coefficients. Another way to think of it is, $U$ contains templates (as columns), $\Sigma$ is the template importance, and $V$ is the combination coefficients.\\
\textbf{Dimensionality Reduction}: The $N$ singular values $\sigma$ tell us about the importance of the new basis vectors. Remove less important basis vectors by setting all singular values except the first $r$ to 0. The new basis matrix $\tilde{U}$ is $N\times r$. \\
\textbf{Compression}: $Z=\tilde{U}^TX^T$ is $r\times N$ (project data points to the lower dimensional basis)\\
\textbf{Theorem - Reconstruction}: For a fixed $r$, $\tilde{U}\tilde{U}^T\approx I$, with approximation error dependent on $r$. $\tilde{X}^T=\tilde{U}\tilde{U}X^T\approx X^T$, $X^T=\tilde{U}Z$\\
\textbf{Mean-centered data}: Compute $\bar{x}$, then compute mean-centered data points $\hat{x}^{(i)}=x^{(i)}-\bar{x}$ for all $i$ \\
\textbf{Theorem}: Given a mean-centered data matrix $\hat{X}^T$, the values $\frac{\sigma_j^2}{N-1}$ are variance of the data in the basis defined by the vectors $u^{(j)}$\\
\textbf{Theorem}: Task: Retain at least 99\% of variance in the data. Solution: choose minimum $r$ s.t. $\frac{\sum_{i=1}^r\sigma_i^2}{\sum_{i=1}^N\sigma_i^2}\geq0.99$. Using this $r$, the original mean-centered data points and the reconstructed data points are close. $\frac{\sum_{i=1}^N||\hat{x}^{(i)}-\tilde{x}^{(i)}||^2}{\sum_{i=1}^N||\hat{x}^{(i)}||^2}\leq0.01$\\
\textbf{Principal Component Analysis}: A statistical application of SVD. Capture components that maximize the statistical variations of the data
\subsubsection*{\underline{9 Neural Networks}}
\textbf{Perceptron}: Given $n$ data points each with $d$ features and target $\{-1,1\}$, $h_w(x)=g(w^Tx)=g(\sum_{j=0}^dw_jx_j)=g(w_0x_0+\dots+w_dx_d)$ where $w_0,,\dots,w_d$ are weights and $w_0=1$ is a dummy variable. $g(z)=\begin{cases}
    +1& z\geq0\\
    -1& z<0
\end{cases}$ is a sign function \\
\textbf{Perceptron Learning Algorithm}: Select one misclassified instance $(x^{(k)},y^{(k)})$, set $w\leftarrow w+\gamma(y^{(k)}-\hat{y}^{(k)})x^{(k)}$ until it converges. Different from SVM, margin is not maximized. If data is not linearly separable, will not converge. Why it works: $w^Tx=|w||x|\cos\theta$, adding/subtracting $x$ from $w$ will decrease/increase $\theta$ to the desired angle. $\gamma$ affects how much the angle changes in one iteration. \\
\textbf{Neuron}: basic building block of a NN. A generalized version of perceptron. $\hat{y}=h_w(x)=g(\sum_{j=0}^dw_jx_j)$ . Activation function can be anything: sigmoid $\sigma(x)=1/(1+e^{-x})$, tanh $\tanh(x)$, ReLU $\max(0,x)$, Leaky ReLU $\max(0.1x,x)$, $ELU(x)=\begin{cases}
    x&x\geq0\\ \alpha(e^x-1) & x<0
\end{cases}$, Maxout $\max(w_1^Tx+b_1,w_2^Tx+b_2)$ \\
\textbf{Neuron vs Linear/Logistic Regression}: If activation function is the identity function $g(z)=z$, neuron becomes the linear regression model. If $g(z)=\sigma(z)$ then it becomes the logistic regression model.\\
\textbf{Modelling OR/AND Gate with Logistic Regression Model}: Plot truth table entries as data points and find a decision boundary \\
\textbf{XNOR Gate}: Data not linearly separable. Feature engineering is required to transform existing features. Can be done manually, or by using neurons to transform them (pass features into two neurons with $g(z)=\sigma(z)$.\\
\textbf{NN vs Linear/Logistic Model}: Linear/Logistic model requires manual feature engineering while NN learns its own features. \\
\textbf{Matrix Multiplication}: no. of rows = no. of inputs, no. of columns = no. of outputs. Each column of $W$ is the weight vector for one neuron. $\hat{y}=g(W^Tx)=g(\begin{bmatrix}
    W_{11}&W_{12}\\
    W_{21}&W_{22}\\
    W_{31}&W_{32}\\
\end{bmatrix}^T\begin{bmatrix}
    x_1\\
    x_2\\
    x_3
\end{bmatrix})=\begin{bmatrix}
    \hat{y_1}\\ 
    \hat{y_2}
\end{bmatrix}$\\
\textbf{Multi-layer}: $\hat{y}=g^{[2]}(W^{[2]^T}\cdot g^{[1]}(W^{[1]^T}x))=\begin{bmatrix}
    \hat{y_1}\\ 
    \hat{y_2}
\end{bmatrix}$\\
\textbf{Forward Propagation}: The process by which input data passes through the NN to generate a prediction. \\$\hat{y}=g^{[L]}(W^{[L]^T}\cdot g^{[L-1]}(\dots))=\begin{bmatrix}
    \hat{y_1}\\ 
    \dots\\
    \hat{y_2}
\end{bmatrix}$\\
\textbf{Multi-class Classification}: $c$-class classification – set the no. of neurons in the last layer as $c$. Predict the probability of each class and pick the highest probability. \\
\textbf{Softmax}: Given $z=[z_1,z_2,\dots,z_c]$, for each $z_i$, $g(z_i)=e^{z_i}/(\sum_{j=1}^c e^{z_j})\in[0,1]$. $\sum g(z_i)=1$. 
\subsubsection*{\underline{10 NN Training and CNN}}
\textbf{Gradient Computation}: $\hat{y}=h_w(x)=g(\sum_{j=0}^dw_jx_j)=g(z)$, $L=\frac{1}{2}(\hat{y}-y)^2$ (MSE) \\
$\frac{dL}{d\hat{y}}=\frac{d(\frac{1}{2}(\hat{y}-y)^2)}{d\hat{y}}=(\hat{y}-y)$, $\frac{d\hat{y}}{dz}=\frac{d(g(z))}{dz}=g'(z)$\\
$\frac{\partial z}{\partial w_j}=\frac{\partial(\sum_{j=0}^dw_jx_j)}{\partial w_j}=x_j$\\
$\frac{dL}{dw_j}=\frac{dL}{d\hat{y}}\frac{d\hat{y}}{dz}\frac{\partial z}{\partial w_j}=(\hat{y}-y)g'(z)x_j$\\
If $w_j$ is used to generate $u_j$ from $v_j$, then $\frac{\partial L}{\partial w_j}=\frac{\partial L}{\partial u_j}v_j$. $\frac{\partial L}{\partial u_j}$ can be computed in one backward pass. $v_j$ can be computed in one forward pass. \\
One training loop: Forward pass to compute loss $\to$ zero the gradients with an optimizer $\to$ backpropagation $\to$ update weights\\
\textbf{Convolution Layer}: Naive computer vision – 800x800 image with 100 neurons in the first layer = 64 million weights. Not all pixels (features) are required for one neuron (e.g. detect ear). Can view a small region at a time and apply over all regions of the image. \\
\textbf{Kernel/Filter}: A matrix used to detect features from input, applied to all regions. For each region, multiply element-wise, then sum. The result is a \textit{feature map}. \\
\textbf{Common Practices}: Add a padding of 1 pixel to the image border. Can also set the stride (no. of pixels to jump to next region). Both practices control the size of the feature map.\\
\textbf{Multiple Input Channels}: E.g. 3-channel RGB input. For each channel, use a separate weight matrix and compute the feature map, then sum all feature maps to create the output channel. The kernel is also multi-channeled, composing of the different weight matrices. \\
\textbf{Multiple Output Channels}: Use more kernels to create more output channels from one input channel. Concatenate the output channels at the end.\\
\textbf{Pooling Layer}: Downsample feature maps. Aggregation methods: Max-Pool, Average-Pool, Sum-Pool. For each $n\times n$ window, take the max/average/sum. Bigger pooling windows reduce the size of the feature map more.\\
\textbf{Typical CNN}: Input $\to$ Feature learning (convolution + ReLu + Pooling) $\to$ Prediction (flatten feature map + fully connected layer4 + softmax). \\
\textbf{Popular CNN Architectures}: VGG, Inception, ResNet, DenseNet\\
\textbf{Applications of CNN}: Image classification (e.g. happy/sad), Object detection (e.g. self-driving cars), Image segmentation (e.g. cancer cell detection; more fine-grained classification task) 
\subsubsection*{\underline{11 RNN on Sequential Data}}
\textbf{Recurrent Neural Network}: Handles sequential data e.g. text (``we saw this saw''), predict label (verb/noun) for each word. \\
\textbf{One-hot encoding}: For each word, create a vector with length equal to the vocabulary (set of all words) size. Each word is assigned a unique index and its vector is all zeros except for a 1 in the position of that index. \\
\textbf{Naive NN}: One-hot vectors of words in different positions in a sentence will give the same $\hat{y}$; we should not predict each label independently $\to$ obtain contextual information from RNN\\
\textbf{Hidden State}: No. of elements in hidden vector = no. of desired outputs. At time step $t$, $h^t=\begin{bmatrix}
    h_1^t\\h_2^t
\end{bmatrix}$, $h^t=g^{[h]}((W^{[xh]})^Tx^t+(W^{[hh]})^Th^{t-1})$, $\hat{y}=g^{[y]}((W^{[hy]})^Th^t)$. Hidden state stores information extracted from $x^1,\dots,x^{t}$ where $x^i$ is the data at time $i$. One column in $W^{[xh]}$ is used to generate one node. \\
\textbf{Many-to-many}: $T_x=T_y>1$. Many RNN layers across time steps to generate many outputs. \\
\textbf{Many-to-one}: $T_x>1, T_y=1$. Only last layer used to generate one output. \\
\textbf{One-to-many}: $T_x=1, T_y>1$. Output prediction is passed into RNN layer of next time step. \\
\textbf{Many-to-many}: $T_x\not=T_y, T_x>1, T_y>1$. Use $<begin>$ vector \\
\textbf{Deep RNN}: Multiple RNN layers for each time step instead of just one \\
\textbf{Applications of RNN}: Sentiment analysis, speech recognition, video captioning\\
\textbf{Properties of RNN}: Captures contextual information, but each prediction must wait for all previous steps to be complete (not parallelism friendly) \\
\textbf{Self-Attention Layer}: Handles sequential data in parallel. Need to identify if previous elements are needed for generating the current output \\
\textbf{Attention Score}: Determines how much focus (attention) each part of the sequence ($x^j$) should receive when processing a specific element $(x^i)$. Query $q^i=W^qx^i$, Key $k^j=W^kx^j$. $W^q$ and $W^k$ are trainable matrices used to extract useful features to calculate attention score. $a_{i\cdot j}=k^j\cdot q^i=(k^j)^Tq^i$. Value $v^i=W^vx^i$. Step 1: Linear Projection - transform input vectors into Query, Key and Value using weight matrices (shared across input) $Q=W^qX$, $K=W^kX$, $V=W^vX$. Step 2: Compute attention scores – $a_{ij}=k^j\cdot q^i=(k^j)^Tq^i$. $A=K^TQ$. Step 3: Apply softmax: $a_{1j}'=e^{a_{1j}}/\sum_je^{a_{1j}}$, $A\to A'$. Step 4: Aggregate information - multiply values by attention score $h^i=\sum_ja_{ij}'v^j$, $VA'=H$. $h_1,\dots,h_n$ can be generated together, need not wait for previous steps \\
\textbf{Positional Encoding}: $x'^i=x^i+PE^i$. $PE^i$ is unique for each position. sin and cos can be used to generate position encoding: $PE(i,2k)=\sin(\frac{i}{10000^{\frac{2k}{d}}})$, $PE(i, 2k+1)=\cos(\frac{i}{10000^{\frac{2k}{d}}})$. $2k$ and $2k+1$ are are dimension indices of the positional encoding vector. $d$ is the input feature dimension. $0\leq k\leq \frac{d}{2}-1$ \\
\textbf{Issues with Deep Learning}: Overfitting, Vanishing/Exploding Gradient \\
\textbf{Prevent Overfitting}: Droput – During training, randomly set some neuron's output to 0. Early Stopping – stop before the loss increases \\
\textbf{Vanishing/Exploding Gradient}: Small gradients get multiplied again and again until it reaches almost zero. Mitigation: change activation functions. Large gradients get multiplied until it overflows. Mitigation: gradient clipping (clip within range [-clip value, clip value]) 
\subsubsection*{\underline{12 Attention Neural Networks}}
\textbf{Many-to-one}: $T_x>1,T_y=1$. $h^{SUM}$ is used to capture relevant summary information throughout the entire input sequence to predict the output, thereby minimizing training loss\\
\textbf{One-to-many}: $T_x=1,T_y>1$. Output is fed as input to next time step generate next hidden vector. $x^1, \hat{y}^1,\dots,\hat{y}^{T_{y-1}}$ used to generate $h^{T_y}$. \\
\textbf{Masked Self-Attention Layer}: In step 3, add mask $[0, -inf, -inf]$ to attention scores before applying softmax. \\
\textbf{Many-to-many}: $T_x\not=T_y,T_x>1,T_y>1$. Use encoder model to generate a representation for input sequence and decoder to use encoded vectors to generate the output sequence. Self-attention layer used in encoder. Masked self-attention layer + Cross-attention layer for one-to-many decoder. \\
\textbf{Cross-Attention Layer}: Key $k_x^i=W^kh_x^i$, Query $q_y^i=W^qh_y^i$, Value $v_x^i=W^vh_x^i$. Attention score $a_{ij}=k_x^j\cdot q_y^i=(k_x^j)^Tq_y^i$. \\
\textbf{Transformer}: a deep attention NN. Basic building blocks: encoder block (Self attention layer + Feed forward NN), decoder block (Masked SA Layer + CA Layer + Feed forward NN). Feed forward NN: Linear layer + ReLu + Linear layer. Encoder: stack of encoder blocks, decoder: stack of decoder blocks. Generative Pretrained Transformer (GPT) are built on the transformer architecture to produce new text based on input prompts. \\
\textbf{Vision Transformer}: convert image into sequence of patches. Flatten each patch into a vector, multiply it by a weight matrix to project it into a patch vector, then add positional encoding vector and add a learnable summary vector. \\
\textbf{AI Ethics}: AI can generate biased output. AI can be used to generate fake content to manipulate human behaviour. Autonomous systems can make decisions independently, but 1. how should they behave, 2. who is accountable for harmful actions \\
\end{multicols*}
}

\end{document}

